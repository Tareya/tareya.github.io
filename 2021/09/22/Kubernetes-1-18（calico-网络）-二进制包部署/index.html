

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.png">
  <link rel="icon" href="/img/icon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="Kubernetes 1.18（calico 网络）二进制包部署">
  <meta name="author" content="Tareya">
  <meta name="keywords" content="">
  
  <title>Kubernetes 1.18（calico 网络）二进制包部署 - 乱序时空</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"tareya.github.io","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>乱序时空</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/background/lomo5.jpeg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Kubernetes 1.18（calico 网络）二进制包部署">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-09-22 15:35" pubdate>
        2021年9月22日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      21.1k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      340
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Kubernetes 1.18（calico 网络）二进制包部署</h1>
            
            <div class="markdown-body">
              <h1 id="Kubernetes-1-18（calico-网络）二进制包部署"><a href="#Kubernetes-1-18（calico-网络）二进制包部署" class="headerlink" title="Kubernetes 1.18（calico 网络）二进制包部署"></a>Kubernetes 1.18（calico 网络）二进制包部署</h1><span id="more"></span>



<h2 id="一、基础信息"><a href="#一、基础信息" class="headerlink" title="一、基础信息"></a>一、基础信息</h2><h3 id="1、集群规划"><a href="#1、集群规划" class="headerlink" title="1、集群规划"></a>1、集群规划</h3><table>
<thead>
<tr>
<th>IP 地址</th>
<th>主机名</th>
<th>角色</th>
</tr>
</thead>
<tbody><tr>
<td>192.168.3.231</td>
<td>k8s-master01</td>
<td>master节点1、etcd 节点1、node节点2</td>
</tr>
<tr>
<td>192.168.3.232</td>
<td>k8s-master02</td>
<td>master节点2、etcd 节点2、node节点3</td>
</tr>
<tr>
<td>192.168.3.233</td>
<td>k8s-master03</td>
<td>master节点3、etcd 节点3、node节点4</td>
</tr>
<tr>
<td>192.168.3.234</td>
<td>k8s-node01</td>
<td>node 节点1</td>
</tr>
<tr>
<td>192.168.3.235</td>
<td>k8s-lb01</td>
<td>LoadBalance 1（VIP 192.168.3.230）</td>
</tr>
<tr>
<td>192.168.3.236</td>
<td>k8s-lb02</td>
<td>LoadBalance 2</td>
</tr>
</tbody></table>
<h3 id="2、版本信息"><a href="#2、版本信息" class="headerlink" title="2、版本信息"></a>2、版本信息</h3><table>
<thead>
<tr>
<th>组件名称</th>
<th>版本信息</th>
</tr>
</thead>
<tbody><tr>
<td>OS</td>
<td>CentOS 7.9</td>
</tr>
<tr>
<td>Docker</td>
<td>20.10.8</td>
</tr>
<tr>
<td>Kubernetes、kubectl</td>
<td>1.18.18</td>
</tr>
<tr>
<td>etcd</td>
<td>3.4.14</td>
</tr>
<tr>
<td>cni</td>
<td>0.6.0</td>
</tr>
<tr>
<td>Calico</td>
<td>3.19.2</td>
</tr>
</tbody></table>
<h3 id="3、路径标准"><a href="#3、路径标准" class="headerlink" title="3、路径标准"></a>3、路径标准</h3><h4 id="1）Kubernetes-master-节点"><a href="#1）Kubernetes-master-节点" class="headerlink" title="1）Kubernetes master 节点"></a>1）Kubernetes master 节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">/data/applications/<br>├── etcd<br>│   ├── bin<br>│   ├── cfg<br>│   ├── data<br>│   ├── logs<br>│   ├── ssl<br>│   └── wal<br>└── kubernetes<br>    ├── bin<br>    ├── cfg<br>    ├── logs<br>    └── ssl<br></code></pre></td></tr></table></figure>



<h4 id="2）-Kubernetes-node-节点"><a href="#2）-Kubernetes-node-节点" class="headerlink" title="2） Kubernetes node 节点"></a>2） Kubernetes node 节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">/data/applications/<br>└── kubernetes<br>    ├── bin<br>    ├── cfg<br>    ├── logs<br>    └── ssl<br></code></pre></td></tr></table></figure>



<h2 id="二、部署实施"><a href="#二、部署实施" class="headerlink" title="二、部署实施"></a>二、部署实施</h2><h3 id="1、环境准备"><a href="#1、环境准备" class="headerlink" title="1、环境准备"></a>1、环境准备</h3><h4 id="1）系统环境"><a href="#1）系统环境" class="headerlink" title="1）系统环境"></a>1）系统环境</h4><h5 id="1-关闭防火墙"><a href="#1-关闭防火墙" class="headerlink" title="1. 关闭防火墙"></a>1. 关闭防火墙</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl stop firewalld<br>systemctl disable firewalld<br></code></pre></td></tr></table></figure>

<h5 id="2-关闭-selinux"><a href="#2-关闭-selinux" class="headerlink" title="2. 关闭 selinux"></a>2. 关闭 selinux</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">setenforce 0																				# 临时关闭<br>sed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/config	# 永久关闭<br></code></pre></td></tr></table></figure>

<h5 id="3-关闭-swap"><a href="#3-关闭-swap" class="headerlink" title="3. 关闭 swap"></a>3. 关闭 swap</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 所有 K8S 节点执行</span><br>swapoff -a      													# 临时关闭<br>sed -i &#x27;/swap/s/UUID/#UUID/g&#x27; /etc/fstab 	# 永久关闭<br></code></pre></td></tr></table></figure>

<h5 id="4-yum-镜像源处理"><a href="#4-yum-镜像源处理" class="headerlink" title="4. yum 镜像源处理"></a>4. yum 镜像源处理</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo &amp;&amp; \<br>curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo <br></code></pre></td></tr></table></figure>

<h5 id="5-安装必要依赖"><a href="#5-安装必要依赖" class="headerlink" title="5. 安装必要依赖"></a>5. 安装必要依赖</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum install -y conntrack ntpdate ntp jq iptables curl sysstat libseccomp wget lsof telnet<br></code></pre></td></tr></table></figure>

<h5 id="6-时间同步"><a href="#6-时间同步" class="headerlink" title="6. 时间同步"></a>6. 时间同步</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 所有 K8S 节点执行</span><br>ntpdate -u time1.aliyun.com 	# 时间同步<br>echo &quot;*/20 * * * * /usr/sbin/ntpdate -u time1.aliyun.com &gt;/dev/null &amp;&quot; &gt;&gt; /var/spool/cron/root  # 添加定时任务<br></code></pre></td></tr></table></figure>

<h5 id="7-设置系统时区"><a href="#7-设置系统时区" class="headerlink" title="7. 设置系统时区"></a>7. 设置系统时区</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 调整系统 TimeZone</span><br>timedatectl set-timezone Asia/Shanghai<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 将当前的 UTC 时间写入硬件时钟</span><br>timedatectl set-local-rtc 0<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 重启依赖于系统时间的服务</span><br>systemctl restart rsyslog<br>systemctl restart crond<br></code></pre></td></tr></table></figure>

<h5 id="8-内核参数优化"><a href="#8-内核参数优化" class="headerlink" title="8. 内核参数优化"></a>8. 内核参数优化</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; /etc/sysctl.d/kubernetes.conf &lt;&lt; EOF<br>net.ipv4.ip_forward=1<br>net.ipv4.tcp_tw_recycle=0  				#由于tcp_tw_recycle与kubernetes的NAT冲突，必须关闭！否则会导致服务不通。<br>vm.swappiness=0            				#禁止使用 swap 空间，只有当系统 OOM 时才允许使用它<br>vm.overcommit_memory=1     				#不检查物理内存是否够用<br>vm.panic_on_oom=0         			 	#开启 OOM<br>fs.inotify.max_user_instances=8192<br>fs.inotify.max_user_watches=1048576<br>fs.file-max=52706963<br>fs.nr_open=52706963<br>net.ipv6.conf.all.disable_ipv6=1  #关闭不使用的ipv6协议栈，防止触发docker BUG.<br>EOF<br><br>sysctl -p /etc/sysctl.d/kubernetes.conf<br></code></pre></td></tr></table></figure>

<h5 id="9-关闭无用服务"><a href="#9-关闭无用服务" class="headerlink" title="9. 关闭无用服务"></a>9. 关闭无用服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl stop postfix.service <br>systemctl disable postfix.service<br></code></pre></td></tr></table></figure>

<h5 id="10-添加-host-dns-解析"><a href="#10-添加-host-dns-解析" class="headerlink" title="10. 添加 host / dns 解析"></a>10. 添加 host / dns 解析</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt;&gt; /etc/hosts &lt;&lt; EOF<br>192.168.3.231    k8s-master01    etcd-01<br>192.168.3.232    k8s-master02    etcd-02<br>192.168.3.233    k8s-master03    etcd-03<br>192.168.3.234    k8s-node01<br>192.168.3.235    k8s-lb01<br>192.168.3.236    k8s-lb02<br>EOF<br></code></pre></td></tr></table></figure>



<h4 id="2）进行节点间免密通信处理"><a href="#2）进行节点间免密通信处理" class="headerlink" title="2）进行节点间免密通信处理"></a>2）进行节点间免密通信处理</h4><h5 id="1-安装-sshpass"><a href="#1-安装-sshpass" class="headerlink" title="1. 安装 sshpass"></a>1. 安装 sshpass</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum install -y sshpass<br></code></pre></td></tr></table></figure>

<h5 id="2-创建自动处理脚本ssh-copy-id-sh-，脚本如下"><a href="#2-创建自动处理脚本ssh-copy-id-sh-，脚本如下" class="headerlink" title="2. 创建自动处理脚本ssh-copy-id.sh ，脚本如下:"></a>2. 创建自动处理脚本<code>ssh-copy-id.sh</code> ，脚本如下:</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">!/bin/sh</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 以下变量根据实际情况自行修改</span><br>IPArray=(192.168.3.231 192.168.3.232 192.168.3.233 192.168.3.234 192.168.3.235 192.168.3.236)<br>Port=36022<br>TargetPass=Union@#JuYin2021<br>TargetUser=uniondrug<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 创建密钥</span><br>rm ~/.ssh/id_rsa* -f<br>ssh-keygen -t rsa -f ~/.ssh/id_rsa -N &quot;&quot; -q<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发公钥</span><br>for IP in $&#123;IPArray[@]&#125;<br>do<br><br>   sshpass -p &quot;$TargetPass&quot; ssh-copy-id -o &quot;StrictHostKeyChecking no&quot;  -i ~/.ssh/id_rsa.pub -p $Port $TargetUser@$IP &amp;&gt;/dev/null<br><br>done<br></code></pre></td></tr></table></figure>

<h5 id="3-运行脚本："><a href="#3-运行脚本：" class="headerlink" title="3. 运行脚本："></a>3. 运行脚本：</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sh -x ssh-copy-id.sh<br></code></pre></td></tr></table></figure>



<h4 id="3）安装批管理工具"><a href="#3）安装批管理工具" class="headerlink" title="3）安装批管理工具"></a>3）安装批管理工具</h4><h5 id="1-安装-python3"><a href="#1-安装-python3" class="headerlink" title="1. 安装 python3"></a>1. 安装 python3</h5><p>创建自动处理脚本 <code>install-python3.py</code>，脚本如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#coding=utf-8</span><br><span class="hljs-keyword">import</span> subprocess<br><span class="hljs-keyword">import</span> os,sys<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">exe_cmd</span>(<span class="hljs-params">cmd</span>):</span><br>    p = subprocess.Popen(cmd, shell=<span class="hljs-literal">True</span>, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        next_line = p.stdout.readline()<br>        return_line = next_line.decode(<span class="hljs-string">&quot;utf-8&quot;</span>, <span class="hljs-string">&quot;ignore&quot;</span>)<br>        <span class="hljs-keyword">if</span> return_line == <span class="hljs-string">&#x27;&#x27;</span> <span class="hljs-keyword">and</span> p.poll() != <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">break</span><br>        <span class="hljs-built_in">print</span>(return_line)<br>    stdout, stderr = p.communicate()<br>    <span class="hljs-keyword">if</span> p.returncode != <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span>():</span><br>    <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;python 3.8.5 一键安装开始！&quot;</span>)<br>    backinfo = exe_cmd(<span class="hljs-string">&#x27;ping -c 1 -w 1 www.baidu.com&#x27;</span>)<br>    <span class="hljs-keyword">if</span> backinfo == <span class="hljs-literal">False</span>:<br>        <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;网络检测失败程序退出，请重新检测网络环境!&quot;</span>)<br>        sys.exit()<br>    <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;网络连接正常!&quot;</span>)<br>    yum_jc = exe_cmd(<span class="hljs-string">&#x27;yum list&#x27;</span>)<br>    <span class="hljs-keyword">if</span> yum_jc == <span class="hljs-literal">False</span>:<br>        <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;yum 不可用，请先手动配置yum安装!&quot;</span>)<br>        sys.exit()<br>    <span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;yum安装检测可用!&#x27;</span>)<br><br>    exe_cmd(<span class="hljs-string">&quot;yum install gcc -y&quot;</span>)<br>    exe_cmd(<span class="hljs-string">&quot;yum install openssl-devel bizp2-devel expat-devel gdbm-devel readline-devel sqlite-devel libffi-devel -y&quot;</span>)<br>    exe_cmd(<span class="hljs-string">&quot;wget http://npm.taobao.org/mirrors/python/3.8.5/Python-3.8.5.tgz&quot;</span>)<br>    exe_cmd(<span class="hljs-string">&quot;tar -zxvf Python-3.8.5.tgz&quot;</span>)<br>    exe_cmd(<span class="hljs-string">&quot;mv Python-3.8.5 /usr/local/&quot;</span>)<br>    exe_cmd(<span class="hljs-string">&quot;rm -rf Python-3.8.5.tgz&quot;</span>)<br>    os.chdir(<span class="hljs-string">&#x27;/usr/local/Python-3.8.5&#x27;</span>)<br>    exe_cmd(<span class="hljs-string">&quot;./configure&quot;</span>)<br>    exe_cmd(<span class="hljs-string">&quot;make&quot;</span>)<br>    exe_cmd(<span class="hljs-string">&quot;make install&quot;</span>)<br>    <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;程序执行完成！输入python3 查看效果。&quot;</span>)<br>    exe_cmd(<span class="hljs-string">&quot;python3 -V&quot;</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></td></tr></table></figure>

<h5 id="2-运行脚本"><a href="#2-运行脚本" class="headerlink" title="2. 运行脚本:"></a>2. 运行脚本:</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python install-python3.py<br></code></pre></td></tr></table></figure>

<h5 id="3-安装-ansible"><a href="#3-安装-ansible" class="headerlink" title="3. 安装 ansible"></a>3. 安装 ansible</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple ansible==2.7.1<br></code></pre></td></tr></table></figure>

<h5 id="4-管理-ansible-配置"><a href="#4-管理-ansible-配置" class="headerlink" title="4. 管理 ansible 配置"></a>4. 管理 ansible 配置</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p /etc/ansible/roles &amp;&amp; \<br>cat &gt; /etc/ansible/ansible.cfg &lt;&lt; EOF<br>[defaults]<br>inventory = /etc/ansible/hosts<br>remote_tmp = /tmp/.ansible/<br>local_tmp = ~/.ansible/tmp<br>remote_user = uniondrug<br>sudo_user = root<br>remote_port = 36022<br>host_key_checking = False  <br><br>roles_path = /etc/ansible/roles<br><br>EOF<br></code></pre></td></tr></table></figure>

<h5 id="5-管理-ansible-主机清单"><a href="#5-管理-ansible-主机清单" class="headerlink" title="5. 管理 ansible 主机清单"></a>5. 管理 ansible 主机清单</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; /etc/ansible/hosts &lt;&lt; EOF<br>[k8s-master01]<br>192.168.3.231 ansible_ssh_port=36022 ansible_ssh_user=uniondrug ansible_become_pass=&#x27;*UiiJC9&amp;#CzV@b0H&#x27;<br><br>[k8s-master02]<br>192.168.3.232 ansible_ssh_port=36022 ansible_ssh_user=uniondrug ansible_become_pass=&#x27;*UiiJC9&amp;#CzV@b0H&#x27;<br><br>[k8s-master03]<br>192.168.3.233 ansible_ssh_port=36022 ansible_ssh_user=uniondrug ansible_become_pass=&#x27;*UiiJC9&amp;#CzV@b0H&#x27;<br><br>[k8s-node01]<br>192.168.3.234 ansible_ssh_port=36022 ansible_ssh_user=uniondrug ansible_become_pass=&#x27;*UiiJC9&amp;#CzV@b0H&#x27;<br><br>[k8s-lb01]<br>192.168.3.235 ansible_ssh_port=36022 ansible_ssh_user=uniondrug ansible_become_pass=&#x27;*UiiJC9&amp;#CzV@b0H&#x27;<br><br>[k8s-lb02]<br>192.168.3.236 ansible_ssh_port=36022 ansible_ssh_user=uniondrug ansible_become_pass=&#x27;*UiiJC9&amp;#CzV@b0H&#x27;<br><br>[k8s-master:children]<br>k8s-master01<br>k8s-master02<br>k8s-master03<br><br>[k8s-node:children]<br>k8s-master01<br>k8s-master02<br>k8s-master03<br>k8s-node01<br><br>[k8s-nginx:children]<br>k8s-lb01<br>k8s-lb02<br>EOF<br></code></pre></td></tr></table></figure>



<h4 id="4）设置-rsyslogd-和-systemd-journald-日志持久化"><a href="#4）设置-rsyslogd-和-systemd-journald-日志持久化" class="headerlink" title="4）设置 rsyslogd 和 systemd journald 日志持久化"></a>4）设置 rsyslogd 和 systemd journald 日志持久化</h4><blockquote>
<p>🌈 systemd 的 journald 是 CentOS 7 缺省日志记录工具，它记录了所有系统、内核、Service Unit 的日志，相较于 systemd 本身，journald 的优势在于:</p>
<ul>
<li>可以记录到内存或文件系统；（默认记录到内存，对应的位置为 /run/log/journal）</li>
<li>可以限制占用的磁盘空间、保证磁盘剩余空间</li>
<li>可以限制日志的大小、保存的时间</li>
</ul>
<p>需要注意的是 <code>journald</code> 默认会将日志转发给 <code>rsyslog</code>，这会导致日志写了多份，``/var/log/messages<code> </code>中包含了太多无关日志，不方便后续查看，同时也影响系统性能，所以我们在这里提前做优化处理。</p>
</blockquote>
<h5 id="1-批量创建日志目录"><a href="#1-批量创建日志目录" class="headerlink" title="1. 批量创建日志目录"></a>1. 批量创建日志目录</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 创建日志持久化目录</span><br>ansible all -S -R root -m file -a &quot;path=/var/log/journal state=directory owner=root group=root mode=0755&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 创建 journald 子配置文件目录</span><br>ansible all -S -R root -m file -a &quot;path=/etc/systemd/journald.conf.d state=directory owner=root group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>

<h5 id="2-创建-journald-持久化配置"><a href="#2-创建-journald-持久化配置" class="headerlink" title="2. 创建 journald 持久化配置"></a>2. 创建 journald 持久化配置</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; 99-prophet.conf &lt;&lt;EOF<br>[Journal]<br><span class="hljs-meta">#</span><span class="bash"> 持久化保存到磁盘</span><br>Storage=persistent<br>     <br><span class="hljs-meta">#</span><span class="bash"> 压缩历史日志</span><br>Compress=yes<br>     <br>SyncIntervalSec=5m<br>RateLimitInterval=30s<br>RateLimitBurst=1000<br>     <br><span class="hljs-meta">#</span><span class="bash"> 最大占用空间 10G</span><br>SystemMaxUse=10G<br>     <br><span class="hljs-meta">#</span><span class="bash"> 单日志文件最大 200M</span><br>SystemMaxFileSize=200M<br>     <br><span class="hljs-meta">#</span><span class="bash"> 日志保存时间 2 周</span><br>MaxRetentionSec=2week<br>     <br><span class="hljs-meta">#</span><span class="bash"> 不将日志转发到 syslog</span><br>ForwardToSyslog=no<br>EOF<br></code></pre></td></tr></table></figure>

<h5 id="3-分发配置文件"><a href="#3-分发配置文件" class="headerlink" title="3. 分发配置文件"></a>3. 分发配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible all -S -R root -m copy -a &quot;src=99-prophet.conf dest=/etc/systemd/journald.conf.d/99-prophet.conf owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>

<h5 id="4-批量重启-journald"><a href="#4-批量重启-journald" class="headerlink" title="4. 批量重启 journald"></a>4. 批量重启 journald</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible all -S -R root -m shell -a &quot;systemctl restart systemd-journald&quot;<br></code></pre></td></tr></table></figure>



<h4 id="5）创建-Kubernetes-相关目录"><a href="#5）创建-Kubernetes-相关目录" class="headerlink" title="5）创建 Kubernetes 相关目录"></a>5）创建 Kubernetes 相关目录</h4><h5 id="1-批量创建-etcd-集群相关目录"><a href="#1-批量创建-etcd-集群相关目录" class="headerlink" title="1. 批量创建 etcd 集群相关目录"></a>1. 批量创建 etcd 集群相关目录</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m shell -a &quot;mkdir -p /data/applications/etcd/&#123;bin,cfg,ssl,data,wal,logs&#125;&quot;<br></code></pre></td></tr></table></figure>

<h5 id="2-批量创建-K8S-集群相关目录"><a href="#2-批量创建-K8S-集群相关目录" class="headerlink" title="2. 批量创建 K8S 集群相关目录"></a>2. 批量创建 K8S 集群相关目录</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m shell -a &quot;mkdir -p /data/applications/kubernetes/&#123;bin,cfg,ssl,logs&#125;&quot;<br></code></pre></td></tr></table></figure>

<h5 id="3-批量创建-docker-相关目录"><a href="#3-批量创建-docker-相关目录" class="headerlink" title="3. 批量创建 docker 相关目录"></a>3. 批量创建 docker 相关目录</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m file -a &quot;path=/data/applications/docker state=directory owner=root group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>



<h4 id="6）下载-kubernetes-组件"><a href="#6）下载-kubernetes-组件" class="headerlink" title="6）下载 kubernetes 组件"></a>6）下载 kubernetes 组件</h4><h5 id="1-下载解压-1-18-18-server-组件"><a href="#1-下载解压-1-18-18-server-组件" class="headerlink" title="1. 下载解压 1.18.18 server 组件"></a>1. 下载解压 1.18.18 server 组件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~ &amp;&amp; \<br>wget -c https://dl.k8s.io/v1.18.18/kubernetes-server-linux-amd64.tar.gz &amp;&amp; \<br>tar xf kubernetes-server-linux-amd64.tar.gz<br></code></pre></td></tr></table></figure>

<h6 id="验证"><a href="#验证" class="headerlink" title="验证:"></a>验证:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">~/kubernetes/server/bin/kube-apiserver --version<br>~/kubernetes/server/bin/kube-controller-manager --version<br>~/kubernetes/server/bin/kube-scheduler --version<br></code></pre></td></tr></table></figure>

<p>正常返回都如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">Kubernetes v1.18.18<br></code></pre></td></tr></table></figure>

<h5 id="2-下载解压-1-18-18-node-组件"><a href="#2-下载解压-1-18-18-node-组件" class="headerlink" title="2. 下载解压 1.18.18 node 组件"></a>2. 下载解压 1.18.18 node 组件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~ &amp;&amp; \<br>wget -c https://dl.k8s.io/v1.18.18/kubernetes-node-linux-amd64.tar.gz &amp;&amp; \<br>tar xf kubernetes-node-linux-amd64.tar.gz<br></code></pre></td></tr></table></figure>

<h6 id="验证-1"><a href="#验证-1" class="headerlink" title="验证:"></a>验证:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">~/kubernetes/node/bin/kubelet --version<br>~/kubernetes/node/bin/kube-proxy --version<br></code></pre></td></tr></table></figure>

<p>正常返回都如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">Kubernetes v1.18.18<br></code></pre></td></tr></table></figure>



<h4 id="7）分发-Kubernetes-组件"><a href="#7）分发-Kubernetes-组件" class="headerlink" title="7）分发 Kubernetes 组件"></a>7）分发 Kubernetes 组件</h4><h5 id="1-分发-server-组件至各-master-节点"><a href="#1-分发-server-组件至各-master-节点" class="headerlink" title="1. 分发 server 组件至各 master 节点"></a>1. 分发 server 组件至各 master 节点</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 分发 kube-apiserver</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/bin/kube-apiserver dest=/data/applications/kubernetes/bin/kube-apiserver owner=root group=root mode=0755&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 kube-controller-manager</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/bin/kube-controller-manager dest=/data/applications/kubernetes/bin/kube-controller-manager owner=root group=root mode=0755&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 kube-scheduler</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/bin/kube-scheduler dest=/data/applications/kubernetes/bin/kube-scheduler owner=root group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>

<h5 id="2-分发-node-组件至各-node-节点（包括-master-节点）"><a href="#2-分发-node-组件至各-node-节点（包括-master-节点）" class="headerlink" title="2. 分发 node 组件至各 node 节点（包括 master 节点）"></a>2. 分发 node 组件至各 node 节点（包括 master 节点）</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 分发 kubelet</span><br>ansible k8s-node -S -R root -m copy -a &quot;src=~/kubernetes/node/bin/kubelet dest=/data/applications/kubernetes/bin/kubelet owner=root group=root mode=0755&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 kube-proxy</span><br>ansible k8s-node -S -R root -m copy -a &quot;src=~/kubernetes/node/bin/kube-proxy dest=/data/applications/kubernetes/bin/kube-proxy owner=root group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>



<h4 id="8）安装-docker-环境"><a href="#8）安装-docker-环境" class="headerlink" title="8）安装 docker 环境"></a>8）安装 docker 环境</h4><blockquote>
<p>🚩 <strong>关于是否需要升级系统内核的情况说明</strong></p>
<p>CentOS 7.x 系统自带的 3.10.x 的内核存在与容器兼容性上的问题，会导致 Docker、Kubernetes 运行不稳定，比如：</p>
<ul>
<li>docker 1.13 版本后默认启用了 3.10 内核实验支持的 <code>kernel memory account</code> 功能且无法关闭，当节点压力大、频繁启动和停止容器时会导致 <code>cgroup memory leak</code>；</li>
<li>网络设备引用的计数泄漏会导致类似报错: <code>&quot;kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1&quot;</code></li>
</ul>
<p><strong>针对这种问题有如下几种方案:</strong></p>
<p>1.升级内核直 4.4.x 以上（已做兼容性处理）</p>
<p>2.手动编译内核，disable 掉 <code>CONFIG_MEMCG_KMEM</code> 特性</p>
<p>3.Docker 18.09.1 版本起已修复了该兼容性问题，所以也可以直接选择安装高版本的 Docker；但由于 kubelet 也会设置 kmem（它 vendor 了 runc），所以需要重新编译 kubelet 并指定 GOFLAGS=<code>&quot;-tags=nokmem&quot;</code></p>
<p>🌈 <strong>解决方案文档</strong></p>
<p>可看之前的博客，<a href="https://tareya.github.io/2021/09/27/Kubernetes-kmem-%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/">Kubernetes kmem 内存泄漏问题解决方案</a></p>
<p>🌟 <strong>批量替换 runc</strong></p>
<p><a target="_blank" rel="noopener" href="https://uniondrug-devops.oss-cn-hangzhou.aliyuncs.com/runc">已处理 kmem runc 二进制包下载</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible all -S -R root -m copy -a &quot;src=runc dest=/usr/bin/runc owner=root group=root mode=0755&quot;<br>ansible all -S -R root -m copy -a &quot;src=runc dest=/usr/bin/docker-runc owner=root group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>
</blockquote>
<h5 id="1-自动安装升级脚本"><a href="#1-自动安装升级脚本" class="headerlink" title="1. 自动安装升级脚本"></a>1. 自动安装升级脚本</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; /tmp/install-docker.sh &lt;&lt; END<br><span class="hljs-meta">#</span><span class="bash">!/bin/bash</span><br><br>echo &quot;Docker 一键安装开始！&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 清理残留版本</span><br>yum remove -y docker \<br>	docker-client \<br>	docker-client-latest \<br>	docker-common \<br>	docker-latest \<br>	docker-latest-logrotate \<br>	docker-logrotate \<br>	docker-engine &amp;&amp; \<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 设置存储库</span><br>yum install -y yum-utils \<br>  device-mapper-persistent-data \<br>  lvm2 wget&amp;&amp; \<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 添加 yum 源</span><br>wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo &amp;&amp; \<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 修改国内镜像源</span><br>sed -i &#x27;s#download.docker.com#mirrors.tuna.tsinghua.edu.cn/docker-ce#g&#x27; /etc/yum.repos.d/docker-ce.repo &amp;&amp; \<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 安装最新版本（也可以指定版本，如 docker-ce-18.09.5-3.el7）</span><br>yum install -y docker-ce &amp;&amp; \<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 添加开机自启</span><br>systemctl enable docker <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 生成 docker 镜像加速地址以及日志清理的配置文件。</span><br>if [ ! -d &quot;/etc/docker&quot; ]; then<br>mkdir /etc/docker<br>fi<br><br>cat &gt; /etc/docker/daemon.json &lt;&lt; EOF<br>&#123;<br>  &quot;registry-mirrors&quot;: [&quot;https://5a8zducs.mirror.aliyuncs.com&quot;],<br>  &quot;log-driver&quot;:&quot;json-file&quot;,<br>  &quot;log-opts&quot;: &#123;&quot;max-size&quot;:&quot;500m&quot;, &quot;max-file&quot;:&quot;3&quot;&#125;<br>&#125;<br>EOF<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 启动 docker (因为要更改 docker 数据目录，建议不直接启动)</span><br><span class="hljs-meta">#</span><span class="bash"> systemctl start docker</span><br><br>echo &quot;Docker 安装已完成!&quot;<br><br>docker version<br><br>END<br></code></pre></td></tr></table></figure>

<h5 id="2-新建-docker-systemd-unit-配置"><a href="#2-新建-docker-systemd-unit-配置" class="headerlink" title="2. 新建 docker systemd unit 配置"></a>2. 新建 docker systemd unit 配置</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; docker.service &lt;&lt; EOF<br>[Unit]<br>Description=Docker Application Container Engine<br>Documentation=https://docs.docker.com<br>After=network-online.target firewalld.service containerd.service<br>Wants=network-online.target<br>Requires=docker.socket containerd.service<br><br>[Service]<br>Type=notify<br><span class="hljs-meta">#</span><span class="bash"> the default is not to use systemd <span class="hljs-keyword">for</span> cgroups because the delegate issues still</span><br><span class="hljs-meta">#</span><span class="bash"> exists and systemd currently does not support the cgroup feature <span class="hljs-built_in">set</span> required</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-keyword">for</span> containers run by docker</span><br>ExecStart=/usr/bin/dockerd -H fd:// --graph=/data/applications/docker --containerd=/run/containerd/containerd.sock<br>ExecReload=/bin/kill -s HUP $MAINPID<br>TimeoutSec=0<br>RestartSec=2<br>Restart=always<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> Note that StartLimit* options were moved from <span class="hljs-string">&quot;Service&quot;</span> to <span class="hljs-string">&quot;Unit&quot;</span> <span class="hljs-keyword">in</span> systemd 229.</span><br><span class="hljs-meta">#</span><span class="bash"> Both the old, and new location are accepted by systemd 229 and up, so using the old location</span><br><span class="hljs-meta">#</span><span class="bash"> to make them work <span class="hljs-keyword">for</span> either version of systemd.</span><br>StartLimitBurst=3<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> Note that StartLimitInterval was renamed to StartLimitIntervalSec <span class="hljs-keyword">in</span> systemd 230.</span><br><span class="hljs-meta">#</span><span class="bash"> Both the old, and new name are accepted by systemd 230 and up, so using the old name to make</span><br><span class="hljs-meta">#</span><span class="bash"> this option work <span class="hljs-keyword">for</span> either version of systemd.</span><br>StartLimitInterval=60s<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> Having non-zero Limit*s causes performance problems due to accounting overhead</span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-keyword">in</span> the kernel. We recommend using cgroups to <span class="hljs-keyword">do</span> container-local accounting.</span><br>LimitNOFILE=infinity<br>LimitNPROC=infinity<br>LimitCORE=infinity<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> Comment TasksMax <span class="hljs-keyword">if</span> your systemd version does not support it.</span><br><span class="hljs-meta">#</span><span class="bash"> Only systemd 226 and above support this option.</span><br>TasksMax=infinity<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">set</span> delegate yes so that systemd does not reset the cgroups of docker containers</span><br>Delegate=yes<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> <span class="hljs-built_in">kill</span> only the docker process, not all processes <span class="hljs-keyword">in</span> the cgroup</span><br>KillMode=process<br>OOMScoreAdjust=-500<br><br>[Install]<br>WantedBy=multi-user.target<br><br>EOF<br></code></pre></td></tr></table></figure>

<h5 id="3-分发-systemd-unit-配置"><a href="#3-分发-systemd-unit-配置" class="headerlink" title="3. 分发 systemd unit 配置"></a>3. 分发 systemd unit 配置</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible all -S -R root -m copy -a &quot;src=docker.service dest=/usr/lib/systemd/system/docker.service owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>

<h5 id="4-批量启动-docker"><a href="#4-批量启动-docker" class="headerlink" title="4. 批量启动 docker"></a>4. 批量启动 docker</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible all -S -R root -m shell -a &quot;systemctl daemon-reload&quot;<br>ansible all -S -R root -m shell -a &quot;systemctl restart docker&quot;<br></code></pre></td></tr></table></figure>

<h5 id="5-验证"><a href="#5-验证" class="headerlink" title="5. 验证"></a>5. 验证</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ll /data/applications/docker/<br></code></pre></td></tr></table></figure>

<p>正常情况，存储目录已转移至<code>/data/applications/docker</code></p>
<p><img src="https://blog-images.tareya.cn/blog_images/image-20210930134233588.png" srcset="/img/loading.gif" lazyload alt="image-20210930134233588"></p>
<h3 id="2、生成证书"><a href="#2、生成证书" class="headerlink" title="2、生成证书"></a>2、生成证书</h3><h4 id="1）前期准备"><a href="#1）前期准备" class="headerlink" title="1）前期准备"></a>1）前期准备</h4><h5 id="1-安装-cfssl-工具"><a href="#1-安装-cfssl-工具" class="headerlink" title="1. 安装 cfssl 工具"></a>1. 安装 cfssl 工具</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">export CFSSL_URL=&quot;https://pkg.cfssl.org/R1.2&quot;<br>wget &quot;$&#123;CFSSL_URL&#125;/cfssl_linux-amd64&quot; -O /usr/bin/cfssl<br>wget &quot;$&#123;CFSSL_URL&#125;/cfssljson_linux-amd64&quot; -O /usr/bin/cfssljson<br>wget &quot;$&#123;CFSSL_URL&#125;/cfssl-certinfo_linux-amd64&quot; -O /usr/bin/cfssl-certinfo<br>chmod +x /usr/bin/cfssl*<br></code></pre></td></tr></table></figure>

<h5 id="2-创建证书统一管理目录（由此分发至正式目录）"><a href="#2-创建证书统一管理目录（由此分发至正式目录）" class="headerlink" title="2. 创建证书统一管理目录（由此分发至正式目录）"></a>2. 创建证书统一管理目录（由此分发至正式目录）</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p ~/tls<br></code></pre></td></tr></table></figure>

<h5 id="3-创建-k8s-相关组件、配置文件统一管理目录（由此分发至正式目录）"><a href="#3-创建-k8s-相关组件、配置文件统一管理目录（由此分发至正式目录）" class="headerlink" title="3. 创建 k8s 相关组件、配置文件统一管理目录（由此分发至正式目录）"></a>3. 创建 k8s 相关组件、配置文件统一管理目录（由此分发至正式目录）</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p ~/kubernetes/&#123;server,node,client,etcd,network,prometheus,nginx&#125;/&#123;cfg,systemd&#125;<br></code></pre></td></tr></table></figure>



<h4 id="2）生成-ca-根证书"><a href="#2）生成-ca-根证书" class="headerlink" title="2）生成 ca 根证书"></a>2）生成 ca 根证书</h4><blockquote>
<p>🚩 <strong>提示：</strong></p>
<p>ca 证书是集群所有节点可以共享的，所以只需要创建一个 ca 证书即可，后续创建的所有证书都由其签发。</p>
</blockquote>
<h5 id="1-创建-ca-配置文件"><a href="#1-创建-ca-配置文件" class="headerlink" title="1. 创建 ca 配置文件"></a>1. 创建 ca 配置文件</h5><blockquote>
<p>🌈  ca 配置文件用于配置根证书的使用场景（profile） 和具体参数 (usages，过期时间、服务端认证、客户端认证、加密等)，后续在签名其它证书时需要指定特定场景。</p>
</blockquote>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs json">cat &gt; ~/tls/ca-config.json &lt;&lt;EOF<br>&#123;<br>  <span class="hljs-attr">&quot;signing&quot;</span>: &#123;<br>    <span class="hljs-attr">&quot;default&quot;</span>: &#123;<br>      <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span><br>    &#125;,<br>    <span class="hljs-attr">&quot;profiles&quot;</span>: &#123;<br>      <span class="hljs-attr">&quot;kubernetes&quot;</span>: &#123;<br>        <span class="hljs-attr">&quot;usages&quot;</span>: [<br>            <span class="hljs-string">&quot;signing&quot;</span>,<br>            <span class="hljs-string">&quot;key encipherment&quot;</span>,<br>            <span class="hljs-string">&quot;server auth&quot;</span>,<br>            <span class="hljs-string">&quot;client auth&quot;</span><br>        ],<br>        <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span><br>      &#125;<br>    &#125;<br>  &#125;<br>&#125;<br>EOF<br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明"><a href="#配置参数说明" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><table>
<thead>
<tr>
<th>参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>signing</td>
<td>表示该证书可用于签名其他证书，生成的 ca.pem 证书中 CA=TRUE</td>
</tr>
<tr>
<td>server auth</td>
<td>表示 client 可以通过该证书对 server 提供的证书进行验证</td>
</tr>
<tr>
<td>client auth</td>
<td>表示 server 可以通过该证书对 client 提供的证书进行验证</td>
</tr>
<tr>
<td>expiry</td>
<td>过期时间，默认情况下为 8760h，即1年</td>
</tr>
</tbody></table>
<h5 id="2-创建-ca-证书签名请求文件"><a href="#2-创建-ca-证书签名请求文件" class="headerlink" title="2. 创建 ca 证书签名请求文件"></a>2. 创建 ca 证书签名请求文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/tls/ca-csr.json &lt;&lt;EOF<br>&#123;<br>  &quot;CN&quot;: &quot;kubernetes&quot;,<br>  &quot;key&quot;: &#123;<br>    &quot;algo&quot;: &quot;rsa&quot;,<br>    &quot;size&quot;: 2048<br>  &#125;,<br>  &quot;names&quot;: [<br>    &#123;<br>      &quot;C&quot;: &quot;CN&quot;,<br>      &quot;ST&quot;: &quot;BeiJing&quot;,<br>      &quot;L&quot;: &quot;BeiJing&quot;,<br>      &quot;O&quot;: &quot;k8s&quot;,<br>      &quot;OU&quot;: &quot;system&quot;<br>    &#125;<br>  ],<br>  &quot;ca&quot;: &#123;<br>      &quot;expiry&quot;: &quot;87600h&quot;<br>  &#125;<br>&#125;<br>EOF<br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明-1"><a href="#配置参数说明-1" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><table>
<thead>
<tr>
<th>参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>CN</td>
<td>Common Name，kube-apiserver 会从证书中提取该字段作为请求的用户名（User Name）、浏览器使用该字段验证网站的合法性</td>
</tr>
<tr>
<td>C</td>
<td>Country，国家</td>
</tr>
<tr>
<td>L</td>
<td>Locality，地区，城市</td>
</tr>
<tr>
<td>ST</td>
<td>State，洲，省</td>
</tr>
<tr>
<td>O</td>
<td>Organization，组织名称，公司名称，kube-apiserver 会从证书中提取该字段作为用户所属的组（Group）</td>
</tr>
<tr>
<td>OU</td>
<td>Organization Unit，组织单位名称，公司部门</td>
</tr>
</tbody></table>
<blockquote>
<p>⚠️ <strong>注意:</strong></p>
<p>kube-apiserver 会提取 Group 和 User 作为 K8S RBAC 授权的用户标识，所以 CN 和 O 的命名和重要，建议命名要规范。</p>
</blockquote>
<h5 id="3-生成-CA-证书和私钥"><a href="#3-生成-CA-证书和私钥" class="headerlink" title="3. 生成 CA 证书和私钥"></a>3. 生成 CA 证书和私钥</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~/tls &amp;&amp; \<br>cfssl gencert -initca ca-csr.json | cfssljson -bare ca<br></code></pre></td></tr></table></figure>



<h5 id="4-证书文件说明"><a href="#4-证书文件说明" class="headerlink" title="4. 证书文件说明"></a>4. 证书文件说明</h5><table>
<thead>
<tr>
<th>证书路径</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>~/tls/ca.csr</td>
<td>ca 证书的签名请求文件</td>
</tr>
<tr>
<td>~/tls/ca.pem</td>
<td>ca 证书文件，即后面 K8S 组件使用的 RootCA</td>
</tr>
<tr>
<td>~/tls/ca-key.pem</td>
<td>ca 证书的私钥文件</td>
</tr>
</tbody></table>
<h5 id="5-批量分发证书文件"><a href="#5-批量分发证书文件" class="headerlink" title="5. 批量分发证书文件"></a>5. 批量分发证书文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 分发 ca 证书（k8s 所有节点、etcd 所有节点）</span><br>ansible k8s-node -S -R root -m copy -a &quot;src=~/tls/ca.pem dest=/data/applications/kubernetes/ssl/ca.pem owner=root group=root mode=0644&quot; <br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/ca.pem dest=/data/applications/etcd/ssl/ca.pem owner=root group=root mode=0644&quot; <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 ca 证书私钥，为了安全性，权限设置他人无权限（k8s master节点、etcd 所有节点）</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/ca-key.pem  dest=/data/applications/kubernetes/ssl/ca-key.pem owner=root group=root mode=0600&quot; <br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/ca-key.pem  dest=/data/applications/etcd/ssl/ca-key.pem owner=root group=root mode=0600&quot; <br></code></pre></td></tr></table></figure>



<h4 id="3）生成-server-证书"><a href="#3）生成-server-证书" class="headerlink" title="3）生成 server 证书"></a>3）生成 server 证书</h4><blockquote>
<p>🚩 <strong>提示:</strong></p>
<p>此处生成的 server 证书同时可提供 etcd 集群和 kube-apiserver 使用（可以根据实际情况拆分成2套）</p>
</blockquote>
<h5 id="1-创建-server-书签名请求文件"><a href="#1-创建-server-书签名请求文件" class="headerlink" title="1. 创建 server 书签名请求文件"></a>1. 创建 server 书签名请求文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/tls/server-csr.json &lt;&lt;EOF<br>&#123;<br>    &quot;CN&quot;: &quot;kubernetes&quot;,<br>    &quot;hosts&quot;: [<br>      &quot;10.0.0.1&quot;,<br>      &quot;127.0.0.1&quot;,<br>      &quot;192.168.3.230&quot;,<br>      &quot;192.168.3.231&quot;,<br>      &quot;192.168.3.232&quot;,<br>      &quot;192.168.3.233&quot;,<br>      &quot;k8s-master01&quot;,<br>      &quot;k8s-master02&quot;,<br>      &quot;k8s-master03&quot;,<br>      &quot;etcd-01&quot;,<br>      &quot;etcd-02&quot;,<br>      &quot;etcd-03&quot;,<br>      &quot;kubernetes&quot;,<br>      &quot;kubernetes.default&quot;,<br>      &quot;kubernetes.default.svc&quot;,<br>      &quot;kubernetes.default.svc.cluster&quot;,<br>      &quot;kubernetes.default.svc.cluster.local&quot;<br>    ],<br>    &quot;key&quot;: &#123;<br>        &quot;algo&quot;: &quot;rsa&quot;,<br>        &quot;size&quot;: 2048<br>    &#125;,<br>    &quot;names&quot;: [<br>        &#123;<br>            &quot;C&quot;: &quot;CN&quot;,<br>            &quot;L&quot;: &quot;BeiJing&quot;,<br>            &quot;ST&quot;: &quot;BeiJing&quot;,<br>            &quot;O&quot;: &quot;k8s&quot;,<br>            &quot;OU&quot;: &quot;System&quot;<br>        &#125;<br>    ]<br>&#125;<br>EOF<br></code></pre></td></tr></table></figure>

<blockquote>
<p>⚠️ <strong>注意:</strong></p>
<p>如果 hosts 字段的值不为空就需要指定具体授权的 IP 和 域名列表，这里列出了 vip、apiserver 节点 ip、kubernetes 服务 ip 和域名</p>
<p>server 证书后续会被 etcd 集群和 K8S master 节点使用，所以要将 etcd、master 节点和 kube-apiserver 的vip都填上，同时还有service 网络的首IP也需要填上。(一般是 kube-apiserver 指定 <code>--service-cluster-ip-range</code> 的网段的第一个IP，如 10.0.0.1)</p>
<hr>
<p>这里需要注意的是，如果使用域名的话，域名最后的字符不能是 . ，如不能为 <code>kubernetes.default.svc.cluster.local.</code> 。虽然按常规dns解析的话最后带 . 表示根域，但是在 k8s 中会导致解析失败，提示 <code>x509: cannot parse dnsName &quot;kubernetes.default.svc.cluster.local.&quot;</code></p>
<p>另外，如果要使用非 <code>cluster.local</code> 的域名，如 <code>tareya.cn</code>，则需要修改域名列表中最后两个域名为 <code>kubernetes.default.svc.tareya</code> 和 <code>kubernetes.default.svc.tareya.cn</code></p>
</blockquote>
<h5 id="2-生成-server-证书和私钥"><a href="#2-生成-server-证书和私钥" class="headerlink" title="2. 生成 server 证书和私钥"></a>2. 生成 server 证书和私钥</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~/tls &amp;&amp; \<br>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server<br></code></pre></td></tr></table></figure>



<h5 id="3-证书文件说明"><a href="#3-证书文件说明" class="headerlink" title="3. 证书文件说明"></a>3. 证书文件说明</h5><table>
<thead>
<tr>
<th>证书路径</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>~/tls/server.csr</td>
<td>server 证书的签名请求文件</td>
</tr>
<tr>
<td>~/tls/server.pem</td>
<td>server 证书，为 etcd、kube-apiserver 使用</td>
</tr>
<tr>
<td>~/tls/server-key.pem</td>
<td>server 证书的私钥文件</td>
</tr>
</tbody></table>
<h5 id="4-批量分发证书文件"><a href="#4-批量分发证书文件" class="headerlink" title="4. 批量分发证书文件"></a>4. 批量分发证书文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 分发 server 证书（k8s master节点、etcd 所有节点）</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/server.pem dest=/data/applications/kubernetes/ssl/server.pem owner=root group=root mode=0644&quot; <br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/server.pem dest=/data/applications/etcd/ssl/server.pem owner=root group=root mode=0644&quot; <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 server 证书私钥，为了安全性，权限设置他人无权限（k8s master节点、etcd 所有节点）</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/server-key.pem dest=/data/applications/kubernetes/ssl/server-key.pem owner=root group=root mode=0600&quot; <br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/server-key.pem dest=/data/applications/etcd/ssl/server-key.pem owner=root group=root mode=0600&quot; <br></code></pre></td></tr></table></figure>



<h4 id="4）生成-etcd-证书"><a href="#4）生成-etcd-证书" class="headerlink" title="4）生成 etcd 证书"></a>4）生成 etcd 证书</h4><h4 id="5）生成-admin-证书"><a href="#5）生成-admin-证书" class="headerlink" title="5）生成 admin 证书"></a>5）生成 admin 证书</h4><blockquote>
<p>🚩 <strong>提示:</strong></p>
<p>k8s 的客户端工具（如 kubectl）和 apiserver 之间是通过 https安全端口进行通信，apiserver 会对客户端提供的证书进行认证、授权。客户端工具通过 kubeconfig 文件来进行鉴权。</p>
<p>kubectl 作为整个集群的管理工具，需要被授予最高的权限，因此这里我们创建具有最高权限的 admin 证书。</p>
</blockquote>
<h5 id="1-创建-admin-证书签名请求文件"><a href="#1-创建-admin-证书签名请求文件" class="headerlink" title="1. 创建 admin 证书签名请求文件"></a>1. 创建 admin 证书签名请求文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/tls/admin-csr.json &lt;&lt;EOF<br>&#123;<br>  &quot;CN&quot;: &quot;admin&quot;,<br>  &quot;hosts&quot;: [],<br>  &quot;key&quot;: &#123;<br>    &quot;algo&quot;: &quot;rsa&quot;,<br>    &quot;size&quot;: 2048<br>  &#125;,<br>  &quot;names&quot;: [<br>    &#123;<br>      &quot;C&quot;: &quot;CN&quot;,<br>      &quot;ST&quot;: &quot;BeiJing&quot;,<br>      &quot;L&quot;: &quot;BeiJing&quot;,<br>      &quot;O&quot;: &quot;system:masters&quot;,<br>      &quot;OU&quot;: &quot;system&quot;<br>    &#125;<br>  ]<br>&#125;<br>EOF<br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明-2"><a href="#配置参数说明-2" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><blockquote>
<p>这里将 O 设置为 <code>system:masters</code>，kube-apiserver 会提取该字段，将请求的 Group 设置为 <code>system:masters</code>；</p>
<p>Kube-apiserver 预定义的 ClusterRoleBinding <code>cluster-admin</code> 将 Group <code>system:masters</code> 和 ClusterRole <code>cluster-admin</code> 绑定，该 Role 授权访问所有 API 的权限；</p>
<p>该证书只会被 kubectl 当作 client 证书使用，因此 hosts 字段不需要限定IP、域名，设置为空即可。</p>
</blockquote>
<h5 id="2-生成-admin-证书和私钥"><a href="#2-生成-admin-证书和私钥" class="headerlink" title="2. 生成 admin 证书和私钥"></a>2. 生成 admin 证书和私钥</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~/tls &amp;&amp; \<br>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin<br></code></pre></td></tr></table></figure>



<h5 id="3-证书文件说明-1"><a href="#3-证书文件说明-1" class="headerlink" title="3. 证书文件说明"></a>3. 证书文件说明</h5><table>
<thead>
<tr>
<th>证书路径</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>~/tls/admin.csr</td>
<td>admin 证书的签名请求文件</td>
</tr>
<tr>
<td>~/tls/admin.pem</td>
<td>admin 证书，kubectl 和其他 k8s 客户端使用</td>
</tr>
<tr>
<td>~/tls/admin-key.pem</td>
<td>admin 证书的私钥文件</td>
</tr>
</tbody></table>
<h5 id="4-批量分发证书文件-1"><a href="#4-批量分发证书文件-1" class="headerlink" title="4. 批量分发证书文件"></a>4. 批量分发证书文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 分发 admin 证书（k8s master节点）</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/admin.pem dest=/data/applications/kubernetes/ssl/admin.pem owner=root group=root mode=0644&quot; <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 admin 证书私钥，为了安全性，权限设置他人无权限（k8s master节点）</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/admin-key.pem dest=/data/applications/kubernetes/ssl/admin-key.pem owner=root group=root mode=0600&quot; <br></code></pre></td></tr></table></figure>



<h4 id="6）生成-metrics-server-证书"><a href="#6）生成-metrics-server-证书" class="headerlink" title="6）生成 metrics-server 证书"></a>6）生成 metrics-server 证书</h4><h5 id="1-创建-metrics-server-证书签名请求文件"><a href="#1-创建-metrics-server-证书签名请求文件" class="headerlink" title="1. 创建 metrics-server 证书签名请求文件"></a>1. 创建 metrics-server 证书签名请求文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/tls/metrics-server-csr.json  &lt;&lt;EOF<br>&#123;<br>  &quot;CN&quot;: &quot;system:metrics-server&quot;,<br>  &quot;hosts&quot;: [],<br>  &quot;key&quot;: &#123;<br>    &quot;algo&quot;: &quot;rsa&quot;,<br>    &quot;size&quot;: 2048<br>  &#125;,<br>  &quot;names&quot;: [<br>    &#123;<br>      &quot;C&quot;: &quot;CN&quot;,<br>      &quot;ST&quot;: &quot;BeiJing&quot;,<br>      &quot;L&quot;: &quot;BeiJing&quot;,<br>      &quot;O&quot;: &quot;k8s&quot;,<br>      &quot;OU&quot;: &quot;system&quot;<br>    &#125;<br>  ]<br>&#125;<br>EOF<br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明-3"><a href="#配置参数说明-3" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><blockquote>
<p>这里 CN 设定的名称，需要与后续 metrics-server 的 <code>--requestheader-allowed-names</code> 参数配置保持一致，否则会被 metrics-server 拒绝访问，</p>
<p>后续需要对 metrics-server 进行专门的 RBAC 授权，建议 CN 配置和 RBAC 授权用户保持一致。</p>
</blockquote>
<h5 id="2-生成-metrics-server-证书和私钥"><a href="#2-生成-metrics-server-证书和私钥" class="headerlink" title="2. 生成 metrics-server 证书和私钥"></a>2. 生成 metrics-server 证书和私钥</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~/tls &amp;&amp; \<br>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes metrics-server-csr.json | cfssljson -bare metrics-server<br></code></pre></td></tr></table></figure>



<h5 id="3-证书文件说明-2"><a href="#3-证书文件说明-2" class="headerlink" title="3. 证书文件说明"></a>3. 证书文件说明</h5><table>
<thead>
<tr>
<th>证书路径</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>~/tls/metrics-server.csr</td>
<td>metrics-server 证书的签名请求文件</td>
</tr>
<tr>
<td>~/tls/metrics-server.pem</td>
<td>metrics-server 证书，metrics-server 使用</td>
</tr>
<tr>
<td>~/tls/metrics-server-key.pem</td>
<td>metrics-server 证书的私钥文件</td>
</tr>
</tbody></table>
<h5 id="4-批量分发证书文件-2"><a href="#4-批量分发证书文件-2" class="headerlink" title="4. 批量分发证书文件"></a>4. 批量分发证书文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 分发 metrics-server 证书（k8s master节点）</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/metrics-server.pem dest=/data/applications/kubernetes/ssl/metrics-server.pem owner=root group=root mode=0644&quot; <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 metrics-server 证书私钥，为了安全性，权限设置他人无权限（k8s master节点）</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/tls/metrics-server-key.pem dest=/data/applications/kubernetes/ssl/metrics-server-key.pem owner=root group=root mode=0600&quot; <br></code></pre></td></tr></table></figure>



<h4 id="7）生成-kube-proxy-证书"><a href="#7）生成-kube-proxy-证书" class="headerlink" title="7）生成 kube-proxy 证书"></a>7）生成 kube-proxy 证书</h4><h5 id="1-创建-kube-proxy-证书签名请求文件"><a href="#1-创建-kube-proxy-证书签名请求文件" class="headerlink" title="1. 创建 kube-proxy 证书签名请求文件"></a>1. 创建 kube-proxy 证书签名请求文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/tls/kube-proxy-csr.json &lt;&lt;EOF<br>&#123;<br>  &quot;CN&quot;: &quot;system:kube-proxy&quot;,<br>  &quot;hosts&quot;: [],<br>  &quot;key&quot;: &#123;<br>    &quot;algo&quot;: &quot;rsa&quot;,<br>    &quot;size&quot;: 2048<br>  &#125;,<br>  &quot;names&quot;: [<br>    &#123;<br>      &quot;C&quot;: &quot;CN&quot;,<br>      &quot;ST&quot;: &quot;BeiJing&quot;,<br>      &quot;L&quot;: &quot;BeiJing&quot;,<br>      &quot;O&quot;: &quot;k8s&quot;,<br>      &quot;OU&quot;: &quot;system&quot;<br>    &#125;<br>  ]<br>&#125;<br>EOF<br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明-4"><a href="#配置参数说明-4" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><blockquote>
<p>这里将 CN 设置为 <code>system:kube-proxy</code>，kube-apiserver 会提取该字段，将请求的 User设置为 <code>system:kube-proxy</code>；</p>
<p>Kube-apiserver 预定义的 ClusterRoleBinding <code>system:node-proxier</code> 将 User <code>system:kube-proxy</code> 和 ClusterRole<code>system:node-proxier </code> 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限；</p>
<p>该证书只会被 kube-proxy 当作 client 证书使用，因此 hosts 字段不需要限定IP、域名，设置为空即可。</p>
</blockquote>
<h5 id="2-生成-kube-proxy-证书和私钥"><a href="#2-生成-kube-proxy-证书和私钥" class="headerlink" title="2. 生成 kube-proxy 证书和私钥"></a>2. 生成 kube-proxy 证书和私钥</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~/tls &amp;&amp; \<br>cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy<br></code></pre></td></tr></table></figure>



<h5 id="3-证书文件说明-3"><a href="#3-证书文件说明-3" class="headerlink" title="3. 证书文件说明"></a>3. 证书文件说明</h5><table>
<thead>
<tr>
<th>证书路径</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>~/tls/kube-proxy.csr</td>
<td>kube-proxy 证书的签名请求文件</td>
</tr>
<tr>
<td>~/tls/kube-proxy.pem</td>
<td>kube-proxy 证书，kube-proxy 使用</td>
</tr>
<tr>
<td>~/tls/kube-proxy-key.pem</td>
<td>Kube-proxy 证书的私钥文件</td>
</tr>
</tbody></table>
<h5 id="4-批量分发证书文件-3"><a href="#4-批量分发证书文件-3" class="headerlink" title="4. 批量分发证书文件"></a>4. 批量分发证书文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 分发 kube-proxy 证书（k8s 所有节点）</span><br>ansible k8s-node -S -R root -m copy -a &quot;src=~/tls/kube-proxy.pem dest=/data/applications/kubernetes/ssl/kube-proxy.pem owner=root group=root mode=0644&quot; <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 kube-proxy 证书私钥，为了安全性，权限设置他人无权限（k8s 所有节点）</span><br>ansible k8s-node -S -R root -m copy -a &quot;src=~/tls/kube-proxy-key.pem dest=/data/applications/kubernetes/ssl/kube-proxy-key.pem owner=root group=root mode=0600&quot; <br></code></pre></td></tr></table></figure>





<h3 id="2、部署-kubectl-命令行工具"><a href="#2、部署-kubectl-命令行工具" class="headerlink" title="2、部署 kubectl 命令行工具"></a>2、部署 kubectl 命令行工具</h3><h4 id="1）下载-kubectl-二进制文件"><a href="#1）下载-kubectl-二进制文件" class="headerlink" title="1）下载 kubectl 二进制文件"></a>1）下载 kubectl 二进制文件</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md">官方下载地址</a></p>
<p>虽然 kubectl 是具备向下兼容能力的，但是在选择版本时还是建议尽量与 K8S 版本保持一致，如本文 K8S 版本为 1.18.18，故 kubectl 版本也选用 1.18.18 。下载二进制时，注意平台匹配。</p>
</blockquote>
<h5 id="1-下载解压-1-18-18-kubectl-二进制包"><a href="#1-下载解压-1-18-18-kubectl-二进制包" class="headerlink" title="1. 下载解压 1.18.18 kubectl 二进制包"></a>1. 下载解压 1.18.18 kubectl 二进制包</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~ &amp;&amp; \<br>wget -c https://dl.k8s.io/v1.18.18/kubernetes-client-linux-amd64.tar.gz &amp;&amp; \<br>tar xf kubernetes-client-linux-amd64.tar.gz<br></code></pre></td></tr></table></figure>

<h5 id="2-验证"><a href="#2-验证" class="headerlink" title="2. 验证"></a>2. 验证</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd kubernetes/client/bin/ &amp;&amp; ./kubectl version<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;18&quot;, GitVersion:&quot;v1.18.18&quot;, GitCommit:&quot;6f6ce59dc8fefde25a3ba0ef0047f4ec6662ef24&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2021-04-15T03:31:30Z&quot;, GoVersion:&quot;go1.13.15&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;<br></code></pre></td></tr></table></figure>



<h4 id="2）分发-kubectl-至各-master-节点"><a href="#2）分发-kubectl-至各-master-节点" class="headerlink" title="2）分发 kubectl 至各 master 节点"></a>2）分发 kubectl 至各 master 节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/client/bin/kubectl dest=/usr/bin/kubectl owner=root<br>group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>



<h4 id="3）创建-kubeconfig"><a href="#3）创建-kubeconfig" class="headerlink" title="3）创建 kubeconfig"></a>3）创建 kubeconfig</h4><h5 id="1-设置集群参数"><a href="#1-设置集群参数" class="headerlink" title="1. 设置集群参数"></a>1. 设置集群参数</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl config set-cluster kubernetes \<br>  --certificate-authority=/data/applications/kubernetes/ssl/ca.pem \<br>  --embed-certs=true \<br>  --server=$&#123;KUBE_APISERVER&#125; \<br>  --kubeconfig=$HOME/.kube/kubectl.kubeconfig<br></code></pre></td></tr></table></figure>

<h6 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明:"></a>参数说明:</h6><table>
<thead>
<tr>
<th>参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>–certificate-authority</td>
<td>验证 kube-apiserver 证书的根证书</td>
</tr>
<tr>
<td>–embed-certs=true</td>
<td>将证书内容加密嵌入kubeconfig中，如果设为 false 则通过指向的文件路径读取文件</td>
</tr>
<tr>
<td>–server</td>
<td>apiserver 的加密通信地址</td>
</tr>
<tr>
<td>–kubeconfig</td>
<td>kubeconfig 生成路径</td>
</tr>
</tbody></table>
<h5 id="2-设置客户端认证参数"><a href="#2-设置客户端认证参数" class="headerlink" title="2. 设置客户端认证参数"></a>2. 设置客户端认证参数</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl config set-credentials admin \<br>  --client-certificate=/data/applications/kubernetes/ssl/admin.pem \<br>  --client-key=/data/applications/kubernetes/ssl/admin-key.pem \<br>  --embed-certs=true \<br>  --kubeconfig=$HOME/.kube/kubectl.kubeconfig<br></code></pre></td></tr></table></figure>

<h6 id="参数说明-1"><a href="#参数说明-1" class="headerlink" title="参数说明:"></a>参数说明:</h6><table>
<thead>
<tr>
<th>参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>–client-certificate</td>
<td>连接 apiserver 时，认证使用的客户端证书</td>
</tr>
<tr>
<td>–client-key</td>
<td>连接 apiserver 时，认证使用的证书私钥</td>
</tr>
</tbody></table>
<h5 id="3-设置上下文参数"><a href="#3-设置上下文参数" class="headerlink" title="3. 设置上下文参数"></a>3. 设置上下文参数</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl config set-context kubernetes \<br> --cluster=kubernetes \<br> --user=admin \<br> --kubeconfig=$HOME/.kube/kubectl.kubeconfig<br></code></pre></td></tr></table></figure>

<h5 id="4-设置默认上下文"><a href="#4-设置默认上下文" class="headerlink" title="4. 设置默认上下文"></a>4. 设置默认上下文</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl config use-context kubernetes --kubeconfig=$HOME/.kube/kubectl.kubeconfig<br></code></pre></td></tr></table></figure>



<h4 id="4）分发-kubeconfig-至各-master-节点"><a href="#4）分发-kubeconfig-至各-master-节点" class="headerlink" title="4）分发 kubeconfig 至各 master 节点"></a>4）分发 kubeconfig 至各 master 节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> root 用户</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=$HOME/.kube/kubectl.kubeconfig dest=/root/.kube/kubectl.kubeconfig owner=root group=root mode=0600&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 普通用户</span><br>ansible k8s-master -m copy -a &quot;src=$HOME/.kube/kubectl.kubeconfig dest=$HOME/.kube/kubectl.kubeconfig mode=0600&quot;<br></code></pre></td></tr></table></figure>





<h3 id="3、部署-etcd-集群"><a href="#3、部署-etcd-集群" class="headerlink" title="3、部署 etcd 集群"></a>3、部署 etcd 集群</h3><p><strong>基础组件介绍 —— etcd:</strong> <a href="https://tareya.github.io/2021/10/21/Kubernetes-%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-etcd/#Kubernetes-%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-etcd">传送门</a></p>
<blockquote>
<p>🌈 etcd 从 3.4 版本开始会自动通过配置文件加载环境变量参数，EnvironmentFile 文件中有的参数不需要在 systems unit 的 ExecStart 中添加了，可以更加的精简化配置，所以我们这里我们选用 3.4 版本的 etcd。方便后期统一的配置管理。</p>
</blockquote>
<h4 id="1）下载-etcd-二进制文件"><a href="#1）下载-etcd-二进制文件" class="headerlink" title="1）下载 etcd 二进制文件"></a>1）下载 etcd 二进制文件</h4><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/etcd-io/etcd/releases">官方下载地址</a></p>
</blockquote>
<h5 id="1-下载解压-3-4-14-etcd-客户端和服务端二进制包"><a href="#1-下载解压-3-4-14-etcd-客户端和服务端二进制包" class="headerlink" title="1. 下载解压 3.4.14 etcd 客户端和服务端二进制包"></a>1. 下载解压 3.4.14 etcd 客户端和服务端二进制包</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~ &amp;&amp; \<br>wget -c https://github.com/coreos/etcd/releases/download/v3.4.14/etcd-v3.4.14-linux-amd64.tar.gz &amp;&amp; \<br>tar xf etcd-v3.4.14-linux-amd64.tar.gz<br></code></pre></td></tr></table></figure>

<h5 id="2-验证-1"><a href="#2-验证-1" class="headerlink" title="2. 验证"></a>2. 验证</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd etcd-v3.4.14-linux-amd64 &amp;&amp; ./etcdctl version<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">etcdctl version: 3.4.14<br>API version: 3.4<br></code></pre></td></tr></table></figure>



<h4 id="2）分发-etcd-至各-master-节点"><a href="#2）分发-etcd-至各-master-节点" class="headerlink" title="2）分发 etcd 至各 master 节点"></a>2）分发 etcd 至各 master 节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 分发 etcd server 端</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/etcd-v3.4.14-linux-amd64/etcd dest=/data/applications/etcd/bin/etcd owner=root group=root mode=0755&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 etcd client 工具</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/etcd-v3.4.14-linux-amd64/etcdctl dest=/data/applications/etcd/bin/etcdctl owner=root group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>



<h4 id="3）创建配置文件"><a href="#3）创建配置文件" class="headerlink" title="3）创建配置文件"></a>3）创建配置文件</h4><h5 id="1-新建-etcd-配置文件-etcd-conf"><a href="#1-新建-etcd-配置文件-etcd-conf" class="headerlink" title="1. 新建 etcd 配置文件 etcd.conf"></a>1. 新建 etcd 配置文件 <code>etcd.conf</code></h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/kubernetes/etcd/cfg/etcd.conf &lt;&lt; EOF<br><span class="hljs-meta">#</span><span class="bash">[Member]</span><br>ETCD_NAME=&quot;etcd-1&quot;<br>ETCD_DATA_DIR=&quot;/data/applications/etcd/data&quot;<br>ETCD_WAL_DIR=&quot;/data/applications/etcd/wal&quot;<br>ETCD_LISTEN_PEER_URLS=&quot;https://192.168.3.231:2380&quot;<br>ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.3.231:2379,http://127.0.0.1:2379&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash">[Clustering]</span><br>ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.3.231:2380&quot;<br>ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.3.231:2379&quot;<br>ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.3.231:2380,etcd-2=https://192.168.3.232:2380,etcd-3=https://192.168.3.233:2380&quot;<br>ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster-0&quot;<br>ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;<br>ETCD_ENABLE_V2=&quot;true&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash">[Security]</span><br>ETCD_CERT_FILE=&quot;/data/applications/etcd/ssl/server.pem&quot;<br>ETCD_KEY_FILE=&quot;/data/applications/etcd/ssl/server-key.pem&quot;<br>ETCD_TRUSTED_CA_FILE=&quot;/data/applications/etcd/ssl/ca.pem&quot;<br>ETCD_CLIENT_CERT_AUTH=&quot;true&quot;<br>ETCD_PEER_CERT_FILE=&quot;/data/applications/etcd/ssl/server.pem&quot;<br>ETCD_PEER_KEY_FILE=&quot;/data/applications/etcd/ssl/server-key.pem&quot;<br>ETCD_PEER_TRUSTED_CA_FILE=&quot;/data/applications/etcd/ssl/ca.pem&quot;<br>ETCD_PEER_CLIENT_CERT_AUTH=&quot;true&quot;<br><br>EOF<br></code></pre></td></tr></table></figure>

<blockquote>
<p>⚠️ <strong>注意:</strong></p>
<p>etcd 每个节点的配置不同，根据实际情况进行修改，可以通过 jinja 模版渲染。</p>
<p>3.4 版本的 etcd 默认使用的是 v3 版本的API，并默认关闭了 v2 版本API的使用，如果要继续使用的话，可以在配置文件中设置 <code>ETCD_ENABLE_V2=&quot;true&quot;</code>，或者在使用命令行时设置 <code>ETCDCTL_API</code> 环境变量，如<code>ETCDCTL_API=2 etcdctl</code>。</p>
<p>flannel 操作 etcd 使用的是 v2 的API， kubernetes 操作 etcd 使用的 v3 的API，calico 3.x 版本也使用 v3 的 API。</p>
</blockquote>
<h5 id="2-新建-systemd-unit-配置-etcd-service"><a href="#2-新建-systemd-unit-配置-etcd-service" class="headerlink" title="2. 新建 systemd unit 配置 etcd.service"></a>2. 新建 systemd unit 配置 <code>etcd.service</code></h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/kubernetes/etcd/systemd/etcd.service &lt;&lt; EOF<br>[Unit]<br>Description=Etcd Server<br>After=network.target<br>After=network-online.target<br>Wants=network-online.target<br>Documentation=https://github.com/coreos<br>   <br>[Service]<br>Type=notify<br>EnvironmentFile=/data/applications/etcd/cfg/etcd.conf<br>ExecStart=/data/applications/etcd/bin/etcd \<br>--auto-compaction-mode=periodic \<br>--auto-compaction-retention=1 \<br>--max-request-bytes=33554432 \<br>--quota-backend-bytes=6442450944 \<br>--heartbeat-interval=250 \<br>--election-timeout=2000 \<br>--logger=zap<br>Restart=on-failure<br>RestartSec=5<br>LimitNOFILE=65536<br>   <br>[Install]<br>WantedBy=multi-user.target<br><br>EOF<br></code></pre></td></tr></table></figure>

<h6 id="重要配置参数说明"><a href="#重要配置参数说明" class="headerlink" title="重要配置参数说明:"></a>重要配置参数说明:</h6><table>
<thead>
<tr>
<th>重要参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>EnvironmentFile</td>
<td>环境变量配置文件，使用改方式可以以加载变量的形式制作统一的 unit 模版</td>
</tr>
<tr>
<td>–cert-file / –key-file</td>
<td>etcd server 与 client 通信时使用的证书和私钥</td>
</tr>
<tr>
<td>–trusted-ca-file</td>
<td>签发 client 证书的 ca 证书，用于验证 client 证书；</td>
</tr>
<tr>
<td>–peer-cert-file / –peer-key-file</td>
<td>etcd peer 之间通信使用的证书和私钥；</td>
</tr>
<tr>
<td>–peer-trusted-ca-file</td>
<td>签发 peer 证书的 ca 证书，用于验证 peer 证书，可以和 client 证书使用同一套，也可以使用不同的；</td>
</tr>
</tbody></table>
<h4 id="4）分发配置文件"><a href="#4）分发配置文件" class="headerlink" title="4）分发配置文件"></a>4）分发配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 分发 systemd unit 配置文件</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/etcd/systemd/etcd.service dest=/usr/lib/systemd/system/etcd.service owner=root group=root mode=0644&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 etcd 配置文件</span><br>ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/etcd/cfg/etcd.conf dest=/data/applications/etcd/cfg/etcd.conf owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h4 id="5）启动集群"><a href="#5）启动集群" class="headerlink" title="5）启动集群"></a>5）启动集群</h4><h5 id="1-批量启动-etcd"><a href="#1-批量启动-etcd" class="headerlink" title="1. 批量启动 etcd"></a>1. 批量启动 etcd</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m shell -a &quot;systemctl daemon-reload&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl enable etcd.service&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl start etcd.service&quot;<br></code></pre></td></tr></table></figure>

<h5 id="2-验证服务状态"><a href="#2-验证服务状态" class="headerlink" title="2. 验证服务状态"></a>2. 验证服务状态</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m shell -a &quot;ETCDCTL_API=3 /data/applications/etcd/bin/etcdctl  --endpoints=http://127.0.0.1:2379 --cacert=/data/applications/etcd/ssl/ca.pem --cert=/data/applications/etcd/ssl/server.pem --key=/data/applications/etcd/ssl/server-key.pem endpoint health&quot;<br></code></pre></td></tr></table></figure>

<h6 id="正常返回结果"><a href="#正常返回结果" class="headerlink" title="正常返回结果:"></a>正常返回结果:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 均返回 healthy 时，集群状态未正常</span><br>192.168.3.233 | CHANGED | rc=0 &gt;&gt;<br>http://127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.802973ms<br><br>192.168.3.231 | CHANGED | rc=0 &gt;&gt;<br>http://127.0.0.1:2379 is healthy: successfully committed proposal: took = 2.426585ms<br><br>192.168.3.232 | CHANGED | rc=0 &gt;&gt;<br>http://127.0.0.1:2379 is healthy: successfully committed proposal: took = 3.491516ms<br></code></pre></td></tr></table></figure>

<h5 id="3-查看集群-leader"><a href="#3-查看集群-leader" class="headerlink" title="3. 查看集群 leader"></a>3. 查看集群 leader</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ETCDCTL_API=3 /data/applications/etcd/bin/etcdctl -w table  --endpoints=https://192.168.3.231:2379,https://192.168.3.232:2379,https://192.168.3.233:2379 --cacert=/data/applications/etcd/ssl/ca.pem --cert=/data/applications/etcd/ssl/server.pem --key=/data/applications/etcd/ssl/server-key.pem endpoint status<br></code></pre></td></tr></table></figure>

<h6 id="正常返回结果-1"><a href="#正常返回结果-1" class="headerlink" title="正常返回结果:"></a>正常返回结果:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 当前 etcd raft leader 为 192.168.3.231</span><br><br>+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+<br>|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |<br>+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+<br>| https://192.168.3.231:2379 | e7d52edc2101ab48 |  3.4.14 |   20 kB |      true |      false |         5 |         28 |                 28 |        |<br>| https://192.168.3.232:2379 | a191bc097f41b735 |  3.4.14 |   20 kB |     false |      false |         5 |         28 |                 28 |        |<br>| https://192.168.3.233:2379 | 307ec828b6239718 |  3.4.14 |   20 kB |     false |      false |         5 |         28 |                 28 |        |<br>+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+<br></code></pre></td></tr></table></figure>





<h3 id="4、部署四层代理-HA-环境"><a href="#4、部署四层代理-HA-环境" class="headerlink" title="4、部署四层代理 HA 环境"></a>4、部署四层代理 HA 环境</h3><blockquote>
<p>🚩 <strong>用 nginx 4 层透明代理来做负载均衡的功能</strong></p>
<p>搭建nginx+keepalived环境，对外提供一个统一的vip地址，后端对接多个 apiserver 实例，nginx 对它们做健康检查和负载均衡。这里我们使用的是 openresty，其实质仍然是 nginx，只是封装了 lua。<code>kubelet</code>、<code>kube-proxy</code>、<code>controller-manager</code>、<code>scheduler</code> 通过vip地址访问 kube-apiserver，从而实现 kube-apiserver 的高可用；</p>
<p>master 节点的 <code>kube-controller-manager</code>、<code>kube-schedule</code>r 是多实例(3个)部署，自身即存在高可用机制，所以只要有一个实例正常，就可以保证高可用。</p>
</blockquote>
<h4 id="1）安装-openresty"><a href="#1）安装-openresty" class="headerlink" title="1）安装 openresty"></a>1）安装 openresty</h4><blockquote>
<p>🚩 以下操作在所有 lb 节点进行</p>
</blockquote>
<h5 id="1-安装依赖库"><a href="#1-安装依赖库" class="headerlink" title="1. 安装依赖库"></a>1. 安装依赖库</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum install -y curl git gcc glibc gcc-c++ openssl-devel pcre-devel yum-utils<br></code></pre></td></tr></table></figure>

<h5 id="2-安装openresty以及openssl-依赖"><a href="#2-安装openresty以及openssl-依赖" class="headerlink" title="2. 安装openresty以及openssl 依赖"></a>2. 安装openresty以及openssl 依赖</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo &amp;&amp; \<br>yum install -y openresty openresty-openssl111-devel<br></code></pre></td></tr></table></figure>

<h5 id="3-软链至标准目录"><a href="#3-软链至标准目录" class="headerlink" title="3. 软链至标准目录"></a>3. 软链至标准目录</h5><p>使用yum安装，默认安装路径在<code>/usr/local/openresty</code> ，将其软链到标准目录 <code>/data/applications</code>，方便统一管理</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ln -s /usr/local/openresty /data/applications<br></code></pre></td></tr></table></figure>

<p>为了使用习惯，可以将 openresty 别名为 nginx</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ln -s /usr/bin/openresty /usr/bin/nginx<br></code></pre></td></tr></table></figure>

<h5 id="4-文件句柄优化"><a href="#4-文件句柄优化" class="headerlink" title="4. 文件句柄优化"></a>4. 文件句柄优化</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ulimit -n 65525<br></code></pre></td></tr></table></figure>

<p>编辑文件 <code>/etc/security/limits.conf </code>，追加以下内容:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 文件底部添加下面四行内容</span><br>* soft nofile 65525<br>* hard nofile 65525<br>* soft nproc 65525<br>* hard nproc 65525<br></code></pre></td></tr></table></figure>



<h4 id="2）创建-nginx-stream-配置文件"><a href="#2）创建-nginx-stream-配置文件" class="headerlink" title="2）创建 nginx stream 配置文件"></a>2）创建 nginx stream 配置文件</h4><blockquote>
<p>🚩 以下操作在 k8s-master01 进行</p>
</blockquote>
<h5 id="1-编辑配置文件"><a href="#1-编辑配置文件" class="headerlink" title="1. 编辑配置文件"></a>1. 编辑配置文件</h5><p>新建文件 <code>~/kubernetes/nginx/cfg/kube-nginx.conf</code> ，文件内容如下:</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">worker_processes</span> <span class="hljs-number">2</span>;<br> <br><span class="hljs-section">events</span> &#123;<br>    <span class="hljs-attribute">worker_connections</span>  <span class="hljs-number">65525</span>;<br>&#125;<br> <br><span class="hljs-section">stream</span> &#123;<br>    <span class="hljs-attribute">upstream</span> apiserver &#123;<br>        <span class="hljs-attribute">hash</span> $remote_addr consistent;<br>        <span class="hljs-attribute">server</span> <span class="hljs-number">172.16.60.231:6443</span>        max_fails=<span class="hljs-number">3</span> fail_timeout=<span class="hljs-number">30s</span>;<br>        <span class="hljs-attribute">server</span> <span class="hljs-number">172.16.60.232:6443</span>        max_fails=<span class="hljs-number">3</span> fail_timeout=<span class="hljs-number">30s</span>;<br>        <span class="hljs-attribute">server</span> <span class="hljs-number">172.16.60.233:6443</span>        max_fails=<span class="hljs-number">3</span> fail_timeout=<span class="hljs-number">30s</span>;<br>    &#125;<br> <br>    <span class="hljs-section">server</span> &#123;<br>        <span class="hljs-attribute">listen</span> <span class="hljs-number">8443</span>;<br>        <span class="hljs-attribute">proxy_connect_timeout</span> <span class="hljs-number">1s</span>;<br>        <span class="hljs-attribute">proxy_pass</span> apiserver;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h5 id="2-分发配置文件"><a href="#2-分发配置文件" class="headerlink" title="2. 分发配置文件"></a>2. 分发配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-nginx -S -R root -m copy -a &quot;src=~/kubernetes/nginx/cfg/kube-nginx.conf dest=/data/applications/openresty/nginx/conf/kube-nginx.conf owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h4 id="3）创建-nginx-systemd-unit-文件"><a href="#3）创建-nginx-systemd-unit-文件" class="headerlink" title="3）创建 nginx systemd unit 文件"></a>3）创建 nginx systemd unit 文件</h4><blockquote>
<p>🚩 以下操作在 k8s-master01 进行</p>
</blockquote>
<h5 id="1-编辑配置文件-1"><a href="#1-编辑配置文件-1" class="headerlink" title="1. 编辑配置文件"></a>1. 编辑配置文件</h5><p>新建文件 <code>~/kubernetes/nginx/systemd/kube-nginx.service</code> ，文件内容如下:</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs nginx">[Unit]<br>Description=kube-<span class="hljs-attribute">apiserver</span> nginx proxy<br>After=network.target<br>After=network-online.target<br>Wants=network-online.target<br> <br>[Service]<br>Type=forking<br>ExecStartPre=/usr/bin/nginx -c /data/applications/openresty/nginx/conf/kube-nginx.conf -p /data/applications/openresty/nginx -t<br>ExecStart=/usr/bin/nginx -c /data/applications/openresty/nginx/conf/kube-nginx.conf -p /data/applications/openresty/nginx<br>ExecReload=/usr/bin/nginx -c data/applications/openresty/nginx/conf/kube-nginx.conf -p /data/applications/openresty/nginx -s reload<br>PrivateTmp=<span class="hljs-literal">true</span><br>Restart=always<br>RestartSec=<span class="hljs-number">5</span><br>StartLimitInterval=<span class="hljs-number">0</span><br>LimitNOFILE=<span class="hljs-number">65536</span><br> <br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure>

<h5 id="2-分发配置文件-1"><a href="#2-分发配置文件-1" class="headerlink" title="2. 分发配置文件"></a>2. 分发配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-nginx -S -R root -m copy -a &quot;src=~/kubernetes/nginx/systemd/kube-nginx.service dest=/usr/lib/systemd/system/kube-nginx.service owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h4 id="4）启动-nginx-代理"><a href="#4）启动-nginx-代理" class="headerlink" title="4）启动 nginx 代理"></a>4）启动 nginx 代理</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-nginx -S -R root -m shell -a &quot;systemctl daemon-reload&quot;<br>ansible k8s-nginx -S -R root -m shell -a &quot;systemctl enable kube-nginx.service&quot;<br>ansible k8s-nginx -S -R root -m shell -a &quot;systemctl start kube-nginx.service&quot;<br>ansible k8s-nginx -S -R root -m shell -a &quot;systemctl status kube-nginx.service&quot;<br></code></pre></td></tr></table></figure>



<h4 id="5）安装-keepalived"><a href="#5）安装-keepalived" class="headerlink" title="5）安装 keepalived"></a>5）安装 keepalived</h4><blockquote>
<p>🚩 以下操作在所有 lb 节点进行</p>
</blockquote>
<h5 id="1-配置内核参数"><a href="#1-配置内核参数" class="headerlink" title="1. 配置内核参数"></a>1. 配置内核参数</h5><p>编辑文件 <code>/etc/sysctl.d/kubernetes.conf </code>，追加以下内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">net.ipv4.ip_nonlocal_bind = 1					# 开启允许绑定非本机IP<br>net.ipv4.conf.lo.arp_announce = 2			# arp 相关配置(lvs 的 DR、TUN 模式需要)<br>net.ipv4.conf.all.arp_announce = 2<br>net.ipv4.conf.lo.arp_ignore = 1<br>net.ipv4.conf.all.arp_ignore = 1<br></code></pre></td></tr></table></figure>

<h6 id="加载配置"><a href="#加载配置" class="headerlink" title="加载配置:"></a>加载配置:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sysctl -p /etc/sysctl.d/kubernetes.conf <br></code></pre></td></tr></table></figure>

<h5 id="2-部署-keepalived"><a href="#2-部署-keepalived" class="headerlink" title="2. 部署 keepalived"></a>2. 部署 keepalived</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">yum install -y keepalived<br></code></pre></td></tr></table></figure>

<h5 id="3-配置-keepalived-独立日志"><a href="#3-配置-keepalived-独立日志" class="headerlink" title="3. 配置 keepalived 独立日志"></a>3. 配置 keepalived 独立日志</h5><p>文件路径: <code>/etc/sysconfig/keepalived</code>，修改 <code>KEEPALIVED_OPTIONS</code> 配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i &#x27;s/&quot;-D&quot;/&quot;-D -d -S 0&quot;/g&#x27; /etc/sysconfig/keepalived <br></code></pre></td></tr></table></figure>

<p>文件路径: <code>/etc/rsyslog.conf </code>，末尾追加配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">echo &quot;local0.*                                                 /var/log/keepalived.log&quot; &gt;&gt; /etc/rsyslog.conf<br></code></pre></td></tr></table></figure>

<h5 id="4-配置-keepalived-日志切割"><a href="#4-配置-keepalived-日志切割" class="headerlink" title="4. 配置 keepalived 日志切割"></a>4. 配置 keepalived 日志切割</h5><p>文件路径: <code>/etc/logrotate.d/keepalived</code>，编辑以下内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">/var/log/keepalived.log &#123;<br>    daily<br>    missingok<br>    dateext<br>    rotate 7<br>    create 0600 root root<br>&#125;<br></code></pre></td></tr></table></figure>

<h5 id="5-重启-rsyslog-服务"><a href="#5-重启-rsyslog-服务" class="headerlink" title="5. 重启 rsyslog 服务"></a>5. 重启 rsyslog 服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl restart rsyslog.service <br></code></pre></td></tr></table></figure>



<h4 id="6）创建-keepalived-配置文件"><a href="#6）创建-keepalived-配置文件" class="headerlink" title="6）创建 keepalived 配置文件"></a>6）创建 keepalived 配置文件</h4><h5 id="1-编辑-master-配置文件"><a href="#1-编辑-master-配置文件" class="headerlink" title="1. 编辑 master 配置文件"></a>1. 编辑 master 配置文件</h5><p>新建文件 <code>~/kubernetes/nginx/cfg/keepalived-master.conf</code> ，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs shell">! Configuration File for keepalived    <br>   <br>global_defs &#123;<br>		router_id master<br>		<br>    script_user root<br>    enable_script_security  <br>&#125;<br>   <br>vrrp_script chk_http_port &#123;     <br>    script &quot;/etc/keepalived/chk_nginx.sh&quot; <br>    interval 2                  <br>    weight -5                  <br>    fall 2              <br>    rise 1                 <br>&#125;<br>   <br>vrrp_instance VI_1 &#123;   <br>    state MASTER   <br>    interface eth0     <br>    virtual_router_id 51        <br>    priority 101               <br>    advert_int 1                <br>    authentication &#123;           <br>        auth_type PASS         <br>        auth_pass kubernetes         <br>    &#125;<br>    <br>    virtual_ipaddress &#123;       <br>        192.168.3.230 dev eth0 label eth0:1<br>    &#125;<br>  <br>		track_script &#123;                     <br>		   chk_http_port                   <br>		&#125;<br>&#125;<br></code></pre></td></tr></table></figure>



<h5 id="2-编辑-backup-配置文件"><a href="#2-编辑-backup-配置文件" class="headerlink" title="2. 编辑 backup 配置文件"></a>2. 编辑 backup 配置文件</h5><p>新建文件 <code>~/kubernetes/nginx/cfg/keepalived-backup.conf</code> ，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs shell">! Configuration File for keepalived    <br>   <br>global_defs &#123;<br>		router_id backup<br>		<br>    script_user root<br>    enable_script_security  <br>&#125;<br>   <br>vrrp_script chk_http_port &#123;     <br>    script &quot;/etc/keepalived/chk_nginx.sh&quot; <br>    interval 2                  <br>    weight -5                  <br>    fall 2              <br>    rise 1                 <br>&#125;<br>   <br>vrrp_instance VI_1 &#123;   <br>    state BACKUP   <br>    interface eth0     <br>    virtual_router_id 50     <br>    priority 99               <br>    advert_int 1                <br>    authentication &#123;           <br>        auth_type PASS         <br>        auth_pass kubernetes         <br>    &#125;<br>    <br>    virtual_ipaddress &#123;       <br>        192.168.3.230 dev eth0 label eth0:1<br>    &#125;<br>  <br>		track_script &#123;                     <br>		   chk_http_port                   <br>		&#125;<br>&#125;<br></code></pre></td></tr></table></figure>



<h5 id="3-编辑-nginx-监控脚本"><a href="#3-编辑-nginx-监控脚本" class="headerlink" title="3. 编辑 nginx 监控脚本"></a>3. 编辑 nginx 监控脚本</h5><p>新建文件 <code>~/kubernetes/nginx/cfg/chk_nginx.sh</code> ，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">!/bin/bash</span><br>counter=$(ps -ef|grep -w kube-nginx|grep -v grep|wc -l)<br>if [ &quot;$&#123;counter&#125;&quot; = &quot;0&quot; ]; then<br>    systemctl restart kube-nginx<br>    sleep 2<br>    counter=$(ps -ef|grep kube-nginx|grep -v grep|wc -l)<br>    if [ &quot;$&#123;counter&#125;&quot; = &quot;0&quot; ]; then<br>        systemctl stop keepalived<br>    fi<br>fi<br></code></pre></td></tr></table></figure>



<h5 id="4-分发配置文件"><a href="#4-分发配置文件" class="headerlink" title="4. 分发配置文件"></a>4. 分发配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> master 配置文件分发到 lb01</span><br>ansible k8s-lb01 -S -R root -m copy -a &quot;src=~/kubernetes/nginx/cfg/keepalived-master.conf dest=/etc/keepalived/keepalived.conf owner=root group=root mode=0644&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> backend 配置文件分发到 lb02</span><br>ansible k8s-lb02 -S -R root -m copy -a &quot;src=~/kubernetes/nginx/cfg/keepalived-backup.conf dest=/etc/keepalived/keepalived.conf owner=root group=root mode=0644&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 监控脚本分发到 2个节点</span><br>ansible k8s-nginx -S -R root -m copy -a &quot;src=~/kubernetes/nginx/cfg/chk_nginx.sh dest=/etc/keepalived/chk_nginx.sh owner=root group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>





<h4 id="7）创建-keepalived-systemd-unit-配置文件"><a href="#7）创建-keepalived-systemd-unit-配置文件" class="headerlink" title="7）创建 keepalived systemd unit 配置文件"></a>7）创建 keepalived systemd unit 配置文件</h4><h5 id="1-编辑配置文件-2"><a href="#1-编辑配置文件-2" class="headerlink" title="1. 编辑配置文件"></a>1. 编辑配置文件</h5><p>新建文件 <code>~/kubernetes/nginx/systemd/keepalived.service</code> ，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell">[Unit]<br>Description=LVS and VRRP High Availability Monitor<br>After=syslog.target network-online.target<br><br>[Service]<br>Type=forking<br>PIDFile=/var/run/keepalived.pid<br>KillMode=process<br>EnvironmentFile=-/etc/sysconfig/keepalived<br>ExecStart=/usr/sbin/keepalived $KEEPALIVED_OPTIONS<br>ExecReload=/bin/kill -HUP $MAINPID<br><br>Restart=on-failure<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure>

<h5 id="2-分发配置文件-2"><a href="#2-分发配置文件-2" class="headerlink" title="2. 分发配置文件"></a>2. 分发配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-nginx -S -R root -m copy -a &quot;src=~/kubernetes/nginx/systemd/keepalived.service dest=/usr/lib/systemd/system/keepalived.service owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h4 id="8）启动-keepalived-HA"><a href="#8）启动-keepalived-HA" class="headerlink" title="8）启动 keepalived HA"></a>8）启动 keepalived HA</h4><h5 id="1-批量启动并加入开机自启"><a href="#1-批量启动并加入开机自启" class="headerlink" title="1. 批量启动并加入开机自启"></a>1. 批量启动并加入开机自启</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-nginx -S -R root -m shell -a &quot;systemctl daemon-reload&quot;<br>ansible k8s-nginx -S -R root -m shell -a &quot;systemctl enable keepalived.service&quot;<br>ansible k8s-nginx -S -R root -m shell -a &quot;systemctl start keepalived.service&quot;<br>ansible k8s-nginx -S -R root -m shell -a &quot;systemctl status keepalived.service&quot;<br></code></pre></td></tr></table></figure>

<h5 id="2-验证-vip-状态"><a href="#2-验证-vip-状态" class="headerlink" title="2. 验证 vip 状态"></a>2. 验证 vip 状态</h5><blockquote>
<p>🚩 由于 master 节点的的权重高，所以 vip 默认会起在 master 节点上，在 k8s-lb01 执行如下命令:</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ifconfig <br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500<br>        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255<br>        ether 02:42:86:cf:0e:ee  txqueuelen 0  (Ethernet)<br>        RX packets 0  bytes 0 (0.0 B)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 0  bytes 0 (0.0 B)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br>eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 192.168.3.235  netmask 255.255.255.0  broadcast 192.168.3.255<br>        ether 00:50:56:b0:b7:c1  txqueuelen 1000  (Ethernet)<br>        RX packets 4501804  bytes 410256599 (391.2 MiB)<br>        RX errors 0  dropped 259641  overruns 0  frame 0<br>        TX packets 51487  bytes 4490345 (4.2 MiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br>eth0:1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 192.168.3.230  netmask 255.255.255.255  broadcast 0.0.0.0<br>        ether 00:50:56:b0:b7:c1  txqueuelen 1000  (Ethernet)<br><br>lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536<br>        inet 127.0.0.1  netmask 255.0.0.0<br>        loop  txqueuelen 1000  (Local Loopback)<br>        RX packets 0  bytes 0 (0.0 B)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 0  bytes 0 (0.0 B)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><br></code></pre></td></tr></table></figure>

<blockquote>
<p>由于我们给 vip 绑定做了别名，故这边可以看到 <code>eth0:1</code> 的虚拟网卡，证明 vip 的确起在 master 节点上了</p>
</blockquote>
<h3 id="5、部署-Kubernetes-master-节点"><a href="#5、部署-Kubernetes-master-节点" class="headerlink" title="5、部署 Kubernetes master 节点"></a>5、部署 Kubernetes master 节点</h3><blockquote>
<p>🌈 <strong>关于 master 节点</strong></p>
<p>部署在 master 节点的必要组件有 <code>kube-apiserver</code>、<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> ，均为多实例模式运行。</p>
</blockquote>
<h4 id="1）部署-kube-apiserver"><a href="#1）部署-kube-apiserver" class="headerlink" title="1）部署 kube-apiserver"></a>1）部署 kube-apiserver</h4><p><strong>基础组件介绍 —— kube-apiserver:</strong> <a href="https://tareya.github.io/2021/10/21/Kubernetes-%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-kube-apiserver/">传送门</a></p>
<blockquote>
<p>🚩 这里部署的是一个三实例的 kube-apiserver 集群环境，kube-apiserver 是无状态的，通过 nginx 四层代理进行访问，并对外提供统一的 vip 地址，以此来保证服务的可用性。</p>
<p><strong>以下的操作都是在 k8s-master01 上进行，然后再通过批管理工具分发至其他节点进行统一管理。</strong></p>
</blockquote>
<h5 id="1-创建-TLS-Bootstrap-认证-token-文件"><a href="#1-创建-TLS-Bootstrap-认证-token-文件" class="headerlink" title="1. 创建 TLS Bootstrap 认证 token 文件"></a>1. 创建 TLS Bootstrap 认证 token 文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 生成一个加密token</span><br>export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#x27; &#x27;) <br><span class="hljs-meta">#</span><span class="bash"> 创建 token文件</span><br>cat &gt; ~/kubernetes/server/cfg/token.csv &lt;&lt;EOF<br><span class="hljs-meta">$</span><span class="bash">&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,<span class="hljs-string">&quot;system:node-bootstrap&quot;</span></span><br>EOF<br></code></pre></td></tr></table></figure>

<blockquote>
<p>🌈 token文件的格式为 token,用户,uid,用户组</p>
</blockquote>
<h6 id="分发-token-至各-master-节点"><a href="#分发-token-至各-master-节点" class="headerlink" title="分发 token 至各 master 节点"></a>分发 token 至各 master 节点</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/cfg/token.csv dest=/data/applications/kubernetes/cfg/token.csv owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="2-创建审计策略文件"><a href="#2-创建审计策略文件" class="headerlink" title="2. 创建审计策略文件"></a>2. 创建审计策略文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/kubernetes/server/cfg/audit-policy.yaml &lt;&lt;EOF<br>apiVersion: audit.k8s.io/v1beta1<br>kind: Policy<br>rules:<br><span class="hljs-meta">  #</span><span class="bash"> The following requests were manually identified as high-volume and low-risk, so drop them.</span><br>  - level: None<br>    resources:<br>      - group: &quot;&quot;<br>        resources:<br>          - endpoints<br>          - services<br>          - services/status<br>    users:<br>      - &#x27;system:kube-proxy&#x27;<br>    verbs:<br>      - watch<br>   <br>  - level: None<br>    resources:<br>      - group: &quot;&quot;<br>        resources:<br>          - nodes<br>          - nodes/status<br>    userGroups:<br>      - &#x27;system:nodes&#x27;<br>    verbs:<br>      - get<br>   <br>  - level: None<br>    namespaces:<br>      - kube-system<br>    resources:<br>      - group: &quot;&quot;<br>        resources:<br>          - endpoints<br>    users:<br>      - &#x27;system:kube-controller-manager&#x27;<br>      - &#x27;system:kube-scheduler&#x27;<br>      - &#x27;system:serviceaccount:kube-system:endpoint-controller&#x27;<br>    verbs:<br>      - get<br>      - update<br>   <br>  - level: None<br>    resources:<br>      - group: &quot;&quot;<br>        resources:<br>          - namespaces<br>          - namespaces/status<br>          - namespaces/finalize<br>    users:<br>      - &#x27;system:apiserver&#x27;<br>    verbs:<br>      - get<br>   <br><span class="hljs-meta">  #</span><span class="bash"> Don<span class="hljs-string">&#x27;t log HPA fetching metrics.</span></span><br>  - level: None<br>    resources:<br>      - group: metrics.k8s.io<br>    users:<br>      - &#x27;system:kube-controller-manager&#x27;<br>    verbs:<br>      - get<br>      - list<br>   <br><span class="hljs-meta">  #</span><span class="bash"><span class="hljs-string"> Don&#x27;</span>t <span class="hljs-built_in">log</span> these read-only URLs.</span><br>  - level: None<br>    nonResourceURLs:<br>      - &#x27;/healthz*&#x27;<br>      - /version<br>      - &#x27;/swagger*&#x27;<br>   <br><span class="hljs-meta">  #</span><span class="bash"> Don<span class="hljs-string">&#x27;t log events requests.</span></span><br>  - level: None<br>    resources:<br>      - group: &quot;&quot;<br>        resources:<br>          - events<br>   <br><span class="hljs-meta">  #</span><span class="bash"><span class="hljs-string"> node and pod status calls from nodes are high-volume and can be large, don&#x27;</span>t <span class="hljs-built_in">log</span> responses <span class="hljs-keyword">for</span> expected updates from nodes</span><br>  - level: Request<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: &quot;&quot;<br>        resources:<br>          - nodes/status<br>          - pods/status<br>    users:<br>      - kubelet<br>      - &#x27;system:node-problem-detector&#x27;<br>      - &#x27;system:serviceaccount:kube-system:node-problem-detector&#x27;<br>    verbs:<br>      - update<br>      - patch<br>   <br>  - level: Request<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: &quot;&quot;<br>        resources:<br>          - nodes/status<br>          - pods/status<br>    userGroups:<br>      - &#x27;system:nodes&#x27;<br>    verbs:<br>      - update<br>      - patch<br>   <br><span class="hljs-meta">  #</span><span class="bash"> deletecollection calls can be large, don<span class="hljs-string">&#x27;t log responses for expected namespace deletions</span></span><br>  - level: Request<br>    omitStages:<br>      - RequestReceived<br>    users:<br>      - &#x27;system:serviceaccount:kube-system:namespace-controller&#x27;<br>    verbs:<br>      - deletecollection<br>   <br><span class="hljs-meta">  #</span><span class="bash"><span class="hljs-string"> Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,</span></span><br><span class="hljs-meta">  #</span><span class="bash"><span class="hljs-string"> so only log at the Metadata level.</span></span><br>  - level: Metadata<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: &quot;&quot;<br>        resources:<br>          - secrets<br>          - configmaps<br>      - group: authentication.k8s.io<br>        resources:<br>          - tokenreviews<br><span class="hljs-meta">  #</span><span class="bash"><span class="hljs-string"> Get repsonses can be large; skip them.</span></span><br>  - level: Request<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: &quot;&quot;<br>      - group: admissionregistration.k8s.io<br>      - group: apiextensions.k8s.io<br>      - group: apiregistration.k8s.io<br>      - group: apps<br>      - group: authentication.k8s.io<br>      - group: authorization.k8s.io<br>      - group: autoscaling<br>      - group: batch<br>      - group: certificates.k8s.io<br>      - group: extensions<br>      - group: metrics.k8s.io<br>      - group: networking.k8s.io<br>      - group: policy<br>      - group: rbac.authorization.k8s.io<br>      - group: scheduling.k8s.io<br>      - group: settings.k8s.io<br>      - group: storage.k8s.io<br>    verbs:<br>      - get<br>      - list<br>      - watch<br>   <br><span class="hljs-meta">  #</span><span class="bash"><span class="hljs-string"> Default level for known APIs</span></span><br>  - level: RequestResponse<br>    omitStages:<br>      - RequestReceived<br>    resources:<br>      - group: &quot;&quot;<br>      - group: admissionregistration.k8s.io<br>      - group: apiextensions.k8s.io<br>      - group: apiregistration.k8s.io<br>      - group: apps<br>      - group: authentication.k8s.io<br>      - group: authorization.k8s.io<br>      - group: autoscaling<br>      - group: batch<br>      - group: certificates.k8s.io<br>      - group: extensions<br>      - group: metrics.k8s.io<br>      - group: networking.k8s.io<br>      - group: policy<br>      - group: rbac.authorization.k8s.io<br>      - group: scheduling.k8s.io<br>      - group: settings.k8s.io<br>      - group: storage.k8s.io<br>         <br><span class="hljs-meta">  #</span><span class="bash"><span class="hljs-string"> Default level for all other requests.</span></span><br>  - level: Metadata<br>    omitStages:<br>      - RequestReceived<br>EOF<br></code></pre></td></tr></table></figure>

<h6 id="分发审计策略文件"><a href="#分发审计策略文件" class="headerlink" title="分发审计策略文件:"></a>分发审计策略文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/cfg/audit-policy.yaml dest=/data/applications/kubernetes/cfg/audit-policy.yaml owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="3-创建-kube-apiserver-配置文件"><a href="#3-创建-kube-apiserver-配置文件" class="headerlink" title="3. 创建 kube-apiserver 配置文件"></a>3. 创建 kube-apiserver 配置文件</h5><p>新建文件 <code>~/kubernetes/server/cfg/kube-apiserver.conf</code>，文件内容如下: (节点IP根据实际情况替换)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs shell">KUBE_APISERVER_OPTS=&quot;--advertise-address=192.168.3.231 \<br>--bind-address=192.168.3.231 \<br>--secure-port=6443 \<br>--insecure-port=8080 \<br>--service-cluster-ip-range=10.0.0.0/16 \<br>--service-node-port-range=30000-50000 \<br>--default-not-ready-toleration-seconds=360 \<br>--default-unreachable-toleration-seconds=360 \<br>--feature-gates=DynamicAuditing=true \<br>--max-mutating-requests-inflight=2000 \<br>--max-requests-inflight=4000 \<br>--default-watch-cache-size=200 \<br>--delete-collection-workers=2 \<br>--etcd-cafile=/data/applications/etcd/ssl/ca.pem \<br>--etcd-certfile=/data/applications/etcd/ssl/server.pem \<br>--etcd-keyfile=/data/applications/etcd/ssl/server-key.pem \<br>--etcd-servers=https://192.168.3.231:2379,https://192.168.3.232:2379,https://192.168.3.233:2379 \<br>--tls-cert-file=/data/applications/kubernetes/ssl/server.pem  \<br>--tls-private-key-file=/data/applications/kubernetes/ssl/server-key.pem \<br>--client-ca-file=/data/applications/kubernetes/ssl/ca.pem \<br>--service-account-key-file=/data/applications/kubernetes/ssl/ca-key.pem \<br>--audit-log-maxage=30 \<br>--audit-log-maxbackup=3 \<br>--audit-log-maxsize=100 \<br>--audit-log-mode=batch \<br>--audit-log-truncate-enabled \<br>--audit-log-batch-buffer-size=20000 \<br>--audit-log-batch-max-size=2 \<br>--audit-log-path=/data/applications/kubernetes/logs/k8s-audit.log \<br>--audit-policy-file=/data/applications/kubernetes/cfg/audit-policy.yaml \<br>--profiling \<br>--enable-bootstrap-token-auth=true \<br>--token-auth-file=/data/applications/kubernetes/cfg/token.csv \<br>--authorization-mode=RBAC,Node \<br>--runtime-config=api/all=true \<br>--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \<br>--allow-privileged=true \<br>--apiserver-count=3 \<br>--event-ttl=168h \<br>--kubelet-certificate-authority=/data/applications/kubernetes/ssl/ca.pem \<br>--kubelet-client-certificate=/data/applications/kubernetes/ssl/server.pem \<br>--kubelet-client-key=/data/applications/kubernetes/ssl/server-key.pem \<br>--kubelet-https=true \<br>--kubelet-timeout=10s \<br>--proxy-client-cert-file=/data/applications/kubernetes/ssl/metrics-server.pem \<br>--proxy-client-key-file=/data/applications/kubernetes/ssl/metrics-server-key.pem \<br>--logtostderr=true \<br>--log-dir=/data/applications/kubernetes/logs \<br>--v=2 \<br>--enable-aggregator-routing=true&quot;<br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明-5"><a href="#配置参数说明-5" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><table>
<thead>
<tr>
<th>配置参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h6 id="分发配置文件"><a href="#分发配置文件" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/cfg/kube-apiserver.conf dest=/data/applications/kubernetes/cfg/kube-apiserver.conf owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>

<blockquote>
<p>⚠️ <strong>注意</strong></p>
<p>分发后要修改对应节点的 <code>advertise-address</code> 和 <code>bind-address</code></p>
</blockquote>
<h5 id="4-创建-systemd-unit-文件"><a href="#4-创建-systemd-unit-文件" class="headerlink" title="4. 创建 systemd unit 文件"></a>4. 创建 systemd unit 文件</h5><p>新建文件 <code>~/kubernetes/server/systemd/kube-apiserver.service</code>，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">[Unit]<br>Description=Kubernetes API Server<br>Documentation=https://github.com/kubernetes/kubernetes<br>After=network.target<br>After=etcd.service<br>Wants=etcd.service<br><br>[Service]<br>EnvironmentFile=/data/application/kubernetes/cfg/kube-apiserver.conf<br>ExecStart=/data/applications/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTS<br>Restart=on-failure<br>RestartSec=10<br>Type=notify<br>LimitNOFILE=65536<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure>

<h6 id="分发-systemd-文件"><a href="#分发-systemd-文件" class="headerlink" title="分发 systemd 文件:"></a>分发 systemd 文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/systemd/kube-apiserver.service dest=/usr/lib/systemd/system/kube-apiserver.service owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="5-启动-kube-apiserver-集群"><a href="#5-启动-kube-apiserver-集群" class="headerlink" title="5. 启动 kube-apiserver 集群"></a>5. 启动 kube-apiserver 集群</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m shell -a &quot;systemctl daemon-reload&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl enable kube-apiserver.service&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl start kube-apiserver.service&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl status kube-apiserver.service&quot;<br></code></pre></td></tr></table></figure>

<h5 id="6-验证kube-apiserver-集群状态"><a href="#6-验证kube-apiserver-集群状态" class="headerlink" title="6. 验证kube-apiserver 集群状态"></a>6. 验证kube-apiserver 集群状态</h5><h6 id="1⃣️-查看集群信息"><a href="#1⃣️-查看集群信息" class="headerlink" title="1⃣️ 查看集群信息:"></a>1⃣️ 查看集群信息:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl cluster-info<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">Kubernetes master is running at https://192.168.3.230:8443<br><br>To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;.<br></code></pre></td></tr></table></figure>

<h6 id="2⃣️-查看集群所有内容"><a href="#2⃣️-查看集群所有内容" class="headerlink" title="2⃣️ 查看集群所有内容:"></a>2⃣️ 查看集群所有内容:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get all --all-namespaces<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAMESPACE   NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE<br>default     service/kubernetes   ClusterIP   10.0.0.1     &lt;none&gt;        443/TCP   49m<br></code></pre></td></tr></table></figure>

<h6 id="3⃣️-查看集群状态信息"><a href="#3⃣️-查看集群状态信息" class="headerlink" title="3⃣️ 查看集群状态信息:"></a>3⃣️ 查看集群状态信息:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get componentstatuses # 或执行 kubectl get cs<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME                 STATUS      MESSAGE                                                                                     ERROR<br>scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused   <br>controller-manager   Unhealthy   Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused   <br>etcd-0               Healthy     &#123;&quot;health&quot;:&quot;true&quot;&#125;                                                                           <br>etcd-2               Healthy     &#123;&quot;health&quot;:&quot;true&quot;&#125;                                                                           <br>etcd-1               Healthy     &#123;&quot;health&quot;:&quot;true&quot;&#125;    <br></code></pre></td></tr></table></figure>

<blockquote>
<p>🚩 此时还没有部署 <code>controller-managerhe</code> 和 <code>schedule</code> 这两个组件，所以状态为 Unhealthy，待后续部署好之后再查看。</p>
</blockquote>
<h5 id="7-查看-kube-apiserver-写入-etcd-中的数据"><a href="#7-查看-kube-apiserver-写入-etcd-中的数据" class="headerlink" title="7. 查看 kube-apiserver 写入 etcd 中的数据"></a>7. 查看 kube-apiserver 写入 etcd 中的数据</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">ETCDCTL_API=3 /data/applications/etcd/bin/etcdctl \<br>--endpoints=https://192.168.3.231:2379,https://192.168.3.232:2379,https://192.168.3.233:2379 \<br>--cacert=/data/applications/etcd/ssl/ca.pem \<br>--cert=/data/applications/etcd/ssl/server.pem \<br>--key=/data/applications/etcd/ssl/server-key.pem \<br>get /registry/ --prefix --keys-only<br></code></pre></td></tr></table></figure>



<h5 id="8-授权-kube-apiserver-访问-kubelet-API-的权限"><a href="#8-授权-kube-apiserver-访问-kubelet-API-的权限" class="headerlink" title="8. 授权 kube-apiserver 访问 kubelet API 的权限"></a>8. 授权 kube-apiserver 访问 kubelet API 的权限</h5><blockquote>
<p>🚩 在执行 kubectl exec、run、logs 等命令时，apiserver 会将请求转发到 kubelet 的 https 端口，因此在这边定义 RBAC 规则，授权 apiserver 使用证书（server.pem）用户名（kubernetes）访问 kubelet API 的权限。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes<br></code></pre></td></tr></table></figure>



<h4 id="2）部署-kube-controller-manager"><a href="#2）部署-kube-controller-manager" class="headerlink" title="2）部署 kube-controller-manager"></a>2）部署 kube-controller-manager</h4><p><strong>基础组件介绍 —— kube-controller-manager:</strong> <a href="https://tareya.github.io/2021/10/21/Kubernetes-%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-kube-controller-manager/">传送门</a></p>
<blockquote>
<p>🚩 这里部署的是一个三实例的 kube-controller-manager 集群，启动后会通过选举机制产生一个 leader（类似 consul 或 etcd的机制），其他节点处于堵塞状态，当 leader 节点不可用时，剩余的堵塞节点会再次选举产生新的 leader，从热保证服务的高可用。</p>
<p><strong>以下的操作都是在 k8s-master01 上进行，然后再通过批管理工具分发至其他节点进行统一管理。</strong></p>
</blockquote>
<h5 id="1-创建-kube-controller-manager-配置文件"><a href="#1-创建-kube-controller-manager-配置文件" class="headerlink" title="1. 创建 kube-controller-manager 配置文件"></a>1. 创建 kube-controller-manager 配置文件</h5><p>新建文件 <code>~/kubernetes/server/cfg/kube-controller-manager.conf</code>，文件内容如下: </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs shell">KUBE_CONTROLLER_MANAGER_OPTS=&quot;--v=2 \<br>--logtostderr=true \<br>--log-dir=/data/applications/kubernetes/logs \<br>--profiling \<br>--cluster-name=kubernetes \<br>--bind-address=127.0.0.1 \<br>--master=127.0.0.1:8080 \<br>--service-cluster-ip-range=10.0.0.0/16 \<br>--cluster-cidr=10.244.0.0/16 \<br>--allocate-node-cidrs=true \<br>--cluster-signing-cert-file=/data/applications/kubernetes/ssl/ca.pem \<br>--cluster-signing-key-file=/data/applications/kubernetes/ssl/ca-key.pem  \<br>--service-account-private-key-file=/data/applications/kubernetes/ssl/ca-key.pem \<br>--root-ca-file=/data/applications/kubernetes/ssl/ca.pem \<br>--controllers=*,bootstrapsigner,tokencleaner \<br>--kube-api-qps=1000 \<br>--kube-api-burst=2000 \<br>--leader-elect=true \<br>--use-service-account-credentials=true \<br>--horizontal-pod-autoscaler-sync-period=10s \<br>--concurrent-service-syncs=2 \<br>--concurrent-deployment-syncs=10 \<br>--concurrent-gc-syncs=30 \<br>--node-cidr-mask-size=24 \<br>--pod-eviction-timeout=6m \<br>--terminated-pod-gc-threshold=10000 \<br>--experimental-cluster-signing-duration=87600h0m0s \<br>--feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true&quot;<br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明-6"><a href="#配置参数说明-6" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h6 id="分发配置文件："><a href="#分发配置文件：" class="headerlink" title="分发配置文件："></a>分发配置文件：</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/cfg/kube-controller-manager.conf dest=/data/applications/kubernetes/cfg/kube-controller-manager.conf owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="2-创建-systemd-unit-文件"><a href="#2-创建-systemd-unit-文件" class="headerlink" title="2. 创建 systemd unit 文件"></a>2. 创建 systemd unit 文件</h5><p>新建文件 <code>~/kubernetes/server/systemd/kube-controller-manager.service</code>，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">[Unit]<br>Description=Kubernetes Controller Manager<br>Documentation=https://github.com/kubernetes/kubernetes<br><br>[Service]<br>EnvironmentFile=/data/applications/kubernetes/cfg/kube-controller-manager.conf<br>ExecStart=/data/applications/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTS<br>Restart=on-failure<br>RestartSec=5<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure>

<h6 id="分发配置文件-1"><a href="#分发配置文件-1" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/systemd/kube-controller-manager.service dest=/usr/lib/systemd/system/kube-controller-manager.service owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="3-启动-kube-controller-manager-集群"><a href="#3-启动-kube-controller-manager-集群" class="headerlink" title="3. 启动 kube-controller-manager 集群"></a>3. 启动 kube-controller-manager 集群</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m shell -a &quot;systemctl daemon-reload&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl enable kube-controller-manager.service&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl start kube-controller-manager.service&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl status kube-controller-manager.service&quot;<br></code></pre></td></tr></table></figure>



<h5 id="4-验证-kube-controller-manager-集群状态"><a href="#4-验证-kube-controller-manager-集群状态" class="headerlink" title="4. 验证 kube-controller-manager 集群状态"></a>4. 验证 kube-controller-manager 集群状态</h5><h6 id="1⃣️-查看集群状态信息"><a href="#1⃣️-查看集群状态信息" class="headerlink" title="1⃣️ 查看集群状态信息:"></a>1⃣️ 查看集群状态信息:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get componentstatuses # 或执行 kubectl get cs<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME                 STATUS      MESSAGE                                                                                     ERROR<br>scheduler            Unhealthy   Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused   <br>controller-manager   Healthy     ok                                                                                          <br>etcd-0               Healthy     &#123;&quot;health&quot;:&quot;true&quot;&#125;                                                                           <br>etcd-1               Healthy     &#123;&quot;health&quot;:&quot;true&quot;&#125;                                                                           <br>etcd-2               Healthy     &#123;&quot;health&quot;:&quot;true&quot;&#125; <br></code></pre></td></tr></table></figure>

<blockquote>
<p>🚩 检查集群状态，controller-manager的状态为”ok”</p>
<p>当 <code>kube-controller-manager</code> 集群中的1个或2个节点的 <code>controller-manager</code> 服务挂掉，只要有一个节点的 <code>controller-manager</code> 服务活着，则集群中 <code>controller-manager</code> 的状态仍然为 <strong>“ok”</strong> ,仍然会继续提供服务！</p>
</blockquote>
<h6 id="2⃣️-查看监听端口"><a href="#2⃣️-查看监听端口" class="headerlink" title="2⃣️ 查看监听端口:"></a>2⃣️ 查看监听端口:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">netstat -lntp | grep kube-controller <br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tcp        0      0 127.0.0.1:10257         0.0.0.0:*               LISTEN      494/kube-controller <br>tcp6       0      0 :::10252                :::*                    LISTEN      494/kube-controller <br></code></pre></td></tr></table></figure>

<blockquote>
<p>🌈 kube-controller-manager 监听端口为 10252 和 10257</p>
<p><code>10257</code>：接收 http 请求，非安全端口，不需要认证授权；<br><code>10252</code>：接收 https 请求，安全端口，需要认证授权；<br>两个接口都对外提供 <code>/metrics</code> 和 <code>/healthz</code> 的访问。</p>
</blockquote>
<h6 id="3⃣️-查看集群当中的-leader"><a href="#3⃣️-查看集群当中的-leader" class="headerlink" title="3⃣️ 查看集群当中的 leader:"></a>3⃣️ 查看集群当中的 leader:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml<br></code></pre></td></tr></table></figure>

<p>返回如下，可见当然 leader 为 <code>k8s-master01</code>:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs shell">apiVersion: v1<br>kind: Endpoints<br>metadata:<br>  annotations:<br>    control-plane.alpha.kubernetes.io/leader: &#x27;&#123;&quot;holderIdentity&quot;:&quot;k8s-master01_a6307360-f814-4a7d-82cc-d4b89928438b&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2021-10-06T13:37:48Z&quot;,&quot;renewTime&quot;:&quot;2021-10-06T13:39:47Z&quot;,&quot;leaderTransitions&quot;:1&#125;&#x27;<br>  creationTimestamp: &quot;2021-10-06T13:11:30Z&quot;<br>  managedFields:<br>  - apiVersion: v1<br>    fieldsType: FieldsV1<br>    fieldsV1:<br>      f:metadata:<br>        f:annotations:<br>          .: &#123;&#125;<br>          f:control-plane.alpha.kubernetes.io/leader: &#123;&#125;<br>    manager: kube-controller-manager<br>    operation: Update<br>    time: &quot;2021-10-06T13:11:30Z&quot;<br>  name: kube-controller-manager<br>  namespace: kube-system<br>  resourceVersion: &quot;7596&quot;<br>  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager<br>  uid: ccc812be-c1cf-4e2c-a1cb-5133198aae47<br></code></pre></td></tr></table></figure>



<h4 id="3）部署-kube-scheduler"><a href="#3）部署-kube-scheduler" class="headerlink" title="3）部署 kube-scheduler"></a>3）部署 kube-scheduler</h4><p><strong>基础组件介绍 —— kube-scheduler:</strong> <a href="https://tareya.github.io/2021/10/21/Kubernetes-%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-kube-scheduler/">传送门</a></p>
<blockquote>
<p>🚩  这里部署的是一个三实例的 kube-scheduler 集群，启动后会通过选举机制产生一个 leader，其他节点处于堵塞状态，当 leader 节点不可用时，剩余的堵塞节点会再次选举产生新的 leader，从热保证服务的高可用。</p>
<p><strong>以下的操作都是在 k8s-master01 上进行，然后再通过批管理工具分发至其他节点进行统一管理。</strong></p>
</blockquote>
<h5 id="1-创建-kube-scheduler-配置文件"><a href="#1-创建-kube-scheduler-配置文件" class="headerlink" title="1. 创建 kube-scheduler 配置文件"></a>1. 创建 kube-scheduler 配置文件</h5><p>新建文件 <code>~/kubernetes/server/cfg/kube-scheduler.conf</code>，文件内容如下: </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">KUBE_SCHEDULER_OPTS=&quot;--master=127.0.0.1:8080 \<br>--address=127.0.0.1 \<br>--leader-elect \<br>--logtostderr=true \<br>--log-dir=/data/applications/kubernetes/logs \<br>--v=2&quot;<br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明-7"><a href="#配置参数说明-7" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h6 id="分发配置文件-2"><a href="#分发配置文件-2" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/cfg/kube-scheduler.conf dest=/data/applications/kubernetes/cfg/kube-scheduler.conf owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="2-创建-systemd-unit-文件-1"><a href="#2-创建-systemd-unit-文件-1" class="headerlink" title="2. 创建 systemd unit 文件"></a>2. 创建 systemd unit 文件</h5><p>新建文件 <code>~/kubernetes/server/systemd/kube-scheduler.service</code>，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">[Unit]<br>Description=Kubernetes Scheduler<br>Documentation=https://github.com/kubernetes/kubernetes<br><br>[Service]<br>EnvironmentFile=/data/applications/kubernetes/cfg/kube-scheduler.conf<br>ExecStart=/data/applications/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTS<br>Restart=on-failure<br>RestartSec=5<br>StartLimitInterval=0<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure>

<h6 id="分发配置文件-3"><a href="#分发配置文件-3" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m copy -a &quot;src=~/kubernetes/server/systemd/kube-scheduler.service dest=/usr/lib/systemd/system/kube-scheduler.service owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="3-启动-kube-scheduler-集群"><a href="#3-启动-kube-scheduler-集群" class="headerlink" title="3. 启动 kube-scheduler 集群"></a>3. 启动 kube-scheduler 集群</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-master -S -R root -m shell -a &quot;systemctl daemon-reload&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl enable kube-scheduler.service&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl start kube-scheduler.service&quot;<br>ansible k8s-master -S -R root -m shell -a &quot;systemctl status kube-scheduler.service&quot;<br></code></pre></td></tr></table></figure>



<h5 id="4-验证-kube-scheduler-集群状态"><a href="#4-验证-kube-scheduler-集群状态" class="headerlink" title="4. 验证 kube-scheduler 集群状态"></a>4. 验证 kube-scheduler 集群状态</h5><h6 id="1⃣️-查看集群状态信息-1"><a href="#1⃣️-查看集群状态信息-1" class="headerlink" title="1⃣️ 查看集群状态信息:"></a>1⃣️ 查看集群状态信息:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get componentstatuses # 或执行 kubectl get cs<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME                 STATUS    MESSAGE             ERROR<br>controller-manager   Healthy   ok                  <br>scheduler            Healthy   ok                  <br>etcd-0               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   <br>etcd-1               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125;   <br>etcd-2               Healthy   &#123;&quot;health&quot;:&quot;true&quot;&#125; <br></code></pre></td></tr></table></figure>

<blockquote>
<p>🚩 检查集群状态，集群的状态就全都为”ok”</p>
<p>当 <code>kube-scheduler </code> 集群中的1个或2个节点的 <code>scheduler </code> 服务挂掉，只要有一个节点的 <code>scheduler </code> 服务活着，则集群中<code>scheduler</code> 的状态仍然为 <strong>“ok”</strong>,仍然会继续提供服务！</p>
</blockquote>
<h6 id="2⃣️-查看监听端口-1"><a href="#2⃣️-查看监听端口-1" class="headerlink" title="2⃣️ 查看监听端口:"></a>2⃣️ 查看监听端口:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">netstat -lntp | grep kube-scheduler<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      2049/kube-scheduler <br>tcp6       0      0 :::10259                :::*                    LISTEN      2049/kube-scheduler cai<br></code></pre></td></tr></table></figure>

<blockquote>
<p>🌈 kube-scheduler 监听端口为 10251 和 10259</p>
<p><code>10251</code>：接收 http 请求，非安全端口，不需要认证授权；<br><code>10259</code>：接收 https 请求，安全端口，需要认证授权；<br>两个接口都对外提供 <code>/metrics</code> 和 <code>/healthz</code> 的访问。</p>
</blockquote>
<h6 id="3⃣️-查看集群当中的-leader-1"><a href="#3⃣️-查看集群当中的-leader-1" class="headerlink" title="3⃣️ 查看集群当中的 leader:"></a>3⃣️ 查看集群当中的 leader:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml<br></code></pre></td></tr></table></figure>

<p>返回如下，可见当然 leader 为 <code>k8s-master03</code>:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs shell">apiVersion: v1<br>kind: Endpoints<br>metadata:<br>  annotations:<br>    control-plane.alpha.kubernetes.io/leader: &#x27;&#123;&quot;holderIdentity&quot;:&quot;k8s-master03_cd78cdca-b2e3-4f36-b2c3-234bb8651d08&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2021-10-07T02:36:24Z&quot;,&quot;renewTime&quot;:&quot;2021-10-07T02:46:33Z&quot;,&quot;leaderTransitions&quot;:0&#125;&#x27;<br>  creationTimestamp: &quot;2021-10-07T02:36:24Z&quot;<br>  managedFields:<br>  - apiVersion: v1<br>    fieldsType: FieldsV1<br>    fieldsV1:<br>      f:metadata:<br>        f:annotations:<br>          .: &#123;&#125;<br>          f:control-plane.alpha.kubernetes.io/leader: &#123;&#125;<br>    manager: kube-scheduler<br>    operation: Update<br>    time: &quot;2021-10-07T02:36:24Z&quot;<br>  name: kube-scheduler<br>  namespace: kube-system<br>  resourceVersion: &quot;69322&quot;<br>  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler<br>  uid: 645f7d7d-6413-4ef4-bd2b-61a5a82adfe8<br></code></pre></td></tr></table></figure>





<h3 id="6、部署-Kubernetes-node-节点"><a href="#6、部署-Kubernetes-node-节点" class="headerlink" title="6、部署 Kubernetes node 节点"></a>6、部署 Kubernetes node 节点</h3><blockquote>
<p>🌈 <strong>关于 node 节点</strong></p>
<p>部署在 master 节点的必要组件有 <code>docker</code>、<code>kubelet</code> 、 <code>kube-proxy</code> 、<code>网络组件</code>，由于考虑到后续 mertics-server、istio 等服务的使用，这里我们将 master 节点也作为 node 加入集群，但是通过污点的方式禁止直接调度。</p>
</blockquote>
<h4 id="1）部署-kubelet"><a href="#1）部署-kubelet" class="headerlink" title="1）部署 kubelet"></a>1）部署 kubelet</h4><p><strong>基础组件介绍 —— kubelet:</strong> <a href="https://tareya.github.io/2021/10/21/Kubernetes-%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-kubelet/">传送门</a></p>
<blockquote>
<p>🚩 kubelet 运行在每个node节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。kubelet 启动时自动向 kube-apiserver 注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况，这里我们在部署时直接开启 TLS bootstrap，以此来自动签发 kubelet 的证书。</p>
<p><strong>以下的操作都是在 k8s-master01 上进行，然后再通过批管理工具分发至其他节点进行统一管理。</strong></p>
</blockquote>
<h5 id="1-创建-kubelet-的-kubeconfig-配置文件"><a href="#1-创建-kubelet-的-kubeconfig-配置文件" class="headerlink" title="1. 创建 kubelet 的 kubeconfig 配置文件"></a>1. 创建 kubelet 的 kubeconfig 配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 设置必要变量</span><br>BOOTSTRAP_TOKEN=$(cat /data/applications/kubernetes/cfg/token.csv | awk -F &#x27;,&#x27; &#x27;&#123;print $1&#125;&#x27;)<br>KUBE_APISERVER=&quot;https://$(hostname -i):6443&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 设置集群参数</span><br>kubectl config set-cluster kubernetes \<br>  --certificate-authority=/data/applications/kubernetes/ssl/ca.pem \<br>  --embed-certs=true \<br>  --server=$&#123;KUBE_APISERVER&#125; \<br>  --kubeconfig=$&#123;HOME&#125;/kubernetes/node/cfg/bootstrap.kubeconfig<br><span class="hljs-meta">  </span><br><span class="hljs-meta">#</span><span class="bash"> 设置客户端认证参数</span><br>kubectl config set-credentials kubelet-bootstrap \<br>  --token=$&#123;BOOTSTRAP_TOKEN&#125; \<br>  --kubeconfig=$&#123;HOME&#125;/kubernetes/node/cfg/bootstrap.kubeconfig<br><span class="hljs-meta">  </span><br><span class="hljs-meta">#</span><span class="bash"> 设置上下文参数</span><br>kubectl config set-context default \<br>  --cluster=kubernetes \<br>  --user=kubelet-bootstrap \<br>  --kubeconfig=$&#123;HOME&#125;/kubernetes/node/cfg/bootstrap.kubeconfig<br><span class="hljs-meta">  </span><br><span class="hljs-meta">#</span><span class="bash"> 设置默认上下文</span><br>kubectl config use-context default --kubeconfig=$&#123;HOME&#125;/kubernetes/node/cfg/bootstrap.kubeconfig<br></code></pre></td></tr></table></figure>

<h6 id="分发配置文件-4"><a href="#分发配置文件-4" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m copy  -a &quot;src=~/kubernetes/node/cfg/bootstrap.kubeconfig dest=/data/applications/kubernetes/cfg/bootstrap.kubeconfig owner=root group=root mode=0600&quot;<br></code></pre></td></tr></table></figure>



<h5 id="2-创建-kubelet-参数-yaml-配置文件"><a href="#2-创建-kubelet-参数-yaml-配置文件" class="headerlink" title="2. 创建 kubelet 参数 yaml 配置文件"></a>2. 创建 kubelet 参数 yaml 配置文件</h5><p>新建文件 <code>~/kubernetes/node/cfg/kubelet-config.yml</code>，文件内容如下: </p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeletConfiguration</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubelet.config.k8s.io/v1beta1</span><br><span class="hljs-attr">address:</span> <span class="hljs-number">0.0</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><br><span class="hljs-attr">port:</span> <span class="hljs-number">10250</span><br><span class="hljs-attr">readOnlyPort:</span> <span class="hljs-number">10255</span><br><span class="hljs-attr">cgroupDriver:</span> <span class="hljs-string">cgroupfs</span><br><span class="hljs-attr">clusterDNS:</span><br><span class="hljs-bullet">-</span> <span class="hljs-number">10.0</span><span class="hljs-number">.0</span><span class="hljs-number">.2</span><br><span class="hljs-attr">clusterDomain:</span> <span class="hljs-string">cluster.local</span> <br><span class="hljs-attr">failSwapOn:</span> <span class="hljs-literal">false</span><br><span class="hljs-attr">authentication:</span><br>  <span class="hljs-attr">anonymous:</span><br>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">webhook:</span><br>    <span class="hljs-attr">cacheTTL:</span> <span class="hljs-string">2m0s</span><br>    <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">x509:</span><br>    <span class="hljs-attr">clientCAFile:</span> <span class="hljs-string">/data/applications/kubernetes/ssl/ca.pem</span> <br><span class="hljs-attr">authorization:</span><br>  <span class="hljs-attr">mode:</span> <span class="hljs-string">Webhook</span><br>  <span class="hljs-attr">webhook:</span><br>    <span class="hljs-attr">cacheAuthorizedTTL:</span> <span class="hljs-string">5m0s</span><br>    <span class="hljs-attr">cacheUnauthorizedTTL:</span> <span class="hljs-string">30s</span><br><span class="hljs-attr">evictionHard:</span><br>  <span class="hljs-attr">imagefs.available:</span> <span class="hljs-number">15</span><span class="hljs-string">%</span><br>  <span class="hljs-attr">memory.available:</span> <span class="hljs-string">100Mi</span><br>  <span class="hljs-attr">nodefs.available:</span> <span class="hljs-number">10</span><span class="hljs-string">%</span><br>  <span class="hljs-attr">nodefs.inodesFree:</span> <span class="hljs-number">5</span><span class="hljs-string">%</span><br><span class="hljs-attr">maxOpenFiles:</span> <span class="hljs-number">1000000</span><br><span class="hljs-attr">maxPods:</span> <span class="hljs-number">80</span><br><span class="hljs-attr">featureGates:</span><br>  <span class="hljs-attr">RotateKubeletServerCertificate:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">rotateCertificates:</span> <span class="hljs-literal">true</span><br><span class="hljs-attr">serverTLSBootstrap:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明-8"><a href="#配置参数说明-8" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h6 id="分发配置文件-5"><a href="#分发配置文件-5" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m copy  -a &quot;src=~/kubernetes/node/cfg/kubelet-config.yml dest=/data/applications/kubernetes/cfg/kubelet-config.yml owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="3-创建-kubelet-配置文件"><a href="#3-创建-kubelet-配置文件" class="headerlink" title="3. 创建 kubelet 配置文件"></a>3. 创建 kubelet 配置文件</h5><p>新建文件 <code>~/kubernetes/node/cfg/kubelet.conf</code>，文件内容如下: (<code>hostname-override</code> 根据实际情况替换)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">KUBELET_OPTS=&quot;--hostname-override=k8s-master01 \<br>--pod_infra_container_image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \<br>--bootstrap-kubeconfig=/data/applications/kubernetes/cfg/bootstrap.kubeconfig \<br>--kubeconfig=/data/application/kubernetes/cfg/kubelet.kubeconfig \<br>--config=/data/applications/kubernetes/cfg/kubelet-config.yml \<br>--cert-dir=/data/applications/kubernetes/ssl \<br>--network-plugin=cni \<br>--fail-swap-on=false \<br>--logtostderr=true \<br>--v=2 \<br>--log-dir=/data/applications/kubernetes/logs \<br>--node-labels=node.kubernetes.io/k8s-master=true&quot;<br></code></pre></td></tr></table></figure>

<h6 id="分发配置文件-6"><a href="#分发配置文件-6" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m copy  -a &quot;src=~/kubernetes/node/cfg/kubelet.conf dest=/data/applications/kubernetes/cfg/kubelet.conf owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>

<blockquote>
<p>⚠️ <strong>注意:</strong></p>
<p><code>hostname-override</code> 设置为各node节点主机IP（或 主机名dns解析）</p>
<p><code>KUBELET_POD_INFRA_CONTAINER</code> 可设置为私有容器仓库地址，如有可设置为 <code>KUBELET_POD_INFRA_CONTAINER=&quot;--pod_infra_container_image=&#123;私有镜像仓库ip&#125;:80/k8s/pause-amd64:v3.1&quot;</code></p>
<p><code>node-labels</code> 如是 master 节点则设置为 master，node 节点则设置为 node</p>
</blockquote>
<h5 id="4-创建-systemd-unit-文件-1"><a href="#4-创建-systemd-unit-文件-1" class="headerlink" title="4. 创建 systemd unit 文件"></a>4. 创建 systemd unit 文件</h5><p>新建文件 <code>~/kubernetes/node/systemd/kubelet.service</code>，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs shell">[Unit]<br>Description=Kubernetes Kubelet<br>Documentation=https://github.com/GoogleCloudPlatform/kubernetes<br>After=docker.service<br>Requires=docker.service<br><br>[Service]<br>EnvironmentFile=/data/applications/kubernetes/cfg/kubelet.conf <br>ExecStart=/data/applications/kubernetes/bin/kubelet $KUBELET_OPTS<br>Restart=always<br>RestartSec=5<br>StartLimitInterval=0<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure>

<h6 id="分发配置文件-7"><a href="#分发配置文件-7" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m copy -a &quot;src=~/kubernetes/node/systemd/kubelet.service dest=/usr/lib/systemd/system/kubelet.service owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="5-启动-kubelet-服务"><a href="#5-启动-kubelet-服务" class="headerlink" title="5. 启动 kubelet 服务"></a>5. 启动 kubelet 服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m shell -a &quot;systemctl daemon-reload&quot;<br>ansible k8s-node -S -R root -m shell -a &quot;systemctl enable kubelet.service&quot;<br>ansible k8s-node -S -R root -m shell -a &quot;systemctl start kubelet.service&quot;<br>ansible k8s-node -S -R root -m shell -a &quot;systemctl status kubelet.service&quot;<br></code></pre></td></tr></table></figure>



<h5 id="6-bootstrap-token-认证授权"><a href="#6-bootstrap-token-认证授权" class="headerlink" title="6. bootstrap token 认证授权"></a>6. bootstrap token 认证授权</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap<br></code></pre></td></tr></table></figure>



<h5 id="7-自动-approce-CSR-请求"><a href="#7-自动-approce-CSR-请求" class="headerlink" title="7. 自动 approce CSR 请求"></a>7. 自动 approce CSR 请求</h5><p>新建文件 <code>~/kubernetes/yaml/csr-crb.yaml </code>，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> Approve all CSRs <span class="hljs-keyword">for</span> the group <span class="hljs-string">&quot;system:bootstrappers&quot;</span></span><br>kind: ClusterRoleBinding<br>apiVersion: rbac.authorization.k8s.io/v1<br>metadata:<br>  name: auto-approve-csrs-for-group<br>subjects:<br>- kind: Group<br>  name: system:node-bootstrap<br>  apiGroup: rbac.authorization.k8s.io<br>roleRef:<br>  kind: ClusterRole<br>  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient<br>  apiGroup: rbac.authorization.k8s.io<br>---<br><span class="hljs-meta">#</span><span class="bash"> To <span class="hljs-built_in">let</span> a node of the group <span class="hljs-string">&quot;system:nodes&quot;</span> renew its own credentials</span><br>kind: ClusterRoleBinding<br>apiVersion: rbac.authorization.k8s.io/v1<br>metadata:<br>  name: node-client-cert-renewal<br>subjects:<br>- kind: Group<br>  name: system:nodes<br>  apiGroup: rbac.authorization.k8s.io<br>roleRef:<br>  kind: ClusterRole<br>  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient<br>  apiGroup: rbac.authorization.k8s.io<br>---<br><span class="hljs-meta">#</span><span class="bash"> A ClusterRole <span class="hljs-built_in">which</span> instructs the CSR approver to approve a node requesting a</span><br><span class="hljs-meta">#</span><span class="bash"> serving cert matching its client cert.</span><br>kind: ClusterRole<br>apiVersion: rbac.authorization.k8s.io/v1<br>metadata:<br>  name: approve-node-server-renewal-csr<br>rules:<br>- apiGroups: [&quot;certificates.k8s.io&quot;]<br>  resources: [&quot;certificatesigningrequests/selfnodeserver&quot;]<br>  verbs: [&quot;create&quot;]<br>---<br><span class="hljs-meta">#</span><span class="bash"> To <span class="hljs-built_in">let</span> a node of the group <span class="hljs-string">&quot;system:nodes&quot;</span> renew its own server credentials</span><br>kind: ClusterRoleBinding<br>apiVersion: rbac.authorization.k8s.io/v1<br>metadata:<br>  name: node-server-cert-renewal<br>subjects:<br>- kind: Group<br>  name: system:nodes<br>  apiGroup: rbac.authorization.k8s.io<br>roleRef:<br>  kind: ClusterRole<br>  name: approve-node-server-renewal-csr<br>  apiGroup: rbac.authorization.k8s.io<br></code></pre></td></tr></table></figure>

<blockquote>
<p>🌈 <strong>rbac规则解释说明</strong></p>
<p><code>auto-approve-csrs-for-group</code>：</p>
<ul>
<li>自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrap（注意 group 要和认证用的 token.csv 中的 group 保持一致）；</li>
</ul>
<p><code>node-client-cert-renewal</code>：</p>
<ul>
<li>自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes；</li>
</ul>
<p><code>node-server-cert-renewal</code>：</p>
<p>自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;`</p>
</blockquote>
<h6 id="执行创建"><a href="#执行创建" class="headerlink" title="执行创建:"></a>执行创建:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f ~/kubernetes/yaml/csr-crb.yaml <br></code></pre></td></tr></table></figure>



<h5 id="8-查看CSR证书请求并批准-kubelet-的-TLS-证书请求"><a href="#8-查看CSR证书请求并批准-kubelet-的-TLS-证书请求" class="headerlink" title="8. 查看CSR证书请求并批准 kubelet 的 TLS 证书请求"></a>8. 查看CSR证书请求并批准 kubelet 的 TLS 证书请求</h5><blockquote>
<p>🚩 如果通过 TLS bootstrap 管理后，kubelet 的 证书就会变成轮询证书的形式，client 证书会自动签发，server 证书基于安全性考虑，CSR approving controllers 不会自动 approve kubelet server 证书签名请求，需要手动 approve，但是通过自定义的触发器，同样也能实现自动化。并且会自动生成 kubelet.kubeconfig 专门用于证书的认证和签发管理。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 批量签发(Pending 的 CSR 用于创建 kubelet server 证书，需要手动 approve)</span><br>kubectl get csr|grep &#x27;Pending&#x27; | awk &#x27;NR&gt;0&#123;print $1&#125;&#x27;|xargs kubectl certificate approve<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 查看 csr 请求（批准申请查看后CONDITION显示Approved,Issued即可）</span><br>kubectl get csr<br></code></pre></td></tr></table></figure>



<h5 id="9-验证-TLS-bootstrap-管理证书"><a href="#9-验证-TLS-bootstrap-管理证书" class="headerlink" title="9. 验证 TLS bootstrap 管理证书:"></a>9. 验证 TLS bootstrap 管理证书:</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ll /data/applications/kubernetes/ssl/kubelet-*.pem<br></code></pre></td></tr></table></figure>

<p>正常情况，结果如下:</p>
<p><img src="https://blog-images.tareya.cn/blog_images/image-20211008113644603.png" srcset="/img/loading.gif" lazyload alt="image-20211008113644603"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat /data/applications/kubernetes/cfg/kubelet.kubeconfig <br></code></pre></td></tr></table></figure>

<p>正常情况，结果如下:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">clusters:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">cluster:</span><br>    <span class="hljs-attr">certificate-authority-data:</span> <span class="hljs-string">LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR2akNDQXFhZ0F3SUJBZ0lVR0lHZ2VQSWNoSWtKRnFZOTJ6MWE2Z1RCNVBnd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1pURUxNQWtHQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbAphVXBwYm1jeEREQUtCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HYzNsemRHVnRNUk13RVFZRFZRUURFd3ByCmRXSmxjbTVsZEdWek1CNFhEVEl4TURreU16QTVNVEF3TUZvWERUTXhNRGt5TVRBNU1UQXdNRm93WlRFTE1Ba0cKQTFVRUJoTUNRMDR4RURBT0JnTlZCQWdUQjBKbGFVcHBibWN4RURBT0JnTlZCQWNUQjBKbGFVcHBibWN4RERBSwpCZ05WQkFvVEEyczRjekVQTUEwR0ExVUVDeE1HYzNsemRHVnRNUk13RVFZRFZRUURFd3ByZFdKbGNtNWxkR1Z6Ck1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBeUU4ZytUWUhMdCtjQXJSRkRyZTYKNmp5ZG9VbFkvM29TeW1uT3d6dEhHMFk0ZDhPOXVBZVZRTTdNWm1BMGhpRHhXUVZiWHlKQTBvNkVwVzFvRlR3YgpHdG5QUWlqSVFJTWlnZVNTNnVHVEZiRklpUTNBam53Y2tGdlI5WHlRTHVyejBRNnJFM21yc3RMNlBLYUx1SEJwCmo1SDZsRUx3OTdxdCtjR0NtQnMraEdFc202THVEWmwxRGlBb1RQQnduZTdlWHRqM2xmam5oUW44a0IrbGJMSmYKYjRSakFSbkN5RUgwMnBzTXNUcW85ckhiZEczUUNCeHlGZ2s1ZHRNczQwVFMyV0VzQjNyRnJnaHF0R2hPRWM2cApGZytrMjZEL0RMTHpNTWxCLzBKOXlkN0FkOUZlbkdjeUZIc2FKclMvM3RCVlFpWk5oaTBaZzlWM013Rk0zUHZPClZ3SURBUUFCbzJZd1pEQU9CZ05WSFE4QkFmOEVCQU1DQVFZd0VnWURWUjBUQVFIL0JBZ3dCZ0VCL3dJQkFqQWQKQmdOVkhRNEVGZ1FVREhEaGVia1hzWlRPZWw3TTdhQmtML2cxSUtvd0h3WURWUjBqQkJnd0ZvQVVESERoZWJrWApzWlRPZWw3TTdhQmtML2cxSUtvd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFFSjZZby9oNklsTWlZSFVHY1NhCkNJKytuUTNHWklKQlJYc0huMWlSRGVUaVBtejQvQm5pWmJlT3dXK1V2WVIwcytEMTlVeTRwa2cvb2tYUGdUM1oKdjBIN1IzdHpmMklGUVNhbnhBTE9hVTdxbm5tWU9JOUpYa0FDclR6QjJ5TlhWSXpZamRDMkxZS1kzRDI2TVZFZwpLQWtVWG5ybWxZcGUxOUh5UWFiVy9uSklPTDNJUzZRQlp2WGZ1aHpRMEJXL3g5OWdPV2swVGFrYnIrdlhhdWZxCk9Eb2hJT0E1cmk0UkVWN0M4WE1QYWNpREVGRVVGNG9hZlV6eFVjNVlIbDhkMnQwN1k3Mkl3WTdFaDY5ZG8weGkKQjNRZS8xc2dLRWZBZXp1NXV5eGhTZlpqODYybzQ1WGpCMzc1dHRETlEwdlhkcVRBUGU1SFpaUzUyeDAyRVFLWAp1V2c9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K</span><br>    <span class="hljs-attr">server:</span> <span class="hljs-string">https://192.168.3.231:6443</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">default-cluster</span><br><span class="hljs-attr">contexts:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">context:</span><br>    <span class="hljs-attr">cluster:</span> <span class="hljs-string">default-cluster</span><br>    <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span><br>    <span class="hljs-attr">user:</span> <span class="hljs-string">default-auth</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">default-context</span><br><span class="hljs-attr">current-context:</span> <span class="hljs-string">default-context</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Config</span><br><span class="hljs-attr">preferences:</span> &#123;&#125;<br><span class="hljs-attr">users:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">default-auth</span><br>  <span class="hljs-attr">user:</span><br>    <span class="hljs-attr">client-certificate:</span> <span class="hljs-string">/data/applications/kubernetes/ssl/kubelet-client-current.pem</span><br>    <span class="hljs-attr">client-key:</span> <span class="hljs-string">/data/applications/kubernetes/ssl/kubelet-client-current.pem</span><br></code></pre></td></tr></table></figure>



<h5 id="10-验证节点状态"><a href="#10-验证节点状态" class="headerlink" title="10. 验证节点状态"></a>10. 验证节点状态</h5><h6 id="1⃣️-查看节点状态信息"><a href="#1⃣️-查看节点状态信息" class="headerlink" title="1⃣️ 查看节点状态信息:"></a>1⃣️ 查看节点状态信息:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get nodes<br></code></pre></td></tr></table></figure>

<p>正常情况，返回如下: (此时发现所有node节点状态均为 “ready”)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME           STATUS   ROLES    AGE   VERSION<br>k8s-master01   Ready    &lt;none&gt;   12h   v1.18.18<br>k8s-master02   Ready    &lt;none&gt;   12h   v1.18.18<br>k8s-master03   Ready    &lt;none&gt;   12h   v1.18.18<br>k8s-node1      Ready    &lt;none&gt;   12h   v1.18.18<br></code></pre></td></tr></table></figure>

<h6 id="2⃣️-查看监听端口-2"><a href="#2⃣️-查看监听端口-2" class="headerlink" title="2⃣️ 查看监听端口:"></a>2⃣️ 查看监听端口:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">netstat -lntp|grep kubelet<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">tcp        0      0 127.0.0.1:40621         0.0.0.0:*               LISTEN      28305/kubelet       <br>tcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      28305/kubelet       <br>tcp6       0      0 :::10250                :::*                    LISTEN      28305/kubelet       <br>tcp6       0      0 :::10255                :::*                    LISTEN      28305/kubelet    <br></code></pre></td></tr></table></figure>

<blockquote>
<p>🌈 kubelet 监听端口为 10248、10250、10255</p>
<p><code>10248</code>：healthz http服务端口，即健康检查服务的端口，接收 http 请求，不需要认证授权；<br><code>10250</code>：kubelet 服务监听的端口, api 会检测他是否存活。接收 https 请求，访问该端口时需要认证和授权（即使访问<code>/healthz</code>也需要）；</p>
<p><code>10255</code>: 只读端口，可以不用验证和授权机制，直接访问。配置 <code>&quot;readOnlyPort: 0&quot;</code> 则表示关闭只读端口；</p>
</blockquote>
<h4 id="2）部署-kube-proxy"><a href="#2）部署-kube-proxy" class="headerlink" title="2）部署 kube-proxy"></a>2）部署 kube-proxy</h4><p><strong>基础组件介绍 —— kube-proxy:</strong> <a href="https://tareya.github.io/2021/10/21/Kubernetes-%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-kube-proxy/">传送门</a></p>
<blockquote>
<p>🚩 kube-proxy运行在所有的node节点上，它监听apiserver中service和endpoint的变化情况，创建路由规则以提供服务IP和负载均衡功能。</p>
<p><strong>以下的操作都是在 k8s-master01 上进行，然后再通过批管理工具分发至其他节点进行统一管理。</strong></p>
</blockquote>
<h5 id="1-创建-kube-proxy-的-kubeconfig-配置文件"><a href="#1-创建-kube-proxy-的-kubeconfig-配置文件" class="headerlink" title="1. 创建 kube-proxy 的 kubeconfig 配置文件"></a>1. 创建 kube-proxy 的 kubeconfig 配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell">KUBE_APISERVER=&quot;https://$(hostname -i):6443&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 设置集群参数</span><br>kubectl config set-cluster kubernetes \<br>  --certificate-authority=/data/applications/kubernetes/ssl/ca.pem \<br>  --embed-certs=true \<br>  --server=$&#123;KUBE_APISERVER&#125; \<br>  --kubeconfig=$&#123;HOME&#125;/kubernetes/node/cfg/kube-proxy.kubeconfig<br><span class="hljs-meta"> </span><br><span class="hljs-meta">#</span><span class="bash"> 设置客户端认证参数</span> <br>kubectl config set-credentials kube-proxy \<br>  --client-certificate=/data/applications/kubernetes/ssl/kube-proxy.pem \<br>  --client-key=/data/applications/kubernetes/ssl/kube-proxy-key.pem \<br>  --embed-certs=true \<br>  --kubeconfig=$&#123;HOME&#125;/kubernetes/node/cfg/kube-proxy.kubeconfig<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 设置上下文参数</span><br>kubectl config set-context default \<br>  --cluster=kubernetes \<br>  --user=kube-proxy \<br>  --kubeconfig=$&#123;HOME&#125;/kubernetes/node/cfg/kube-proxy.kubeconfig<br><span class="hljs-meta">  </span><br><span class="hljs-meta">#</span><span class="bash"> 设置默认上下文</span> <br>kubectl config use-context default --kubeconfig=$&#123;HOME&#125;/kubernetes/node/cfg/kube-proxy.kubeconfig<br></code></pre></td></tr></table></figure>

<h6 id="分发配置文件-8"><a href="#分发配置文件-8" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m copy  -a &quot;src=~/kubernetes/node/cfg/kube-proxy.kubeconfig dest=/data/applications/kubernetes/cfg/kube-proxy.kubeconfig owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="2-创建-kube-proxy-参数-yaml-配置文件"><a href="#2-创建-kube-proxy-参数-yaml-配置文件" class="headerlink" title="2. 创建 kube-proxy 参数 yaml 配置文件"></a>2. 创建 kube-proxy 参数 yaml 配置文件</h5><p>新建文件 <code>~/kubernetes/node/cfg/kube-proxy-config.yml</code>，文件内容如下:  (<code>hostname-override</code> 根据实际情况替换)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs shell">kind: KubeProxyConfiguration<br>apiVersion: kubeproxy.config.k8s.io/v1alpha1<br>hostnameOverride: k8s-master01<br>address: 0.0.0.0<br>metricsBindAddress: 0.0.0.0:10249<br>healthzBindAddress: 0.0.0.0:10256<br>clientConnection:<br>  burst: 200<br>  kubeconfig: &quot;/data/applications/kubernetes/cfg/kube-proxy.kubeconfig&quot;<br>  qps: 100<br>enableProfiling: true<br>clusterCIDR: 10.0.0.0/16<br>portRange: &quot;&quot;<br>mode: &quot;ipvs&quot;<br>kubeProxyIPTablesConfiguration:<br>  masqueradeAll: true<br>kubeProxyIPVSConfiguration:<br>  scheduler: rr<br>  excludeCIDRs: []<br></code></pre></td></tr></table></figure>

<h6 id="配置参数说明-9"><a href="#配置参数说明-9" class="headerlink" title="配置参数说明:"></a>配置参数说明:</h6><table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h6 id="分发配置文件-9"><a href="#分发配置文件-9" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m copy  -a &quot;src=~/kubernetes/node/cfg/kube-proxy-config.yml dest=/data/applications/kubernetes/cfg/kube-proxy-config.yml owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="3-创建-kube-proxy-配置文件"><a href="#3-创建-kube-proxy-配置文件" class="headerlink" title="3. 创建 kube-proxy 配置文件"></a>3. 创建 kube-proxy 配置文件</h5><p>新建文件 <code>~/kubernetes/node/cfg/kube-proxy.conf</code>，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">KUBE_PROXY_OPTS=&quot;--logtostderr=true \<br>--v=2 \<br>--log-dir=/data/applications/kubernetes/logs \<br>--ipvs-min-sync-period=5s \<br>--ipvs-sync-period=5s \<br>--config=/data/applications/kubernetes/cfg/kube-proxy-config.yml&quot;<br></code></pre></td></tr></table></figure>

<h6 id="分发配置文件-10"><a href="#分发配置文件-10" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m copy  -a &quot;src=~/kubernetes/node/cfg/kube-proxy.conf dest=/data/applications/kubernetes/cfg/kube-proxy.conf owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="4-创建-systemd-unit-文件-2"><a href="#4-创建-systemd-unit-文件-2" class="headerlink" title="4. 创建 systemd unit 文件"></a>4. 创建 systemd unit 文件</h5><p>新建文件 <code>~/kubernetes/node/systemd/kube-proxy.service</code>，文件内容如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">[Unit]<br>Description=Kubernetes Kube-Proxy Server<br>Documentation=https://github.com/GoogleCloudPlatform/kubernetes<br>After=network.target<br><br>[Service]<br>EnvironmentFile=/data/applications/kubernetes/cfg/kube-proxy.conf<br>ExecStart=/data/applications/kubernetes/bin/kube-proxy $KUBE_PROXY_OPTS<br>Restart=on-failure<br>RestartSec=5<br>LimitNOFILE=65536<br><br>[Install]<br>WantedBy=multi-user.target<br></code></pre></td></tr></table></figure>

<h6 id="分发配置文件-11"><a href="#分发配置文件-11" class="headerlink" title="分发配置文件:"></a>分发配置文件:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m copy -a &quot;src=~/kubernetes/node/systemd/kube-proxy.service dest=/usr/lib/systemd/system/kube-proxy.service owner=root group=root mode=0644&quot;<br></code></pre></td></tr></table></figure>



<h5 id="5-开启-ipvs"><a href="#5-开启-ipvs" class="headerlink" title="5. 开启 ipvs"></a>5. 开启 ipvs</h5><blockquote>
<p>🌈 从 <code>k8s</code> 的1.8版本开始，<code>kube-proxy</code> 引入了 <code>IPVS</code> 模式，<code>IPVS</code>模式与 <code>iptables</code> 同样基于 <code>Netfilter</code> ，但是采用的 <code>hash</code> 表，因此当<code>service</code>数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高 <code>service</code>的服务性能。</p>
<p><strong>以下的操作都是在所有 node 节点上执行。</strong></p>
</blockquote>
<h6 id="1⃣️-开启内核参数"><a href="#1⃣️-开启内核参数" class="headerlink" title="1⃣️ 开启内核参数"></a>1⃣️ 开启内核参数</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 追加内核参数，开启二层网桥处理</span><br>cat &gt;&gt; /etc/sysctl.d/kubernetes.conf &lt;&lt; EOF<br>net.bridge.bridge-nf-call-iptables = 1<br>net.bridge.bridge-nf-call-ip6tables = 1<br>EOF<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 加载参数</span><br>sysctl -p /etc/sysctl.d/kubernetes.conf <br></code></pre></td></tr></table></figure>

<h6 id="2⃣️-安装-ipvsadm-等工具"><a href="#2⃣️-安装-ipvsadm-等工具" class="headerlink" title="2⃣️ 安装 ipvsadm 等工具"></a>2⃣️ 安装 ipvsadm 等工具</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m shell -a &quot;yum install -y ipvsadm ipset bridge-utils conntrack&quot;<br></code></pre></td></tr></table></figure>

<h6 id="3⃣️-开启-ipvs-支持"><a href="#3⃣️-开启-ipvs-支持" class="headerlink" title="3⃣️ 开启 ipvs 支持"></a>3⃣️ 开启 ipvs 支持</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 创建 ipvs 模块配置</span><br>cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF<br>modprobe -- ip_vs<br>modprobe -- ip_vs_rr<br>modprobe -- ip_vs_wrr<br>modprobe -- ip_vs_sh<br>modprobe -- nf_conntrack_ipv4<br>EOF<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 赋予执行权限</span><br>chmod 755 /etc/sysconfig/modules/ipvs.modules<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 加载配置</span><br>bash /etc/sysconfig/modules/ipvs.modules<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 查看加载信息</span><br>lsmod | grep ip_vs<br></code></pre></td></tr></table></figure>

<p>正确情况，返回如下:</p>
<p><img src="https://blog-images.tareya.cn/blog_images/image-20211008141710750.png" srcset="/img/loading.gif" lazyload alt="image-20211008141710750"></p>
<h5 id="6-启动-kube-proxy-服务"><a href="#6-启动-kube-proxy-服务" class="headerlink" title="6. 启动 kube-proxy 服务"></a>6. 启动 kube-proxy 服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m shell -a &quot;systemctl daemon-reload&quot;<br>ansible k8s-node -S -R root -m shell -a &quot;systemctl enable kube-proxy.service&quot;<br>ansible k8s-node -S -R root -m shell -a &quot;systemctl start kube-proxy.service&quot;<br>ansible k8s-node -S -R root -m shell -a &quot;systemctl status kube-proxy.service&quot;<br></code></pre></td></tr></table></figure>



<h5 id="7-验证节点状态"><a href="#7-验证节点状态" class="headerlink" title="7. 验证节点状态"></a>7. 验证节点状态</h5><h6 id="1⃣️-查看监听端口"><a href="#1⃣️-查看监听端口" class="headerlink" title="1⃣️ 查看监听端口:"></a>1⃣️ 查看监听端口:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">netstat -lnpt|grep kube-proxy<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">tcp6       0      0 :::10256                :::*                    LISTEN      25828/kube-proxy    <br>tcp6       0      0 :::10249                :::*                    LISTEN      25828/kube-proxy       <br></code></pre></td></tr></table></figure>

<blockquote>
<p>🌈 kubelet 监听端口为 10249、10256</p>
<p><code>10249</code>：接收 http 请求，不需要认证授权，用于提供 <code>/metrics </code>；<br><code>10256</code>：接收 http 请求，不需要认证授权，用于提供<code> /healthz</code>。</p>
</blockquote>
<h6 id="2⃣️-查看-ipvs-路由规则"><a href="#2⃣️-查看-ipvs-路由规则" class="headerlink" title="2⃣️ 查看 ipvs 路由规则"></a>2⃣️ 查看 ipvs 路由规则</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ipvsadm -ln<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">IP Virtual Server version 1.2.1 (size=4096)<br>Prot LocalAddress:Port Scheduler Flags<br><span class="hljs-meta">  -&gt;</span><span class="bash"> RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br>TCP  10.0.0.1:443 rr<br><span class="hljs-meta">  -&gt;</span><span class="bash"> 192.168.3.231:6443           Masq    1      0          0</span>         <br><span class="hljs-meta">  -&gt;</span><span class="bash"> 192.168.3.232:6443           Masq    1      0          0</span>         <br><span class="hljs-meta">  -&gt;</span><span class="bash"> 192.168.3.233:6443           Masq    1      0          0</span>        <br></code></pre></td></tr></table></figure>

<blockquote>
<p>🚩 <strong>由上面可以看出</strong>：所有通过 https 访问 K8S SVC kubernetes 的请求都转发到 kube-apiserver 节点的 6443 端口；</p>
</blockquote>
<h3 id="7、部署-calico-网络方案"><a href="#7、部署-calico-网络方案" class="headerlink" title="7、部署 calico 网络方案"></a>7、部署 calico 网络方案</h3><p><strong>基础组件介绍 —— 网络组件:</strong> <a href="https://tareya.github.io/2021/10/21/Kubernetes-%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6/">传送门</a></p>
<h4 id="1）下载和分发-calico-和-cni-插件"><a href="#1）下载和分发-calico-和-cni-插件" class="headerlink" title="1）下载和分发 calico 和 cni 插件"></a>1）下载和分发 calico 和 cni 插件</h4><h5 id="1-批量创建-calico-和-cni-组件相关目录"><a href="#1-批量创建-calico-和-cni-组件相关目录" class="headerlink" title="1. 批量创建 calico 和 cni 组件相关目录"></a>1. 批量创建 calico 和 cni 组件相关目录</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m shell -a &quot;mkdir -p /etc/calico/&#123;conf,yaml&#125; /opt/cni/bin/ /etc/cni/net.d&quot;<br></code></pre></td></tr></table></figure>

<h5 id="2-下载-calico-二进制文件"><a href="#2-下载-calico-二进制文件" class="headerlink" title="2. 下载 calico 二进制文件"></a>2. 下载 calico 二进制文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p ~/kubernetes/network/bin	&amp;&amp; \<br>wget -c https://github.com/projectcalico/calicoctl/releases/download/v3.19.2/calicoctl-linux-amd64 -O ~/kubernetes/network/bin/calicoctl &amp;&amp; \<br>chmod +x ~/kubernetes/network/bin/calicoctl <br></code></pre></td></tr></table></figure>

<h6 id="验证版本"><a href="#验证版本" class="headerlink" title="验证版本:"></a>验证版本:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">~/kubernetes/network/bin/calicoctl version<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">Client Version:    v3.19.2<br>Git commit:        6f3d4900<br>invalid configuration: no configuration has been provided, try setting KUBERNETES_MASTER environment variable<br></code></pre></td></tr></table></figure>

<blockquote>
<p>由于尚未进行配置，故无法获取具体的配置信息。</p>
</blockquote>
<h6 id="分发二进制包至各-master-节点"><a href="#分发二进制包至各-master-节点" class="headerlink" title="分发二进制包至各 master 节点:"></a>分发二进制包至各 master 节点:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m copy -a &quot;src=~/kubernetes/network/bin/calicoctl dest=/usr/local/bin/calicoctl owner=root group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>



<h5 id="3-下载-cni-组件"><a href="#3-下载-cni-组件" class="headerlink" title="3. 下载 cni 组件"></a>3. 下载 cni 组件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 下载解 cni 网络插件</span><br>wget -c https://github.com/containernetworking/plugins/releases/download/v0.8.6/cni-plugins-linux-amd64-v1.0.1.tgz -O ~/kubernetes/network/bin/cni-plugins-linux-amd64-v1.0.1.tgz <br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 下载 calico cni 组件</span><br>wget -c https://github.com/projectcalico/cni-plugin/releases/download/v3.19.2/calico-amd64 -O ~/kubernetes/network/bin/calico &amp;&amp; \<br>wget -c  https://github.com/projectcalico/cni-plugin/releases/download/v3.19.2/calico-ipam-amd64 -O ~/kubernetes/network/bin/calico-ipam<br></code></pre></td></tr></table></figure>

<h6 id="分发二进制包至各-master-节点-1"><a href="#分发二进制包至各-master-节点-1" class="headerlink" title="分发二进制包至各 master 节点:"></a>分发二进制包至各 master 节点:</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 解压 cni 插件压缩包到各节点 cni 默认目录</span><br>ansible k8s-node -S -R root -m unarchive -a &quot;src=~/kubernetes/network/bin/cni-plugins-linux-amd64-v1.0.1.tgz dest=/opt/cni/bin/ mode=0755 copy=yes&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 分发 calico cni 组件</span><br>ansible k8s-node -S -R root -m copy -a &quot;src=~/kubernetes/network/bin/calico dest=/opt/cni/bin/calico owner=root group=root mode=0755&quot;<br>ansible k8s-node -S -R root -m copy -a &quot;src=~/kubernetes/network/bin/calico-ipam dest=/opt/cni/bin/calico-ipam owner=root group=root mode=0755&quot;<br></code></pre></td></tr></table></figure>



<h5 id="4-镜像准备"><a href="#4-镜像准备" class="headerlink" title="4. 镜像准备"></a>4. 镜像准备</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 提前下载如下镜像</span><br>ansible k8s-node -S -R root -m shell -a &quot;docker pull calico/node:v3.19.2&quot;<br>ansible k8s-node -S -R root -m shell -a &quot;docker pull calico/cni:v3.19.2&quot;<br>ansible k8s-node -S -R root -m shell -a &quot;docker pull calico/kube-controllers:v3.19.2&quot;<br>ansible k8s-node -S -R root -m shell -a &quot;docker pull calico/pod2daemon-flexvol:v3.19.2&quot;<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 上传至私仓（可选）</span><br></code></pre></td></tr></table></figure>

<h4 id=""><a href="#" class="headerlink" title=""></a></h4><h4 id="2）创建-calico-配置文件"><a href="#2）创建-calico-配置文件" class="headerlink" title="2）创建 calico 配置文件"></a>2）创建 calico 配置文件</h4><h5 id="1-获取-calico-manifest"><a href="#1-获取-calico-manifest" class="headerlink" title="1. 获取 calico manifest"></a>1. 获取 calico manifest</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">wget -c --no-check-certificate https://docs.projectcalico.org/archive/v3.19/manifests/calico.yaml -O ~/kubernetes/network/cfg/calico.yaml<br></code></pre></td></tr></table></figure>



<h5 id="2-修改镜像版本（可选）"><a href="#2-修改镜像版本（可选）" class="headerlink" title="2. 修改镜像版本（可选）"></a>2. 修改镜像版本（可选）</h5><blockquote>
<p>🚩 <strong>关于镜像版本</strong></p>
<p>由于官方提供的 yaml 文件是实时更新的，其容器的版本也会随之更新。故，如果要使用指定版本的 calico 的话，就需要自行替换镜像版本，如本文使用 v3.19.2，而官方默认提供的可能已经是 v3.19.3 了。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 获取官方默认提供的镜像版本</span><br>DEFAULT_VERSION=$(awk -F &quot;:&quot; &#x27;/image:/&#123;print $NF&#125;&#x27; ~/kubernetes/network/cfg/calico.yaml|head -1)<br><span class="hljs-meta">#</span><span class="bash"> 替换为指定版本</span><br>sed -i &quot;s/$DEFAULT_VERSION/v3.19.2/g&quot; ~/kubernetes/network/cfg/calico.yaml<br></code></pre></td></tr></table></figure>



<h5 id="3-关闭-IPIP-模式（可选）"><a href="#3-关闭-IPIP-模式（可选）" class="headerlink" title="3. 关闭 IPIP 模式（可选）"></a>3. 关闭 IPIP 模式（可选）</h5><blockquote>
<p>🚩 <strong>关于 CALICO_IPV4POOL_IPIP 参数</strong></p>
<p>calico 网络默认使用 IPIP 模式，它会在每个 node 节点创建一个 <code>tunl0</code> 网口，通过隧道路由的方式将所有的node节点连接起来，也就是说，即使是不同网段的节点通过这种方式也可以纳入统一个 k8s 集群，所以官方推荐这种情况时，使用该模式，但是由于我们本文所有节点都在同一网段内，所以这里我们关闭 IPIP 模式。</p>
<p><code>CALICO_IPV4POOL_IPIP</code> 提供以下三种 value:</p>
<ul>
<li><code>Always</code>: 开启 IPIP 模式(默认)</li>
<li><code>CrossSubnet</code>: 只在跨网段时才开启 IPIP 模式，适合有 Kubernetes 节点在其他网段的情况，属于中肯友好方案</li>
<li><code>Never</code>: 关闭 IPIP 模式，适合确认所有 Kubernetes 节点都在同一个网段下的情况</li>
</ul>
</blockquote>
<p>在 DaemonSet 部分 <code>calico-node</code>的 <code>pod</code> 的变量中，修改 <code>CALICO_IPV4POOL_IPIP</code> 值为 <code>Never</code>。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Enable IPIP</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_IPV4POOL_IPIP</span><br>	<span class="hljs-attr">value:</span> <span class="hljs-string">&quot;Never&quot;</span><br></code></pre></td></tr></table></figure>



<h5 id="4-修改-pod-的网段"><a href="#4-修改-pod-的网段" class="headerlink" title="4. 修改 pod 的网段"></a>4. 修改 pod 的网段</h5><blockquote>
<p>🚩 <strong>关于 CALICO_IPV4POOL_CIDR 参数</strong></p>
<p>calico 默认使用的 k8s 集群的 pod 网段为 <code>192.168.0.0/16</code> ，由于我们使用的 CIDR 不同于这个段，所以要自行修改，注意保持和 kube-controller-manager 中的 <code>--cluster-cidr</code> 保持一致。</p>
</blockquote>
<p>在 DaemonSet 部分 <code>calico-node</code>的 <code>pod</code> 的变量中，开启 <code>CALICO_IPV4POOL_CIDR</code> ，修改成集群使用的 CIDR 网段。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># The default IPv4 pool to create on startup if none exists. Pod IPs will be</span><br><span class="hljs-comment"># chosen from this range. Changing this value after installation will have</span><br><span class="hljs-comment"># no effect. This should fall within `--cluster-cidr`.</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">CALICO_IPV4POOL_CIDR</span><br>  <span class="hljs-attr">value:</span> <span class="hljs-string">&quot;10.244.0.0/16&quot;</span><br></code></pre></td></tr></table></figure>



<h5 id="5-修改-apiserver-的信息"><a href="#5-修改-apiserver-的信息" class="headerlink" title="5. 修改 apiserver 的信息"></a>5. 修改 apiserver 的信息</h5><blockquote>
<p>🚩 <strong>关于 calico 访问 apiserver</strong></p>
<p>如果未配置 apiserver 信息。calico默认将设置默认的 calico 网段（CIDR） 和 443端口，如 10.0.0.1:443。会导致 node 节点无法连接 apiserver，报错类似以下情况:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[ERROR][9] startup/startup.go 159: failed to query kubeadm&#x27;s config map error=Get &quot;https://10.0.0.1:443/api/v1/namespaces/kube-system/configmaps/kubeadm-config?timeout=2s&quot;: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)<br></code></pre></td></tr></table></figure>

<p>以及（calico-kube-controllers）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container &quot;2c9d1a481b2ab8328466ba5f7b236bdc822957371579812fa8e7ce3e464dce04&quot; network for pod &quot;calico-kube-controllers-86497987b6-j2fch&quot;: networkPlugin cni failed to set up pod &quot;calico-kube-controllers-86497987b6-j2fch_kube-system&quot; network: error getting ClusterInformation: Get &quot;https://[10.0.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default&quot;: dial tcp 10.0.0.1:443: i/o timeout<br></code></pre></td></tr></table></figure>

<p>apiserver 配置字段为：<code>KUBERNETES_SERVICE_HOST</code>、<code>KUBERNETES_SERVICE_PORT</code>、<code>KUBERNETES_SERVICE_PORT_HTTPS</code>。</p>
<p>分别在 DaemonSet 部分 <code>calico-node</code> 的 <code>upgrade-ipam</code>, <code>install-cni</code>, <code>calico-node</code> 和 Deployment 部分 <code>calico-kube-controllers</code> 的 <code>env</code> 变量中添加 apiserver 信息。</p>
</blockquote>
<p>在 DaemonSet 部分 <code>calico-node</code> 的  <code>env</code> 变量和 Deployment 部分 <code>calico-kube-controllers</code> 的 <code>env</code> 变量中增加 apiserver 的信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> The kubernetes apiserver info</span><br>- name: KUBERNETES_SERVICE_HOST<br>  value: &quot;192.168.3.231&quot;<br>- name: KUBERNETES_SERVICE_PORT<br>  value: &quot;6443&quot;<br>- name: KUBERNETES_SERVICE_PORT_HTTPS<br>  value: &quot;6443&quot;<br></code></pre></td></tr></table></figure>



<h5 id="6-修改-typha-service-name（集群节点数超过50以上可选）"><a href="#6-修改-typha-service-name（集群节点数超过50以上可选）" class="headerlink" title="6. 修改 typha_service_name（集群节点数超过50以上可选）"></a>6. 修改 typha_service_name（集群节点数超过50以上可选）</h5><blockquote>
<p>🚩 <strong>关于 calico-typha</strong></p>
<p>官方推荐当k8s数据存储模式超过50各节点时启用typha模式，Typha 组件可以帮助 Calico 扩展到大量的节点，并且不会对 Kubernetes API 服务器造成过度影响。</p>
<p>官方提供的 manifest 地址: <a target="_blank" rel="noopener" href="https://docs.projectcalico.org/archive/v3.19/manifests/calico-typha.yaml">https://docs.projectcalico.org/archive/v3.19/manifests/calico-typha.yaml</a></p>
</blockquote>
<p>在 ConfigMap 部分的 <code>data</code>变量中，修改 <code>typha_service_name</code> 为 <code>calico-typha</code></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">calico-config</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><br><span class="hljs-attr">data:</span><br>  <span class="hljs-comment"># Typha is disabled.</span><br>  <span class="hljs-attr">typha_service_name:</span> <span class="hljs-string">&quot;calico-typha&quot;</span><br></code></pre></td></tr></table></figure>



<h4 id="3）创建-calico-网络"><a href="#3）创建-calico-网络" class="headerlink" title="3）创建 calico 网络"></a>3）创建 calico 网络</h4><blockquote>
<p>🚩 创建前先清空 /etc/cni/net.d 目录下的 cni 网络配置，如 <code>10-flannel.conflist</code> 、<code>10-calico.conflist</code>、<code>calico-kubeconfig</code> 、<code>calico-tls</code> 等。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ansible k8s-node -S -R root -m shell -a &quot;\rm -r /etc/cni/net.d/*&quot;<br></code></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f ~/kubernetes/network/cfg/calico.yaml<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs shell">configmap/calico-config created<br>customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created<br>customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created<br>clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created<br>clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created<br>clusterrole.rbac.authorization.k8s.io/calico-node created<br>clusterrolebinding.rbac.authorization.k8s.io/calico-node created<br>daemonset.apps/calico-node created<br>serviceaccount/calico-node created<br>deployment.apps/calico-kube-controllers created<br>serviceaccount/calico-kube-controllers created<br>poddisruptionbudget.policy/calico-kube-controllers created<br></code></pre></td></tr></table></figure>



<h4 id="4）集群状态验证"><a href="#4）集群状态验证" class="headerlink" title="4）集群状态验证"></a>4）集群状态验证</h4><h5 id="1-查看-pod-状态"><a href="#1-查看-pod-状态" class="headerlink" title="1. 查看 pod 状态"></a>1. 查看 pod 状态</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get pods -n kube-system<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME                                       READY   STATUS    RESTARTS   AGE<br>calico-kube-controllers-5cdd469496-sqqdh   1/1     Running   0          4h<br>calico-node-927gm                          1/1     Running   0          4h<br>calico-node-j5886                          1/1     Running   0          4h<br>calico-node-pbgcb                          1/1     Running   0          4h<br>calico-node-zn4g8                          1/1     Running   0          4h<br></code></pre></td></tr></table></figure>



<h5 id="2-查看-node-状态"><a href="#2-查看-node-状态" class="headerlink" title="2. 查看 node 状态"></a>2. 查看 node 状态</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get nodes -n kube-system<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME           STATUS   ROLES    AGE   VERSION<br>k8s-master01   Ready    &lt;none&gt;   5h   v1.18.18<br>k8s-master02   Ready    &lt;none&gt;   5h   v1.18.18<br>k8s-master03   Ready    &lt;none&gt;   5h   v1.18.18<br>k8s-node01     Ready    &lt;none&gt;   5h   v1.18.18<br></code></pre></td></tr></table></figure>



<h5 id="3-查看路由（BGP-模式）"><a href="#3-查看路由（BGP-模式）" class="headerlink" title="3. 查看路由（BGP 模式）"></a>3. 查看路由（BGP 模式）</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ip route<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">default via 192.168.3.1 dev eth0 proto static metric 100 <br>10.244.32.128 dev calie5ddea5eea8 scope link <br>blackhole 10.244.32.128/26 proto bird <br>10.244.122.128/26 via 192.168.3.232 dev eth0 proto bird <br>172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 <br>192.168.3.0/24 dev eth0 proto kernel scope link src 192.168.3.231 metric 100 <br></code></pre></td></tr></table></figure>

<p>相关路由说明:</p>
<blockquote>
<ul>
<li><p><code>10.244.32.128 dev calie5ddea5eea8 scope link</code> : 该路由会将通向容器IP的请求导向 veth <code>calie5ddea5eea8</code>，进而让请求直达容器内的网卡。</p>
</li>
<li><p><code>blackhole 10.244.32.128/26 proto bird </code>: 该路由表示发往 <code>10.244.32.128/26</code> 网段的报文都会被丢弃且不会回复源地址，所以是 <code>blackhole</code>。配置这条路由以避免报文被发送到非该网段的外部地址。</p>
</li>
<li><p><code>10.244.122.128/26 via 192.168.3.232 dev eth0 proto bird</code>: 要访问 calico 网络中的某个网段，需要以 node IP作为网关，通过 eth0 发包。</p>
</li>
</ul>
</blockquote>
<h3 id="7、部署必要-Kubernetes集群插件"><a href="#7、部署必要-Kubernetes集群插件" class="headerlink" title="7、部署必要 Kubernetes集群插件"></a>7、部署必要 Kubernetes集群插件</h3><blockquote>
<p>🌈 插件是Kubernetes集群的附件组件，丰富和完善了集群的功能，这里分别介绍的插件有coredns、Dashboard、Metrics Server，需要注意的是：kuberntes 自带插件的 manifests yaml 文件使用 gcr.io 的 docker registry，国内被墙，需要手动替换为其它 registry 地址或提前在FQ服务器上下载，然后再同步到对应的k8s部署机器上。</p>
</blockquote>
<h4 id="1）部署-CoreDNS"><a href="#1）部署-CoreDNS" class="headerlink" title="1）部署 CoreDNS"></a>1）部署 CoreDNS</h4><p><strong>插件介绍 —— CoreDNS:</strong> <a href="https://tareya.github.io/2021/10/21/Kubernetes-%E6%8F%92%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-CoreDNS/">传送门</a></p>
<h5 id="1-获取-CoreDNS-manifest"><a href="#1-获取-CoreDNS-manifest" class="headerlink" title="1. 获取 CoreDNS manifest"></a>1. 获取 CoreDNS manifest</h5><blockquote>
<p>🚩 我们之前在部署 k8s server 端的时候，已经解压了 <code>kubernetes-server-linux-amd64.tar.gz</code> ，其中会有 <code>kubernetes-src.tar.gz</code>，其中包含的是 k8s 的源码。解压后，coredns 的目录是 <code>cluster/addons/dns</code></p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~/kubernetes &amp;&amp; \<br>mkdir -p src addons &amp;&amp; \<br>tar xf kubernetes-src.tar.gz -C src &amp;&amp; \<br>cp -a src/cluster/addons/dns/coredns/coredns.yaml.base addons/coredns.yaml<br></code></pre></td></tr></table></figure>



<h5 id="2-修改-CoreDNS-manifest"><a href="#2-修改-CoreDNS-manifest" class="headerlink" title="2. 修改 CoreDNS manifest"></a>2. 修改 CoreDNS manifest</h5><p>文件路径 <code>~/kubernetes/addons/coredns.yaml</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sed -i -e &quot;s/__PILLAR__DNS__DOMAIN__/cluster.local/&quot; -e &quot;s/__PILLAR__DNS__SERVER__/10.0.0.2/&quot; -e &quot;s/__PILLAR__DNS__MEMORY__LIMIT__/170Mi/&quot; -e &quot;s/k8s.gcr.io/coredns/&quot; ~/kubernetes/addons/coredns.yaml<br></code></pre></td></tr></table></figure>

<h6 id="参数说明-2"><a href="#参数说明-2" class="headerlink" title="参数说明:"></a>参数说明:</h6><blockquote>
<ul>
<li><code>__PILLAR__DNS__DOMAIN__</code>: 集群 DNS 域名，这里替换成 cluster.local，即沿用本地解析</li>
<li><code>__PILLAR__DNS__SERVER__</code>：集群 DNS 服务 IP，从 SERVICE_CIDR 中预分配一个 IP 即可（注意不要用 CIDR 第一个 IP）</li>
<li><code>__PILLAR__DNS__MEMORY__LIMIT__</code>: CoreDNS 服务内存最大限制，<code>requests</code> 为 70Mi，limit 多给 100Mi 即可</li>
<li><code>k8s.gcr.io</code>: k8s 默认的镜像地址国内被墙的，但是在 docker hub 上有官方提供的镜像地址，故可以使用 docke hub 地址</li>
</ul>
</blockquote>
<h5 id="3-创建-CoreDNS"><a href="#3-创建-CoreDNS" class="headerlink" title="3. 创建 CoreDNS"></a>3. 创建 CoreDNS</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f ~/kubernetes/addons/coredns.yaml <br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">serviceaccount/coredns created<br>clusterrole.rbac.authorization.k8s.io/system:coredns created<br>clusterrolebinding.rbac.authorization.k8s.io/system:coredns created<br>configmap/coredns created<br>deployment.apps/coredns created<br>service/kube-dns created<br></code></pre></td></tr></table></figure>



<h5 id="4-验证-CoreDNS-状态"><a href="#4-验证-CoreDNS-状态" class="headerlink" title="4. 验证 CoreDNS 状态"></a>4. 验证 CoreDNS 状态</h5><p>检查 coredns 功能 (执行下面命令后，稍微等一会儿，确保 READY 状态都是可用的)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get pod -n kube-system -o wide|grep coredns<br></code></pre></td></tr></table></figure>

<p>返回结果如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">coredns-55f9545c85-dbf67                   1/1     Running   0          44s   10.244.32.128    k8s-master01   &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure>

<p>查看创建的coredns的pod状态,确保没有报错</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl describe pod -n kube-system coredns-55f9545c85-dbf67<br></code></pre></td></tr></table></figure>

<p>正确返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">......<br>......<br>Events:<br>  Type    Reason     Age   From               Message<br>  ----    ------     ----  ----               -------<br>  Normal  Scheduled  2m    default-scheduler  Successfully assigned kube-system/coredns-55f9545c85-dbf67 to k8s-master01<br>  Normal  Pulling    119s  kubelet            Pulling image &quot;coredns/coredns:1.6.5&quot;<br>  Normal  Pulled     92s   kubelet            Successfully pulled image &quot;coredns/coredns:1.6.5&quot;<br>  Normal  Created    92s   kubelet            Created container coredns<br>  Normal  Started    92s   kubelet            Started container coredns<br></code></pre></td></tr></table></figure>



<h5 id="5-验证-CoreDNS-功能"><a href="#5-验证-CoreDNS-功能" class="headerlink" title="5. 验证 CoreDNS 功能"></a>5. 验证 CoreDNS 功能</h5><h6 id="1⃣️-创建-nginx-demo-的-Deployment-manifest"><a href="#1⃣️-创建-nginx-demo-的-Deployment-manifest" class="headerlink" title="1⃣️ 创建 nginx-demo 的 Deployment  manifest"></a>1⃣️ 创建 nginx-demo 的 Deployment  manifest</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/kubernetes/yaml/nginx-demo.yaml &lt;&lt;EOF<br>apiVersion: v1<br>kind: Deployment<br>metadata:<br>  name: nginx-demo<br>spec:<br>  replicas: 2<br>  selector:<br>    matchLabels:<br>      app: nginx-demo<br>  template:<br>    metadata:<br>      labels:<br>        app: nginx-demo<br>    spec:<br>      containers:<br>      - name: nginx-demo<br>        image: nginx:stable<br>        ports:<br>        - containerPort: 80<br>EOF<br></code></pre></td></tr></table></figure>

<h6 id="2⃣️-创建资源"><a href="#2⃣️-创建资源" class="headerlink" title="2⃣️ 创建资源"></a>2⃣️ 创建资源</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f ~/kubernetes/yaml/nginx-demo.yaml<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">deployment.apps/nginx-demo created<br></code></pre></td></tr></table></figure>

<p>查看 pod 状态:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get pod <br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME                          READY   STATUS    RESTARTS   AGE<br>nginx-demo-7fc57f6f87-8sgxm   1/1     Running   0          78s<br>nginx-demo-7fc57f6f87-tzfzh   1/1     Running   0          78s<br></code></pre></td></tr></table></figure>

<h6 id="3⃣️-暴露-nginx-demo"><a href="#3⃣️-暴露-nginx-demo" class="headerlink" title="3⃣️ 暴露 nginx-demo"></a>3⃣️ 暴露 nginx-demo</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl expose deployment nginx-demo<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">service/nginx-demo exposed<br></code></pre></td></tr></table></figure>

<p>查看 service 状态:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get svc<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE<br>kubernetes   ClusterIP   10.0.0.1      &lt;none&gt;        443/TCP   2d18h<br>nginx-demo   ClusterIP   10.0.53.182   &lt;none&gt;        80/TCP    3s				#<br></code></pre></td></tr></table></figure>

<h6 id="4⃣️-验证内部解析功能"><a href="#4⃣️-验证内部解析功能" class="headerlink" title="4⃣️ 验证内部解析功能"></a>4⃣️ 验证内部解析功能</h6><blockquote>
<p>🚩 我们知道 k8s 内部通讯是通过 service 向下找对应 pod 的，每个 service 的解析原理实际上也是遵循 dns 解析原理的，因为如果我们能从另一个 pod 中正确解析 nginx-demo 的 service，则能证明 coredns 内部解析功能是正常的。</p>
</blockquote>
<p>再创建另一个 daemonset  <code>dnsutils-demo</code> 用于解析测试</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat &gt; ~/kubernetes/yaml/dnsutils-demo.yml &lt;&lt;EOF<br>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: dnsutils-demo<br>  labels:<br>    app: dnsutils-demo<br>spec:<br>  type: NodePort<br>  selector:<br>    app: dnsutils-demo<br>  ports:<br>  - name: http<br>    port: 80<br>    targetPort: 80<br>---<br>apiVersion: apps/v1<br>kind: DaemonSet<br>metadata:<br>  name: dnsutils-demo<br>  labels:<br>    addonmanager.kubernetes.io/mode: Reconcile<br>spec:<br>	selector:<br>		matchLabels:<br>      app: dnsutils-demo<br>  template:<br>    metadata:<br>      labels:<br>        app: dnsutils-demo<br>    spec:<br>      containers:<br>      - name: dnsutils-demo<br>        image: tutum/dnsutils:latest<br>        command:<br>          - sleep<br>          - &quot;3600&quot;<br>        ports:<br>        - containerPort: 80<br>EOF<br></code></pre></td></tr></table></figure>

<p>创建资源:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl apply -f ~/kubernetes/yaml/dnsutils-demo.yml<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">service/dnsutils-demo unchanged<br>daemonset.apps/dnsutils-demo created<br></code></pre></td></tr></table></figure>

<p>查看 pod 信息:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get pod -o wide<br></code></pre></td></tr></table></figure>

<p>返回如下, 在每个节点都生成了 dnsutils-demo 的 pod</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME                          READY   STATUS    RESTARTS   AGE    IP               NODE           NOMINATED NODE   READINESS GATES<br>dnsutils-demo-9nbzt           1/1     Running   0          3m8s   10.244.32.129    k8s-master01   &lt;none&gt;           &lt;none&gt;<br>dnsutils-demo-fqcnf           1/1     Running   0          3m8s   10.244.195.1     k8s-master03   &lt;none&gt;           &lt;none&gt;<br>dnsutils-demo-jgkll           1/1     Running   0          3m8s   10.244.85.193    k8s-node01     &lt;none&gt;           &lt;none&gt;<br>dnsutils-demo-n7j9r           1/1     Running   0          3m8s   10.244.122.130   k8s-master02   &lt;none&gt;           &lt;none&gt;<br>nginx-demo-7fc57f6f87-8sgxm   1/1     Running   0          145m   10.244.85.192    k8s-node01     &lt;none&gt;           &lt;none&gt;<br>nginx-demo-7fc57f6f87-tzfzh   1/1     Running   0          145m   10.244.195.0     k8s-master03   &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure>

<p>查看 service 信息:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get sv<br></code></pre></td></tr></table></figure>

<p>返回如下，多了 dnsutils-demo 的 svc</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE<br>dnsutils-demo   NodePort    10.0.243.227   &lt;none&gt;        80:36461/TCP   4m21s<br>kubernetes      ClusterIP   10.0.0.1       &lt;none&gt;        443/TCP        2d20h<br>nginx-demo      ClusterIP   10.0.53.182    &lt;none&gt;        80/TCP         139m<br></code></pre></td></tr></table></figure>

<p>依次通过 dnsutils-demo 的 pod 中解析 nginx-demo 的 svc，验证是否可以解析得到对应的 clusterIP</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">for i in $(kubectl get pod|awk &#x27;/dnsutils/&#123;print $1&#125;&#x27;|xargs);do kubectl exec $i nslookup nginx-demo;done<br></code></pre></td></tr></table></figure>

<p>正常返回结果如下，证明可以通过 CoreDNS 解析 service 获取到正确的 ClusterIP。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs shell">Server:		10.0.0.2<br>Address:	10.0.0.2#53<br><br>Name:	nginx-demo.default.svc.cluster.local<br>Address: 10.0.53.182<br><br>Server:		10.0.0.2<br>Address:	10.0.0.2#53<br><br>Name:	nginx-demo.default.svc.cluster.local<br>Address: 10.0.53.182<br><br>Server:		10.0.0.2<br>Address:	10.0.0.2#53<br><br>Name:	nginx-demo.default.svc.cluster.local<br>Address: 10.0.53.182<br><br>Server:		10.0.0.2<br>Address:	10.0.0.2#53<br><br>Name:	nginx-demo.default.svc.cluster.local<br>Address: 10.0.53.182<br><br></code></pre></td></tr></table></figure>

<h6 id="5⃣️-验证外部解析功能"><a href="#5⃣️-验证外部解析功能" class="headerlink" title="5⃣️ 验证外部解析功能"></a>5⃣️ 验证外部解析功能</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl exec -it  nginx-demo-7fc57f6f87-8sgxm /bin/bash<br></code></pre></td></tr></table></figure>

<p>进入pod后查看 <code>/etc/resolv.conf</code>。确保 pod 容器中<code>/etc/resolv.conf</code> 里的 nameserver 地址为 kubelet 配置文件中配置的 clusterDNS 以及 clusterDomain。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">root@nginx-demo-7fc57f6f87-8sgxm:/# cat /etc/resolv.conf <br>nameserver 10.0.0.2<br>search default.svc.cluster.local svc.cluster.local cluster.local<br>options ndots:5<br></code></pre></td></tr></table></figure>

<blockquote>
<p>🚩 官方提供的 CoreDNS manifest 中 DNS 相关 configmap 中 <code>forward . /etc/resolv.conf</code> 的配置表示 CoreDNS 会继承宿主机本地 <code>/etc/resolv.conf</code> 中的 nameserver，因此如果 CoreDNS 如果功能正常的话，我们在 pod 内即可进行正常的 dns 解析公网域名，如 <code>www.baidu.com</code> 等。</p>
</blockquote>
<p>在 pod 内安装 ping，nginx 默认提供的镜像 OS 为 Debian，使用 apt 进行包管理，自行安装 ping 命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">apt-get update<br>apt-get install inetutils-ping<br></code></pre></td></tr></table></figure>

<p>安装完成后，ping baidu，验证解析</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ping -c4 www.baidu.com<br></code></pre></td></tr></table></figure>

<p>正常返回如下，说明外部 dns 解析功能正常:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">PING www.a.shifen.com (180.101.49.12): 56 data bytes<br>64 bytes from 180.101.49.12: icmp_seq=0 ttl=47 time=8.186 ms<br>64 bytes from 180.101.49.12: icmp_seq=1 ttl=47 time=8.048 ms<br>64 bytes from 180.101.49.12: icmp_seq=2 ttl=47 time=7.854 ms<br>64 bytes from 180.101.49.12: icmp_seq=3 ttl=47 time=7.688 ms<br>--- www.a.shifen.com ping statistics ---<br>4 packets transmitted, 4 packets received, 0% packet loss<br>round-trip min/avg/max/stddev = 7.688/7.944/8.186/0.189 ms<br></code></pre></td></tr></table></figure>

<p>也可以通过 dnsutils 进行解析查询</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl exec dnsutils-demo-9nbzt nslookup www.baidu.com<br></code></pre></td></tr></table></figure>

<p>正常返回如下，可以通过 CoreDNS 解析到外部域名:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">Server:		10.0.0.2<br>Address:	10.0.0.2#53<br><br>Non-authoritative answer:<br>www.baidu.com	canonical name = www.a.shifen.com.<br>Name:	www.a.shifen.com<br>Address: 180.101.49.12<br>Name:	www.a.shifen.com<br>Address: 180.101.49.11<br></code></pre></td></tr></table></figure>





<h4 id="2）部署-dashboard"><a href="#2）部署-dashboard" class="headerlink" title="2）部署 dashboard"></a>2）部署 dashboard</h4><p><strong>插件介绍 —— Dashboard:</strong> <a href="https://tareya.github.io/2021/10/21/Kubernetes-%E6%8F%92%E4%BB%B6%E4%BB%8B%E7%BB%8D-%E2%80%94%E2%80%94-Dashboard/">传送们</a></p>
<h5 id="1-获取-Dashboard-manifest"><a href="#1-获取-Dashboard-manifest" class="headerlink" title="1. 获取 Dashboard manifest"></a>1. 获取 Dashboard manifest</h5><blockquote>
<p>🚩 我们之前在部署 coredns 的时候，已经解压了 <code>kubernetes-src.tar.gz</code>，dashboard 的目录是 <code>cluster/addons/dashboard</code></p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir -p ~/kubernetes/addons/dashboard &amp;&amp; \<br>wget -c https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml -O  ~/kubernetes/addons/dashboard/recommended.yaml<br></code></pre></td></tr></table></figure>



<h5 id="2-修改-Dashboard-manifest"><a href="#2-修改-Dashboard-manifest" class="headerlink" title="2. 修改 Dashboard manifest"></a>2. 修改 Dashboard manifest</h5><p>文件路径 <code>~/kubernetes/addons/dashboard/recommended.yaml </code>，修改文件，将 service 类型改为 NodePort，这样就外部可以直接通过访问 NodeIP:NodePort 访问 Dashboard。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">...</span><br><span class="hljs-meta">---</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">labels:</span><br>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kubernetes-dashboard</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">kubernetes-dashboard</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kubernetes-dashboard</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>					<span class="hljs-comment"># 新增</span><br>  <span class="hljs-attr">ports:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">443</span><br>      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8443</span><br>      <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30443</span>			<span class="hljs-comment"># 新增</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">kubernetes-dashboard</span><br><span class="hljs-string">...</span><br></code></pre></td></tr></table></figure>



<h5 id="3-创建-Dashboard"><a href="#3-创建-Dashboard" class="headerlink" title="3. 创建 Dashboard"></a>3. 创建 Dashboard</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd ~/kubernetes/addons/dashboard/ &amp;&amp; \<br>kubectl apply -f .<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">namespace/kubernetes-dashboard created<br>serviceaccount/kubernetes-dashboard created<br>service/kubernetes-dashboard created<br>secret/kubernetes-dashboard-certs created<br>secret/kubernetes-dashboard-csrf created<br>secret/kubernetes-dashboard-key-holder created<br>configmap/kubernetes-dashboard-settings created<br>role.rbac.authorization.k8s.io/kubernetes-dashboard created<br>clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created<br>rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created<br>clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created<br>deployment.apps/kubernetes-dashboard created<br>service/dashboard-metrics-scraper created<br>deployment.apps/dashboard-metrics-scraper created<br></code></pre></td></tr></table></figure>



<h5 id="4-验证-Dashboard-状态"><a href="#4-验证-Dashboard-状态" class="headerlink" title="4. 验证 Dashboard 状态"></a>4. 验证 Dashboard 状态</h5><p>获取 dashboard pod 信息 (执行下面命令后，稍微等一会儿，确保 READY 状态都是可用的)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get pod -A -o wide|grep dashboard<br></code></pre></td></tr></table></figure>

<p>返回结果如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubernetes-dashboard   dashboard-metrics-scraper-6b4884c9d5-kzrdw   1/1     Running   0          81s     10.244.195.4     k8s-master03   &lt;none&gt;           &lt;none&gt;<br>kubernetes-dashboard   kubernetes-dashboard-7b544877d5-ngbts        1/1     Running   0          81s     10.244.85.197    k8s-node01     &lt;none&gt;           &lt;none&gt;<br></code></pre></td></tr></table></figure>

<p>查看创建的 dashboard 的 pod 状态，确保没有报错</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl describe pod -n kubernetes-dashboard kubernetes-dashboard-7b544877d5-ngbts<br></code></pre></td></tr></table></figure>

<p>正确返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">......<br>......<br>Events:<br>  Type    Reason     Age   From               Message<br>  ----    ------     ----  ----               -------<br>  Normal  Scheduled  118s  default-scheduler  Successfully assigned kubernetes-dashboard/kubernetes-dashboard-7b544877d5-ngbts to k8s-node01<br>  Normal  Pulling    118s  kubelet            Pulling image &quot;kubernetesui/dashboard:v2.0.0&quot;<br>  Normal  Pulled     102s  kubelet            Successfully pulled image &quot;kubernetesui/dashboard:v2.0.0&quot;<br>  Normal  Created    102s  kubelet            Created container kubernetes-dashboard<br>  Normal  Started    102s  kubelet            Started container kubernetes-dashboard<br></code></pre></td></tr></table></figure>

<p>查看分配的 NodePort </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">kubectl get svc -n kubernetes-dashboard<br></code></pre></td></tr></table></figure>

<p>正常返回如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE<br>dashboard-metrics-scraper   ClusterIP   10.0.251.223   &lt;none&gt;        8000/TCP        2m38s<br>kubernetes-dashboard        NodePort    10.0.73.122    &lt;none&gt;        443:30443/TCP   2m38s<br></code></pre></td></tr></table></figure>



<h5 id="5-通过-kube-apiserver-外部访问-dashboard"><a href="#5-通过-kube-apiserver-外部访问-dashboard" class="headerlink" title="5. 通过 kube-apiserver 外部访问 dashboard"></a>5. 通过 kube-apiserver 外部访问 dashboard</h5><h6 id="1⃣️-导入客户端证书"><a href="#1⃣️-导入客户端证书" class="headerlink" title="1⃣️ 导入客户端证书"></a>1⃣️ 导入客户端证书</h6><blockquote>
<p>⚠️  <strong>注意</strong></p>
<p>需要注意，外部访问必须通过 kube-apiserver 的安全端口(https)访问 dashbaord，访问时浏览器需要进行认证，否则会被 kube-apiserver 拒绝访问。报错如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">services &quot;https:kubernetes-dashboard:&quot; is forbidden: User &quot;system:anonymous&quot; cannot get resource &quot;services/proxy&quot; in API group &quot;&quot; in the namespace &quot;kube-system&quot;<br></code></pre></td></tr></table></figure>

<p><strong>Mac OS系统客户机上导入证书的方法:</strong></p>
<p>1）创建客户端证书</p>
<p>需要使用我们一开始生成的生成的 admin 证书对，创建的客户端证书为 Chrome 或 Firefox 识别的 p12 格式</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">openssl pkcs12 -export  -out kube-admin.pfx -name &quot;kubernetes-client&quot; -inkey /data/applications/kubernetes/ssl/admin-key.pem -in /data/applications/kubernetes/ssl/admin.pem -certfile /data/applications/kubernetes/ssl/ca.pem<br></code></pre></td></tr></table></figure>

<p>返回交互命令:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">Enter Export Password:                      # 这里输入自己设定的任意密码，比如&quot;123456&quot;<br>Verifying - Enter Export Password:          # 确认密码: 123456<br></code></pre></td></tr></table></figure>

<p>2）本地导入证书</p>
<p>将生成的客户端证书拷贝到本地，在 “钥匙串访问” =&gt; “系统” =&gt; “添加钥匙串”，导入客户端证书（导入时会提示输入证书密码，即 “123456”），如下图</p>
<p><img src="https://blog-images.tareya.cn/blog_images/image-20211012171934292.png" srcset="/img/loading.gif" lazyload alt="image-20211012171934292"></p>
<p>3）设置 kubernetes （即 apiserver的）tls 证书永久信任</p>
<p>点击Mac本上的”<strong>钥匙串访问</strong>“ -&gt; “<strong>系统</strong>“ -&gt; “<strong>证书</strong>“ -&gt; “<strong>kebernetes</strong>“(双击里面的”<strong>信任</strong>“，改成”<strong>始终信任</strong>“),如下图：</p>
<p><img src="https://blog-images.tareya.cn/blog_images/image-20211012171426530.png" srcset="/img/loading.gif" lazyload alt="image-20211012171426530"></p>
<p>清除浏览器历史记录，一定要重启浏览器，接着访问 apiserver地址，接着会提示选择一个浏览器证书，会提示证书认证, 然后再次访问 apiserver，发现相应的 metrics 数据就成功显示出来了！！(注意，如果失败了。则可以删除证书，然后重新生成，重新导入再跟着操作步骤来一遍，清除浏览器缓存，重启浏览器，选择导入的证书，再次访问即可！）</p>
<p>访问 <a target="_blank" rel="noopener" href="https://192.168.3.231:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login%EF%BC%8C%E4%BC%9A%E6%9C%89%E5%A6%82%E4%B8%8B%E8%AE%A4%E8%AF%81%E6%8F%90%E7%A4%BA">https://192.168.3.231:6443/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#/login，会有如下认证提示</a></p>
<p><img src="https://blog-images.tareya.cn/blog_images/image-20211012172918878.png" srcset="/img/loading.gif" lazyload alt="image-20211012172918878"></p>
<p>选择证书后，即跳转到 dashboard 的认证页面</p>
<p><img src="https://blog-images.tareya.cn/blog_images/image-20211012182139093.png" srcset="/img/loading.gif" lazyload alt="image-20211012182139093"></p>
</blockquote>
<h6 id="2⃣️-创建登录-Dashboard-的-token"><a href="#2⃣️-创建登录-Dashboard-的-token" class="headerlink" title="2⃣️ 创建登录 Dashboard 的 token"></a>2⃣️ 创建登录 Dashboard 的 token</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 创建 service account</span><br>kubectl create sa dashboard-admin -n kube-system<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 创建 clusterrolebinding</span> <br>kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> 获取 token</span><br>ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk &#x27;&#123;print $1&#125;&#x27;)<br>DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system $ADMIN_SECRET| grep -E &#x27;^token&#x27; | awk &#x27;&#123;print $2&#125;&#x27;)<br>echo $DASHBOARD_LOGIN_TOKEN<br></code></pre></td></tr></table></figure>



<h6 id="3⃣️-创建登录-Dashboard-的-kubeconfig-配置文件（推荐）"><a href="#3⃣️-创建登录-Dashboard-的-kubeconfig-配置文件（推荐）" class="headerlink" title="3⃣️ 创建登录 Dashboard 的 kubeconfig 配置文件（推荐）"></a>3⃣️ 创建登录 Dashboard 的 kubeconfig 配置文件（推荐）</h6><p>dashboard 默认只支持 token 认证（不支持 client 证书认证），所以如果使用 Kubeconfig 文件，需要将 token 写入到该文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 设置集群参数</span><br>kubectl config set-cluster kubernetes \<br>  --certificate-authority=/data/applications/kubernetes/ssl/ca.pem \<br>  --embed-certs=true \<br>  --server=https://192.168.3.231:6443 \<br>  --kubeconfig=dashboard.kubeconfig<br><span class="hljs-meta">  </span><br><span class="hljs-meta">#</span><span class="bash"> 设置客户端认证参数，使用上面创建的 Token</span><br>kubectl config set-credentials dashboard_user \<br>  --token=$&#123;DASHBOARD_LOGIN_TOKEN&#125; \<br>  --kubeconfig=dashboard.kubeconfig<br><span class="hljs-meta">  </span><br><span class="hljs-meta">#</span><span class="bash"> 设置上下文参数</span><br>kubectl config set-context default \<br>  --cluster=kubernetes \<br>  --user=dashboard_user \<br>  --kubeconfig=dashboard.kubeconfig<br><span class="hljs-meta">  </span><br><span class="hljs-meta">#</span><span class="bash"> 设置默认上下文</span><br>kubectl config use-context default --kubeconfig=dashboard.kubeconfig<br></code></pre></td></tr></table></figure>

<p>将上面生成的 dashboard.kubeconfig 文件拷贝到本地，然后使用这个文件登录 Dashboard 即可，访问结果如下图:</p>
<p><img src="https://blog-images.tareya.cn/blog_images/image-20211012182619869.png" srcset="/img/loading.gif" lazyload alt="image-20211012182619869"></p>
<blockquote>
<p>🚩 <strong>这里由于缺少Heapster或metrics-server插件，当前dashboard还不能展示 Pod、Nodes 的 CPU、内存等统计数据和图表。</strong></p>
</blockquote>
<h4 id="3）部署-metrics-server"><a href="#3）部署-metrics-server" class="headerlink" title="3）部署 metrics-server"></a>3）部署 metrics-server</h4><h4 id="4）部署-kube-state-metrics"><a href="#4）部署-kube-state-metrics" class="headerlink" title="4）部署 kube-state-metrics"></a>4）部署 kube-state-metrics</h4><h4 id="5）部署-prometheus"><a href="#5）部署-prometheus" class="headerlink" title="5）部署 prometheus"></a>5）部署 prometheus</h4>
            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Kubernetes/">Kubernetes</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Kubernetes-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/">Kubernetes 快速入门手册</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2021/09/27/Kubernetes-kmem-%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Kubernetes kmem 内存泄漏问题解决方案</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/09/21/%E7%AC%AC12%E7%AB%A0%E3%80%81%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E9%AB%98%E7%BA%A7%E7%9F%A5%E8%AF%86/">
                        <span class="hidden-mobile">第12章、项目管理高级知识</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
